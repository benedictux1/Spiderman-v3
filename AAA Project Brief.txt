================================================================================
KITH PLATFORM - COMPREHENSIVE TECHNICAL IMPLEMENTATION GUIDE
Project Brief - Spiderman V4 (Complete Technical Documentation with Multimodal Intelligence)
================================================================================

PROJECT OVERVIEW
================================================================================

Project Name: Kith Platform (Spiderman V4)
Version: 4.0 (Major Feature Integration - January 2025)
Status: Production Ready with Multimodal Intelligence & Advanced Visualization
Type: Personal Intelligence & Relationship Management Platform
Primary Languages: Python (Flask), JavaScript (Vanilla), HTML5, CSS3
Repository: Local development environment with comprehensive version control

EXECUTIVE SUMMARY
================================================================================

Kith Platform is a sophisticated personal intelligence system designed to help users organize, analyze, and derive insights from their personal relationships and social interactions. The platform combines modern web technologies with AI-powered analysis to transform unstructured notes and conversation data into actionable relationship intelligence.

The platform now includes advanced multimodal capabilities including:
- **File Upload & AI Analysis**: Upload images, PDFs, and documents for AI-powered content extraction and analysis using Gemini 1.5 Pro
- **Voice Recording & Transcription**: Record voice memos directly in the browser with automatic transcription via OpenAI Whisper
- **Interactive Relationship Graph**: Visualize and manage contact relationships with an advanced graph interface using vis.js
- **Telegram Integration**: Automated message synchronization and analysis
- **Calendar Integration**: Context-aware scheduling and relationship insights

This documentation provides complete technical specifications, implementation details, and step-by-step instructions for recreating the entire application from scratch. Every component, from database schemas to JavaScript functions, is documented with actual code examples and pseudo-code for complex algorithms.

COMPLETE TECHNICAL ARCHITECTURE
================================================================================

BACKEND STACK SPECIFICATION:

Framework: Flask 2.3.3
```python
# Core Flask application initialization (app.py:35-40)
from flask import Flask, request, jsonify, render_template, Response
from flask_apscheduler import APScheduler
from dotenv import load_dotenv

load_dotenv()
app = Flask(__name__)
app.config['TEMPLATES_AUTO_RELOAD'] = True
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0
```

Database: SQLite with SQLAlchemy 2.0.21 ORM
```sql
-- Complete Database Schema (Generated from models.py)

CREATE TABLE users (
    id INTEGER NOT NULL PRIMARY KEY,
    username VARCHAR(255) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE contacts (
    id INTEGER NOT NULL PRIMARY KEY,
    user_id INTEGER NOT NULL DEFAULT 1,
    full_name VARCHAR(255) NOT NULL,
    tier INTEGER NOT NULL DEFAULT 2,
    vector_collection_id VARCHAR(255) UNIQUE,
    telegram_id VARCHAR(255),
    telegram_username VARCHAR(255),
    telegram_phone VARCHAR(255),
    telegram_handle VARCHAR(255),
    is_verified BOOLEAN DEFAULT FALSE,
    is_premium BOOLEAN DEFAULT FALSE,
    telegram_last_sync DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(user_id) REFERENCES users (id)
);

CREATE TABLE raw_notes (
    id INTEGER NOT NULL PRIMARY KEY,
    contact_id INTEGER NOT NULL,
    content TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    tags VARCHAR,
    FOREIGN KEY(contact_id) REFERENCES contacts (id)
);

CREATE TABLE synthesized_entries (
    id INTEGER NOT NULL PRIMARY KEY,
    contact_id INTEGER NOT NULL,
    source_note_id INTEGER,
    category VARCHAR(255) NOT NULL,
    content TEXT NOT NULL,
    summary TEXT,
    narrative_text TEXT,
    confidence_score FLOAT,
    ai_confidence FLOAT,
    is_approved BOOLEAN DEFAULT TRUE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(contact_id) REFERENCES contacts (id) ON DELETE CASCADE,
    FOREIGN KEY(source_note_id) REFERENCES raw_notes (id)
);

CREATE TABLE import_tasks (
    id VARCHAR(255) NOT NULL PRIMARY KEY,
    user_id INTEGER NOT NULL DEFAULT 1,
    contact_id INTEGER,
    task_type VARCHAR(50) NOT NULL DEFAULT 'telegram_import',
    status VARCHAR(50) NOT NULL DEFAULT 'pending',
    progress INTEGER DEFAULT 0,
    status_message TEXT,
    error_details TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    completed_at DATETIME,
    FOREIGN KEY(user_id) REFERENCES users (id),
    FOREIGN KEY(contact_id) REFERENCES contacts (id)
);

-- Multimodal Intelligence: File Upload & Analysis
CREATE TABLE uploaded_files (
    id INTEGER NOT NULL PRIMARY KEY,
    contact_id INTEGER NOT NULL,
    user_id INTEGER NOT NULL,
    
    -- File Metadata
    original_filename VARCHAR(255) NOT NULL,
    stored_filename VARCHAR(255) UNIQUE NOT NULL, -- UUID-based unique name
    file_path TEXT NOT NULL, -- Relative path on local filesystem
    file_type VARCHAR(100) NOT NULL, -- MIME type (e.g., 'image/png', 'application/pdf')
    file_size_bytes BIGINT NOT NULL,
    
    -- Link to analysis task and resulting note
    analysis_task_id VARCHAR(255),
    generated_raw_note_id INTEGER,
    
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(contact_id) REFERENCES contacts (id) ON DELETE CASCADE,
    FOREIGN KEY(user_id) REFERENCES users (id),
    FOREIGN KEY(analysis_task_id) REFERENCES import_tasks (id),
    FOREIGN KEY(generated_raw_note_id) REFERENCES raw_notes (id)
);

-- Index for fast retrieval of contact's files
CREATE INDEX idx_uploaded_files_contact_id ON uploaded_files(contact_id);

-- Relationship Graph: Contact Groups
CREATE TABLE contact_groups (
    id INTEGER NOT NULL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    name VARCHAR(255) NOT NULL,
    color VARCHAR(7) DEFAULT '#97C2FC', -- Hex color for graph visualization
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

-- Relationship Graph: Group Memberships
CREATE TABLE contact_group_memberships (
    contact_id INTEGER NOT NULL,
    group_id INTEGER NOT NULL,
    PRIMARY KEY (contact_id, group_id),
    FOREIGN KEY(contact_id) REFERENCES contacts (id) ON DELETE CASCADE,
    FOREIGN KEY(group_id) REFERENCES contact_groups (id) ON DELETE CASCADE
);

-- Relationship Graph: Contact Relationships
CREATE TABLE contact_relationships (
    id INTEGER NOT NULL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    source_contact_id INTEGER NOT NULL,
    target_contact_id INTEGER NOT NULL,
    label VARCHAR(100), -- Relationship type (e.g., 'Siblings', 'Colleagues')
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(user_id) REFERENCES users (id),
    FOREIGN KEY(source_contact_id) REFERENCES contacts (id) ON DELETE CASCADE,
    FOREIGN KEY(target_contact_id) REFERENCES contacts (id) ON DELETE CASCADE,
    UNIQUE(user_id, source_contact_id, target_contact_id)
);

-- Indexes for relationship queries
CREATE INDEX idx_contact_relationships_source ON contact_relationships(source_contact_id);
CREATE INDEX idx_contact_relationships_target ON contact_relationships(target_contact_id);

-- Additional audit logging table
CREATE TABLE contact_audit_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    contact_id INTEGER NOT NULL,
    user_id INTEGER NOT NULL,
    event_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    event_type TEXT NOT NULL,
    source TEXT NOT NULL,
    before_state TEXT,
    after_state TEXT,
    raw_input TEXT,
    FOREIGN KEY(contact_id) REFERENCES contacts(id),
    FOREIGN KEY(user_id) REFERENCES users(id)
);
```

SQLALCHEMY MODEL DEFINITIONS:

```python
# models.py - Complete SQLAlchemy Models
from sqlalchemy import create_engine, Column, Integer, String, Text, Boolean, DateTime, ForeignKey, Float
from sqlalchemy.orm import declarative_base, sessionmaker, relationship
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    username = Column(String(255), unique=True, nullable=False)
    password_hash = Column(String(255), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    contacts = relationship("Contact", back_populates="user")

class Contact(Base):
    __tablename__ = 'contacts'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), default=1, nullable=False)
    full_name = Column(String(255), nullable=False)
    tier = Column(Integer, default=2, nullable=False)  # 1 for inner circle, 2 for outer
    vector_collection_id = Column(String(255), unique=True)
    
    # Telegram Integration Fields
    telegram_id = Column(String(255))
    telegram_username = Column(String(255))
    telegram_phone = Column(String(255))
    telegram_handle = Column(String(255))
    is_verified = Column(Boolean, default=False)
    is_premium = Column(Boolean, default=False)
    telegram_last_sync = Column(DateTime)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    user = relationship("User", back_populates="contacts")
    raw_notes = relationship("RawNote", back_populates="contact", cascade="all, delete-orphan")
    synthesized_entries = relationship("SynthesizedEntry", back_populates="contact", cascade="all, delete-orphan")

class RawNote(Base):
    __tablename__ = 'raw_notes'
    
    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id'), nullable=False)
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    tags = Column(String)  # Store as JSON string for SQLite compatibility
    
    # Relationships
    contact = relationship("Contact", back_populates="raw_notes")
    synthesized_entries = relationship("SynthesizedEntry", back_populates="source_note")

class SynthesizedEntry(Base):
    __tablename__ = 'synthesized_entries'
    
    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'), nullable=False)
    source_note_id = Column(Integer, ForeignKey('raw_notes.id'))
    category = Column(String(255), nullable=False)
    content = Column(Text, nullable=False)
    summary = Column(Text)
    narrative_text = Column(Text)
    confidence_score = Column(Float)
    ai_confidence = Column(Float)
    is_approved = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    contact = relationship("Contact", back_populates="synthesized_entries")
    source_note = relationship("RawNote", back_populates="synthesized_entries")

class ImportTask(Base):
    __tablename__ = 'import_tasks'
    
    id = Column(String(255), primary_key=True)  # UUID string
    user_id = Column(Integer, ForeignKey('users.id'), default=1, nullable=False)
    contact_id = Column(Integer, ForeignKey('contacts.id'))
    task_type = Column(String(50), default='telegram_import', nullable=False)
    status = Column(String(50), default='pending', nullable=False)
    progress = Column(Integer, default=0)
    status_message = Column(Text)
    error_details = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
    
    # Relationships
    user = relationship("User")
    contact = relationship("Contact")

# Database setup functions
def get_database_url():
    """Get database URL from environment or use SQLite for development."""
    database_url = os.getenv('DATABASE_URL')
    if database_url and database_url.startswith('postgresql://'):
        return database_url
    else:
        return 'sqlite:///kith_platform.db'

def init_db():
    """Initialize the database and create tables."""
    try:
        engine = create_engine(get_database_url())
        Base.metadata.create_all(engine)
        return engine
    except Exception as e:
        print(f"Error initializing database: {e}")
        engine = create_engine('sqlite:///kith_platform.db')
        Base.metadata.create_all(engine)
        return engine

def get_session():
    """Get a database session."""
    engine = create_engine(get_database_url())
    Session = sessionmaker(bind=engine)
    return Session()
```

AI INTEGRATION SPECIFICATION:

Vector Database: ChromaDB 0.4.15
```python
# ChromaDB Configuration (app.py:69-76)
import chromadb
import os

# Disable anonymized telemetry by default unless explicitly enabled
os.environ.setdefault('ANONYMIZED_TELEMETRY', 'FALSE')
_PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
_DEFAULT_CHROMA_DIR = os.path.join(_PROJECT_ROOT, 'chroma_db')
CHROMA_DB_PATH = os.getenv('CHROMA_DB_PATH', _DEFAULT_CHROMA_DIR)
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
```

OpenAI Integration:
```python
# OpenAI Configuration (app.py:53-56)
import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY', '')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4')
OPENAI_MODEL_VERSION = os.getenv('OPENAI_MODEL_VERSION', '')  # optional extra pin
```

FRONTEND ARCHITECTURE:

JavaScript Modular System:
```javascript
// main.js - Core application functionality (618 lines)
// Global variables
let currentView = 'main';
let currentContactId = null;

// Setup event listeners for all buttons
function setupEventListeners() {
    // Add Note button
    const addNoteBtn = document.getElementById('profile-add-note-btn');
    if (addNoteBtn) {
        addNoteBtn.addEventListener('click', function() {
            const noteArea = document.getElementById('profile-note-input-area');
            noteArea.style.display = 'block';
            document.getElementById('profile-note-input').focus();
        });
    }

    // Wire Back to Main buttons
    const backToMainFromProfileBtn = document.getElementById('back-to-main-from-profile');
    if (backToMainFromProfileBtn) {
        backToMainFromProfileBtn.addEventListener('click', function() {
            showMainView();
        });
    }

    // Sync Telegram Chat button
    const syncTelegramBtn = document.getElementById('profile-sync-telegram-btn');
    if (syncTelegramBtn) {
        syncTelegramBtn.addEventListener('click', function() {
            handleProfileTelegramSync();
        });
    }
}

// Helper function to get selected contacts
function getSelectedContacts() {
    const checkboxes = document.querySelectorAll('input[name="contact_ids"]:checked');
    return Array.from(checkboxes).map(checkbox => parseInt(checkbox.value));
}

// Helper function to delete selected contacts
function deleteSelectedContacts(contactIds) {
    fetch('/api/contacts/bulk-delete', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({ contact_ids: contactIds })
    })
    .then(response => response.json().then(data => ({ ok: response.ok, data })))
    .then(({ ok, data }) => {
        if (ok && !data.error) {
            alert(`Successfully deleted ${contactIds.length} contact(s)`);
            loadContacts(); // Reload the contact list
        } else {
            alert('Error deleting contacts: ' + (data.error || data.message || 'Unknown error'));
        }
    })
    .catch(error => {
        console.error('Error:', error);
        alert('An error occurred while deleting contacts');
    });
}
```

RELATIONSHIP ANALYSIS CATEGORIES SYSTEM:

```python
# constants.py - Complete category system
class Categories:
    ACTIONABLE = "Actionable"
    GOALS = "Goals" 
    RELATIONSHIP_STRATEGY = "Relationship_Strategy"
    SOCIAL = "Social"
    WELLBEING = "Wellbeing"
    AVOCATION = "Avocation"
    PROFESSIONAL_BACKGROUND = "Professional_Background"
    ENVIRONMENT_AND_LIFESTYLE = "Environment_And_Lifestyle"
    PSYCHOLOGY_AND_VALUES = "Psychology_And_Values"
    COMMUNICATION_STYLE = "Communication_Style"
    CHALLENGES_AND_DEVELOPMENT = "Challenges_And_Development"
    ADMIN_MATTERS = "Admin_Matters"
    DEEPER_INSIGHTS = "Deeper_Insights"
    FINANCIAL_SITUATION = "Financial_Situation"
    ESTABLISHED_PATTERNS = "ESTABLISHED_PATTERNS"
    CORE_IDENTITY = "CORE_IDENTITY"
    INFORMATION_GAPS = "INFORMATION_GAPS"
    MEMORY_ANCHORS = "MEMORY_ANCHORS"
    POSITIONALITY = "POSITIONALITY"
    OTHERS = "Others"

# Category list for validation
VALID_CATEGORIES = [
    Categories.ACTIONABLE,
    Categories.GOALS,
    Categories.RELATIONSHIP_STRATEGY,
    Categories.SOCIAL,
    Categories.WELLBEING,
    Categories.AVOCATION,
    Categories.PROFESSIONAL_BACKGROUND,
    Categories.ENVIRONMENT_AND_LIFESTYLE,
    Categories.PSYCHOLOGY_AND_VALUES,
    Categories.COMMUNICATION_STYLE,
    Categories.CHALLENGES_AND_DEVELOPMENT,
    Categories.ADMIN_MATTERS
]

# Extended category list for UI display (includes all categories in preferred order)
CATEGORY_ORDER = [
    Categories.ACTIONABLE,
    Categories.GOALS,
    Categories.RELATIONSHIP_STRATEGY,
    Categories.SOCIAL,
    Categories.WELLBEING,
    Categories.AVOCATION,
    Categories.PROFESSIONAL_BACKGROUND,
    Categories.ENVIRONMENT_AND_LIFESTYLE,
    Categories.PSYCHOLOGY_AND_VALUES,
    Categories.COMMUNICATION_STYLE,
    Categories.CHALLENGES_AND_DEVELOPMENT,
    Categories.DEEPER_INSIGHTS,
    Categories.FINANCIAL_SITUATION,
    Categories.ADMIN_MATTERS,
    Categories.ESTABLISHED_PATTERNS,
    Categories.CORE_IDENTITY,
    Categories.INFORMATION_GAPS,
    Categories.MEMORY_ANCHORS,
    Categories.POSITIONALITY,
    Categories.OTHERS
]
```

COMPLETE API ENDPOINT DOCUMENTATION
================================================================================

CONTACT MANAGEMENT ENDPOINTS:

```python
# GET /contacts - Retrieve contact list with filtering
@app.route('/contacts', methods=['GET'])
def get_contacts():
    """
    Retrieve contacts with optional filtering by tier and search term.
    
    Query Parameters:
        tier (int, optional): Filter by contact tier (1, 2, or 3)
        search (str, optional): Search term for contact names
        
    Returns:
        JSON response with contact list:
        {
            "contacts": [
                {
                    "id": 1,
                    "full_name": "John Doe",
                    "tier": 1,
                    "telegram_username": "johndoe",
                    "created_at": "2025-01-01T00:00:00",
                    "updated_at": "2025-01-01T00:00:00"
                }
            ]
        }
    """
    pass

# POST /contacts - Create new contact with validation
@app.route('/contacts', methods=['POST'])
def create_contact():
    """
    Create a new contact with comprehensive validation.
    
    Request Body:
        {
            "full_name": "John Doe",
            "tier": 1,
            "telegram_username": "johndoe" (optional),
            "telegram_handle": "@johndoe" (optional)
        }
        
    Returns:
        Success: {"message": "Contact created successfully", "contact_id": 123}
        Error: {"error": "Validation error message"}
    """
    pass

# PATCH /contacts/<id> - Update contact information
@app.route('/contacts/<int:contact_id>', methods=['PATCH'])
def update_contact(contact_id):
    """
    Update existing contact information with audit logging.
    
    Request Body (partial updates supported):
        {
            "full_name": "Jane Doe",
            "tier": 2,
            "telegram_username": "janedoe"
        }
        
    Returns:
        Success: {"message": "Contact updated successfully"}
        Error: {"error": "Update error message"}
    """
    pass

# DELETE /contacts/<id> - Delete contact with cascading
@app.route('/contacts/<int:contact_id>', methods=['DELETE'])
def delete_contact(contact_id):
    """
    Delete contact and all associated data (notes, synthesis entries).
    
    Returns:
        Success: {"message": "Contact deleted successfully"}
        Error: {"error": "Deletion error message"}
    """
    pass

# POST /contacts/bulk-delete - Delete multiple contacts
@app.route('/api/contacts/bulk-delete', methods=['POST'])
def bulk_delete_contacts():
    """
    Delete multiple contacts in a single operation.
    
    Request Body:
        {
            "contact_ids": [1, 2, 3, 4]
        }
        
    Returns:
        Success: {"message": "4 contacts deleted successfully"}
        Error: {"error": "Bulk deletion error message"}
    """
    pass
```

AI ANALYSIS ENDPOINTS:

```python
# POST /analyze_note - Process and categorize note content
@app.route('/analyze_note', methods=['POST'])
def analyze_note():
    """
    Analyze unstructured note content using AI categorization.
    
    Request Body:
        {
            "contact_id": 123,
            "note_content": "Had coffee with John. He mentioned wanting to switch careers to data science and is looking for networking opportunities. Seems stressed about current job.",
            "temperature": 0.1 (optional),
            "model": "gpt-4" (optional)
        }
        
    Processing Steps:
        1. Validate contact exists
        2. Send note to OpenAI with categorization prompt
        3. Parse AI response into categories
        4. Store in ChromaDB for semantic search
        5. Return structured analysis for review
        
    Returns:
        {
            "analysis": {
                "Actionable": "Follow up with networking contacts in data science",
                "Professional_Background": "Currently working in a role he wants to leave, interested in data science transition",
                "Wellbeing": "Experiencing job-related stress",
                "Social": "Met for coffee, open to professional networking"
            },
            "raw_note_id": 456
        }
    """
    pass

# POST /process-note - Manual note analysis and processing
@app.route('/process-note', methods=['POST'])
def process_note():
    """
    Process note content for a specific contact with manual override options.
    
    Request Body:
        {
            "contact_id": 123,
            "content": "Note content here",
            "force_reprocess": false (optional)
        }
        
    Returns:
        Processed note analysis ready for review/saving
    """
    pass

# POST /save-synthesis - Save analyzed data with audit logging
@app.route('/save-synthesis', methods=['POST'])
def save_synthesis():
    """
    Save AI-analyzed and user-reviewed synthesis data.
    
    Request Body:
        {
            "contact_id": 123,
            "raw_note_id": 456,
            "synthesis_data": {
                "Actionable": "Follow up with networking contacts",
                "Professional_Background": "Data science career transition"
            }
        }
        
    Processing Steps:
        1. Validate all data
        2. Create synthesized_entries records
        3. Update ChromaDB collections
        4. Log audit trail
        5. Update contact timestamp
        
    Returns:
        Success confirmation with synthesis IDs
    """
    pass
```

TELEGRAM INTEGRATION ENDPOINTS:

```python
# POST /telegram/start-import - Initiate Telegram import with progress tracking
@app.route('/telegram/start-import', methods=['POST'])
def start_telegram_import():
    """
    Start background Telegram chat import for a contact.
    
    Request Body:
        {
            "contact_id": 123,
            "telegram_handle": "@johndoe",
            "days_back": 30 (optional, default 30)
        }
        
    Processing Steps:
        1. Validate contact exists
        2. Create import task with UUID
        3. Launch background worker process
        4. Return task ID for progress tracking
        
    Returns:
        {
            "task_id": "uuid-string-here",
            "message": "Import started successfully"
        }
    """
    pass

# GET /telegram/import-status/<task_id> - Poll import status
@app.route('/telegram/import-status/<task_id>', methods=['GET'])
def get_import_status(task_id):
    """
    Get real-time status of background import task.
    
    Returns:
        {
            "task_id": "uuid-string",
            "status": "processing", // pending, connecting, fetching, processing, completed, failed
            "progress": 65, // 0-100
            "status_message": "Processing 150 of 230 messages",
            "error_details": null // or error message if failed
        }
    """
    pass
```

SEARCH AND DISCOVERY ENDPOINTS:

```python
# POST /search - Perform semantic or keyword search
@app.route('/search', methods=['POST'])
def search_contacts():
    """
    Perform advanced search across all contact data.
    
    Request Body:
        {
            "query": "data science networking",
            "search_type": "semantic", // or "keyword"
            "tier_filter": [1, 2], // optional tier filtering
            "category_filter": ["Actionable", "Professional_Background"], // optional
            "limit": 10 // optional, default 10
        }
        
    Processing Steps:
        1. Parse search parameters
        2. If semantic: Query ChromaDB vector database
        3. If keyword: Use SQLite FTS or LIKE queries
        4. Apply filters (tier, category)
        5. Rank and return results
        
    Returns:
        {
            "results": [
                {
                    "contact_id": 123,
                    "contact_name": "John Doe",
                    "relevance_score": 0.89,
                    "matching_content": "snippet of matching text",
                    "category": "Actionable"
                }
            ],
            "total_results": 25
        }
    """
    pass

# GET /export/csv - Export comprehensive data as CSV
@app.route('/export/csv', methods=['GET'])
def export_csv():
    """
    Export all contact data and synthesis entries as CSV.
    
    Query Parameters:
        tier (int, optional): Filter by tier
        include_raw_notes (bool, optional): Include raw notes in export
        
    Processing Steps:
        1. Query all contacts and synthesized entries
        2. Apply filters if specified
        3. Format data for CSV export
        4. Generate downloadable CSV file
        
    Returns:
        CSV file download with headers:
        contact_id,full_name,tier,category,content,confidence_score,created_at
    """
    pass
```

MULTIMODAL INTELLIGENCE ENDPOINTS:

```python
# File Upload & AI Analysis Endpoint
@app.route('/api/files/upload', methods=['POST'])
def upload_file_endpoint():
    """Handles file uploads and initiates background AI analysis."""
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400
    
    file = request.files['file']
    contact_id = request.form.get('contact_id')

    if file.filename == '' or not contact_id:
        return jsonify({"error": "No selected file or missing contact_id"}), 400

    # Securely save file to local filesystem
    original_filename = secure_filename(file.filename)
    file_extension = os.path.splitext(original_filename)[1]
    stored_filename = f"{uuid.uuid4()}{file_extension}"
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], stored_filename)
    file.save(file_path)

    # Create file record in database
    session = get_session()
    try:
        new_file = UploadedFile(
            contact_id=contact_id, user_id=1,
            original_filename=original_filename,
            stored_filename=stored_filename,
            file_path=file_path,
            file_type=file.mimetype,
            file_size_bytes=os.path.getsize(file_path)
        )
        session.add(new_file)
        session.commit()
        file_id = new_file.id

        # Create background analysis task
        task_id = str(uuid.uuid4())
        new_task = ImportTask(
            id=task_id, user_id=1, contact_id=contact_id, 
            task_type='file_analysis', status='pending'
        )
        session.add(new_task)
        
        # Link task to file
        new_file.analysis_task_id = task_id
        session.commit()

        # Schedule background job
        scheduler.add_job(
            id=task_id,
            func='file_worker.run_file_analysis',
            trigger='date',
            args=[task_id, file_id, file_path, file.mimetype, contact_id]
        )

        return jsonify({"task_id": task_id, "message": "File uploaded and analysis started."}), 202
    
    except Exception as e:
        session.rollback()
        return jsonify({"error": f"Upload failed: {e}"}), 500
    finally:
        session.close()

# Voice Transcription Endpoint
@app.route('/api/transcribe-audio', methods=['POST'])
def transcribe_audio_endpoint():
    """Transcribes audio files using OpenAI Whisper API."""
    if 'audio_file' not in request.files:
        return jsonify({"error": "No audio file part"}), 400
    
    audio_file = request.files['audio_file']
    temp_filename = f"temp_audio_{uuid.uuid4()}.webm"
    temp_filepath = os.path.join('/tmp', temp_filename)
    
    try:
        audio_file.save(temp_filepath)
        
        # Send to Whisper API for transcription
        with open(temp_filepath, "rb") as f:
            transcript_response = openai.Audio.transcribe(
                model="whisper-1", 
                file=f
            )
        
        transcript = transcript_response['text']
        return jsonify({"transcript": transcript})

    except Exception as e:
        return jsonify({"error": "Failed to transcribe audio."}), 500
    finally:
        if os.path.exists(temp_filepath):
            os.remove(temp_filepath)
```

RELATIONSHIP GRAPH ENDPOINTS:

```python
# Main Graph Data Endpoint
@app.route('/api/graph-data', methods=['GET'])
def get_graph_data():
    """Fetches all data required to render the relationship graph."""
    user_id = 1  # Assume single user
    session = get_session()
    try:
        # Fetch all contacts (nodes)
        contacts = session.query(Contact).filter_by(user_id=user_id).all()
        nodes_dict = {contact.id: {
            "id": contact.id,
            "label": contact.full_name,
            "group": None,
            "tier": contact.tier,
            "value": 10 + session.query(SynthesizedEntry).filter_by(contact_id=contact.id).count()
        } for contact in contacts}

        # Assign group memberships to nodes
        memberships = session.query(ContactGroupMembership).join(Contact).filter(Contact.user_id == user_id).all()
        for member in memberships:
            if member.contact_id in nodes_dict:
                nodes_dict[member.contact_id]['group'] = member.group_id

        # Fetch relationships (edges)
        relationships = session.query(ContactRelationship).filter_by(user_id=user_id).all()
        edges = [{
            "from": rel.source_contact_id,
            "to": rel.target_contact_id,
            "label": rel.label,
            "arrows": "to"
        } for rel in relationships]

        # Fetch group definitions for styling
        groups_db = session.query(ContactGroup).filter_by(user_id=user_id).all()
        group_definitions = {group.id: {
            "color": group.color,
            "name": group.name
        } for group in groups_db}

        # Add "self" node representing the user
        nodes_dict[0] = {"id": 0, "label": "You", "group": "self", "fixed": True, "value": 40}
        group_definitions["self"] = {"color": "#FF6384", "name": "Self"}
        
        # Add edges from "You" to all Tier 1 contacts
        for contact in contacts:
            if contact.tier == 1:
                edges.append({"from": 0, "to": contact.id, "length": 150})

        return jsonify({
            "nodes": list(nodes_dict.values()),
            "edges": edges,
            "groups": group_definitions
        })
    finally:
        session.close()

# Group Management Endpoints
@app.route('/api/groups', methods=['POST'])
def create_group():
    """Creates a new contact group for graph visualization."""
    data = request.get_json()
    name = data.get('name')
    color = data.get('color', '#97C2FC')
    user_id = 1

    if not name:
        return jsonify({"error": "Group name is required"}), 400

    session = get_session()
    try:
        new_group = ContactGroup(name=name, color=color, user_id=user_id)
        session.add(new_group)
        session.commit()
        return jsonify({
            "message": "Group created", 
            "group": {"id": new_group.id, "name": new_group.name, "color": new_group.color}
        }), 201
    except Exception as e:
        session.rollback()
        return jsonify({"error": f"Could not create group: {e}"}), 500
    finally:
        session.close()

@app.route('/api/groups/<int:group_id>/members', methods=['POST'])
def add_member_to_group(group_id):
    """Adds a contact to a group."""
    data = request.get_json()
    contact_id = data.get('contact_id')

    if not contact_id:
        return jsonify({"error": "Contact ID is required"}), 400

    session = get_session()
    try:
        # Check if membership already exists
        existing = session.query(ContactGroupMembership).filter_by(
            contact_id=contact_id, group_id=group_id
        ).first()
        if existing:
            return jsonify({"message": "Contact is already in this group"}), 200

        new_membership = ContactGroupMembership(contact_id=contact_id, group_id=group_id)
        session.add(new_membership)
        session.commit()
        return jsonify({"message": "Contact added to group"})
    except Exception as e:
        session.rollback()
        return jsonify({"error": f"Could not add member: {e}"}), 500
    finally:
        session.close()

# Relationship Management Endpoints
@app.route('/api/relationships', methods=['POST'])
def create_relationship():
    """Creates a new relationship between two contacts."""
    data = request.get_json()
    source_id = data.get('source_contact_id')
    target_id = data.get('target_contact_id')
    label = data.get('label')
    user_id = 1

    if not source_id or not target_id:
        return jsonify({"error": "Source and Target contact IDs are required"}), 400

    session = get_session()
    try:
        new_rel = ContactRelationship(
            source_contact_id=source_id,
            target_contact_id=target_id,
            label=label,
            user_id=user_id
        )
        session.add(new_rel)
        session.commit()
        return jsonify({
            "message": "Relationship created", 
            "relationship": {
                "id": new_rel.id, 
                "from": new_rel.source_contact_id, 
                "to": new_rel.target_contact_id, 
                "label": new_rel.label
            }
        }), 201
    except Exception as e:
        session.rollback()
        return jsonify({"error": f"Could not create relationship: {e}"}), 500
    finally:
        session.close()
```

TELEGRAM INTEGRATION IMPLEMENTATION
================================================================================

TELETHON SETUP AND CONFIGURATION:

```python
# telegram_worker.py - Background processing worker (311 lines)
import os
import requests
import sqlite3
import time
import asyncio
import logging
from telethon import TelegramClient
from telethon.errors.rpcerrorlist import UserNotParticipantError
from datetime import datetime, timedelta
from constants import DEFAULT_DB_NAME

# Configuration (loaded from .env)
API_ID = os.getenv('TELEGRAM_API_ID')
API_HASH = os.getenv('TELEGRAM_API_HASH')
KITH_API_URL = os.getenv('KITH_API_URL', 'http://127.0.0.1:5001')
KITH_API_TOKEN = os.getenv('KITH_API_TOKEN', 'dev_token')
SESSION_NAME = os.getenv('TELEGRAM_SESSION_NAME', 'kith_telegram_session')

# Check for required Telegram API credentials
if not API_ID or not API_HASH:
    logging.error("TELEGRAM_API_ID and TELEGRAM_API_HASH must be set in .env file")
    logging.error("Get these credentials from https://my.telegram.org/apps")
    exit(1)

def get_db_connection():
    """Get database connection with retry logic."""
    max_retries = 5
    retry_delay = 1
    
    db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), DEFAULT_DB_NAME)
    
    for attempt in range(max_retries):
        try:
            conn = sqlite3.connect(db_path, timeout=30.0)
            conn.execute('PRAGMA foreign_keys=ON')
            conn.execute('PRAGMA journal_mode=WAL')  # Better for concurrent access
            conn.execute('PRAGMA synchronous=NORMAL')  # Better performance
            conn.execute('PRAGMA temp_store=memory')  # Use memory for temp tables
            conn.execute('PRAGMA busy_timeout=5000')  # 5 second busy timeout
            conn.row_factory = sqlite3.Row  # Enable dict-like access
            
            # Test the connection
            conn.execute('SELECT 1').fetchone()
            logging.info("Database connection successful.")
            return conn
        except sqlite3.OperationalError as e:
            if 'database is locked' in str(e).lower() and attempt < max_retries - 1:
                logging.warning(f"Database locked, retrying in {retry_delay}s... (attempt {attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                logging.error(f"Final attempt to connect to database failed: {e}", exc_info=True)
                raise

def update_task_status(task_id, status, message="", error="", progress=None):
    """Helper function to update the task status in the database."""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            conn = get_db_connection()
            if not conn:
                logging.error(f"[Task {task_id}] Could not connect to the database to update status.")
                return
                
            try:
                # First, verify the task exists
                cursor = conn.execute('SELECT id FROM import_tasks WHERE id = ?', (task_id,))
                if not cursor.fetchone():
                    logging.warning(f"Warning: Task {task_id} not found in database for status update.")
                    return
                
                # Prepare update parameters
                update_params = [status, message[:1000] if message else "", error[:2000] if error else ""]
                
                if progress is not None:
                    progress = max(0, min(100, int(progress)))  # Clamp between 0-100
                    update_params.append(progress)
                    update_params.append(task_id)
                    
                    conn.execute('''
                        UPDATE import_tasks 
                        SET status = ?, status_message = ?, error_details = ?, progress = ?
                        WHERE id = ?
                    ''', update_params)
                else:
                    update_params.append(task_id)
                    conn.execute('''
                        UPDATE import_tasks 
                        SET status = ?, status_message = ?, error_details = ?
                        WHERE id = ?
                    ''', update_params)
                
                conn.commit()
                logging.info(f"[Task {task_id}] Status updated: {status} - {message}")
                break
                
            finally:
                conn.close()
                
        except Exception as e:
            if attempt == max_retries - 1:
                logging.error(f"[Task {task_id}] Failed to update status after {max_retries} attempts: {e}")
            else:
                logging.warning(f"[Task {task_id}] Attempt {attempt + 1} failed, retrying: {e}")
                time.sleep(retry_delay)
                retry_delay *= 2

async def import_telegram_chat(task_id, contact_id, telegram_handle, days_back=30):
    """
    Main function to import Telegram chat history.
    
    Processing Steps:
        1. Initialize Telethon client with session
        2. Connect and authenticate
        3. Resolve user/chat entity
        4. Fetch message history with pagination
        5. Process and analyze messages
        6. Update progress throughout process
        7. Handle errors and edge cases
    """
    update_task_status(task_id, "connecting", "Initializing Telegram connection...")
    
    try:
        # Initialize Telethon client
        client = TelegramClient(SESSION_NAME, API_ID, API_HASH)
        
        # Connect to Telegram
        await client.connect()
        
        # Check if we're authorized
        if not await client.is_user_authorized():
            update_task_status(task_id, "failed", error="Telegram authorization required. Please run telegram_setup.py first.")
            return
        
        update_task_status(task_id, "fetching", "Resolving user and fetching messages...", progress=10)
        
        # Resolve the user/chat
        try:
            entity = await client.get_entity(telegram_handle)
        except Exception as e:
            update_task_status(task_id, "failed", error=f"Could not find user '{telegram_handle}': {str(e)}")
            await client.disconnect()
            return
        
        # Calculate date range
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # Fetch messages
        messages = []
        total_messages = 0
        processed_messages = 0
        
        update_task_status(task_id, "fetching", f"Fetching messages from last {days_back} days...", progress=20)
        
        async for message in client.iter_messages(entity, offset_date=start_date, reverse=True):
            if message.date < start_date:
                break
                
            if message.text:  # Only process text messages
                messages.append({
                    'id': message.id,
                    'date': message.date,
                    'text': message.text,
                    'from_user': message.from_id == entity.id if hasattr(entity, 'id') else False
                })
                total_messages += 1
                
            # Update progress during fetching
            if total_messages % 50 == 0:
                progress = min(50, 20 + (total_messages / max(1, total_messages)) * 30)
                update_task_status(task_id, "fetching", f"Fetched {total_messages} messages...", progress=int(progress))
        
        update_task_status(task_id, "processing", f"Processing {total_messages} messages...", progress=60)
        
        # Process messages and create raw notes
        for i, msg in enumerate(messages):
            try:
                # Create raw note entry
                note_content = f"[Telegram - {msg['date'].strftime('%Y-%m-%d %H:%M')}] {msg['text']}"
                
                # Here you would call your note processing API
                response = requests.post(f"{KITH_API_URL}/process-note", 
                    json={
                        'contact_id': contact_id,
                        'content': note_content,
                        'source': 'telegram',
                        'telegram_message_id': msg['id'],
                        'telegram_date': msg['date'].isoformat()
                    },
                    headers={'Authorization': f'Bearer {KITH_API_TOKEN}'},
                    timeout=30
                )
                
                if response.status_code == 200:
                    processed_messages += 1
                
                # Update progress
                progress = 60 + (i / max(1, len(messages))) * 35
                update_task_status(task_id, "processing", f"Processed {i+1}/{len(messages)} messages", progress=int(progress))
                
            except Exception as e:
                logging.warning(f"Error processing message {msg['id']}: {e}")
                continue
        
        # Update contact's last sync timestamp
        conn = get_db_connection()
        try:
            conn.execute('''
                UPDATE contacts 
                SET telegram_last_sync = CURRENT_TIMESTAMP 
                WHERE id = ?
            ''', (contact_id,))
            conn.commit()
        finally:
            conn.close()
        
        # Final status update
        update_task_status(task_id, "completed", f"Successfully imported {processed_messages} messages", progress=100)
        
        await client.disconnect()
        
    except Exception as e:
        logging.error(f"Error in telegram import task {task_id}: {e}", exc_info=True)
        update_task_status(task_id, "failed", error=str(e))

# Entry point for background worker
if __name__ == "__main__":
    import sys
    if len(sys.argv) != 4:
        print("Usage: python telegram_worker.py <task_id> <contact_id> <telegram_handle>")
        sys.exit(1)
    
    task_id = sys.argv[1]
    contact_id = int(sys.argv[2])
    telegram_handle = sys.argv[3]
    
    # Run the async import function
    asyncio.run(import_telegram_chat(task_id, contact_id, telegram_handle))
```

ANALYTICS AND RELATIONSHIP HEALTH SCORING
================================================================================

```python
# analytics.py - Advanced relationship analytics
class RelationshipAnalytics:
    """Advanced analytics for relationship health and insights."""
    
    def __init__(self, db_path: str = DEFAULT_DB_NAME):
        self.db_path = db_path
    
    def calculate_relationship_health_score(self, contact_id: int) -> Dict:
        """
        Calculate comprehensive relationship health score for a contact.
        
        Scoring Algorithm:
        - Recency Weight: 30% (recent interactions score higher)
        - Engagement Weight: 30% (frequency of interactions)
        - Quality Weight: 20% (AI confidence and content depth)
        - Diversity Weight: 20% (variety of interaction categories)
        
        Returns score from 0-100 with detailed breakdown
        """
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Get all entries for this contact
            cursor.execute("""
                SELECT category, ai_confidence, created_at, is_approved
                FROM synthesized_entries 
                WHERE contact_id = ? AND is_approved = TRUE
                ORDER BY created_at DESC
            """, (contact_id,))
            
            entries = cursor.fetchall()
            
            if not entries:
                return {
                    "health_score": 0,
                    "total_interactions": 0,
                    "last_interaction": None,
                    "category_distribution": {},
                    "confidence_avg": 0,
                    "insights": ["No data available for this contact"]
                }
            
            # Calculate recency score (0-100)
            most_recent = datetime.fromisoformat(entries[0][2])
            days_since_last = (datetime.now() - most_recent).days
            recency_score = max(0, 100 - (days_since_last * Analytics.RECENCY_POINTS_LOSS_PER_DAY))
            
            # Calculate engagement score (0-100)
            weeks_with_data = len(set((datetime.fromisoformat(entry[2]).isocalendar()[1] for entry in entries)))
            engagement_score = min(100, weeks_with_data * Analytics.INTERACTIONS_PER_WEEK_MAX_SCORE)
            
            # Calculate quality score (0-100)
            confidence_scores = [entry[1] for entry in entries if entry[1] is not None]
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
            quality_score = avg_confidence * Analytics.CONFIDENCE_SCALE_MULTIPLIER
            
            # Calculate diversity score (0-100)
            unique_categories = len(set(entry[0] for entry in entries))
            diversity_score = min(100, unique_categories * Analytics.CATEGORY_DIVERSITY_MULTIPLIER)
            
            # Calculate weighted final score
            final_score = (
                recency_score * Analytics.RECENCY_WEIGHT +
                engagement_score * Analytics.ENGAGEMENT_WEIGHT +
                quality_score * Analytics.QUALITY_WEIGHT +
                diversity_score * Analytics.DIVERSITY_WEIGHT
            )
            
            # Generate insights
            insights = []
            if final_score >= Analytics.EXCELLENT_HEALTH_THRESHOLD:
                insights.append("Excellent relationship health - frequent, high-quality interactions")
            elif final_score >= Analytics.GOOD_HEALTH_THRESHOLD:
                insights.append("Good relationship health with room for improvement")
            elif final_score >= Analytics.MODERATE_HEALTH_THRESHOLD:
                insights.append("Moderate relationship health - consider increasing engagement")
            else:
                insights.append("Low relationship health - relationship may need attention")
            
            if days_since_last > Analytics.FOLLOW_UP_DAYS_THRESHOLD:
                insights.append(f"No recent contact ({days_since_last} days) - consider following up")
            
            category_distribution = {}
            for entry in entries:
                category = entry[0]
                category_distribution[category] = category_distribution.get(category, 0) + 1
            
            return {
                "health_score": round(final_score, 2),
                "total_interactions": len(entries),
                "last_interaction": most_recent.isoformat(),
                "category_distribution": category_distribution,
                "confidence_avg": round(avg_confidence, 2),
                "scoring_breakdown": {
                    "recency_score": round(recency_score, 2),
                    "engagement_score": round(engagement_score, 2),
                    "quality_score": round(quality_score, 2),
                    "diversity_score": round(diversity_score, 2)
                },
                "insights": insights
            }
    
    def get_actionable_items_summary(self, days_back: int = 30) -> Dict:
        """Get summary of all actionable items across all contacts."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            cutoff_date = datetime.now() - timedelta(days=days_back)
            
            cursor.execute("""
                SELECT c.full_name, se.content, se.created_at, se.ai_confidence
                FROM synthesized_entries se
                JOIN contacts c ON se.contact_id = c.id
                WHERE se.category = ? AND se.created_at > ? AND se.is_approved = TRUE
                ORDER BY se.created_at DESC
            """, (Categories.ACTIONABLE, cutoff_date.isoformat()))
            
            actionables = cursor.fetchall()
            
            # Group by contact
            by_contact = {}
            for item in actionables:
                contact_name = item[0]
                if contact_name not in by_contact:
                    by_contact[contact_name] = []
                by_contact[contact_name].append({
                    'content': item[1],
                    'created_at': item[2],
                    'confidence': item[3]
                })
            
            return {
                "total_actionables": len(actionables),
                "contacts_with_actionables": len(by_contact),
                "by_contact": by_contact,
                "high_priority_count": len([a for a in actionables if a[3] and a[3] > 0.8])
            }
```

STEP-BY-STEP IMPLEMENTATION GUIDE
================================================================================

DEVELOPMENT ENVIRONMENT SETUP:

1. **System Requirements:**
```bash
# Required software
- Python 3.8+ (tested with 3.11)
- pip package manager
- Git for version control
- SQLite3 (usually included with Python)
- Web browser (Chrome/Firefox/Safari)
```

2. **Project Structure Creation:**
```bash
# Create project directory
mkdir kith-platform
cd kith-platform

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Create directory structure
mkdir -p static/js
mkdir -p templates
mkdir -p chroma_db
touch app.py models.py constants.py analytics.py
touch static/style.css
touch static/js/main.js static/js/contacts.js static/js/settings.js
touch templates/index.html
touch requirements.txt
touch .env
```

3. **Dependencies Installation:**
```bash
# Install exact versions for consistency
pip install Flask==2.3.3
pip install SQLAlchemy==2.0.21
pip install chromadb==0.4.15
pip install telethon==1.34.0
pip install openai==0.28.1
pip install python-dotenv==1.0.0
pip install requests==2.32.4
pip install gunicorn==21.2.0

# Generate requirements file
pip freeze > requirements.txt
```

4. **Environment Configuration:**
```bash
# Create .env file with required variables
cat > .env << EOF
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
OPENAI_MODEL_VERSION=

# Telegram API Configuration
TELEGRAM_API_ID=your_telegram_api_id
TELEGRAM_API_HASH=your_telegram_api_hash
TELEGRAM_SESSION_NAME=kith_telegram_session

# Application Configuration
KITH_API_URL=http://127.0.0.1:5001
KITH_API_TOKEN=dev_token
DATABASE_URL=sqlite:///kith_platform.db
CHROMA_DB_PATH=./chroma_db

# Optional: Sentry for error tracking
SENTRY_DSN=

# Disable ChromaDB telemetry
ANONYMIZED_TELEMETRY=FALSE
EOF
```

CORE IMPLEMENTATION STEPS:

**Step 1: Database Models (models.py)**
```python
# Copy the complete models.py code from the SQLAlchemy section above
# This includes all table definitions, relationships, and database setup functions
```

**Step 2: Application Constants (constants.py)**
```python
# Copy the complete constants.py code from the constants section above
# This includes all categories, configuration values, and application settings
```

**Step 3: Main Flask Application (app.py)**
```python
# Start with basic Flask setup
import os
import json
import logging
import threading
import openai
import chromadb
from flask import Flask, request, jsonify, render_template
from dotenv import load_dotenv
from models import init_db, get_session, Contact, RawNote, SynthesizedEntry, User

load_dotenv()
app = Flask(__name__)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kith_platform.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Initialize database
init_db()

# Basic route structure
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/contacts', methods=['GET'])
def get_contacts():
    session = get_session()
    try:
        contacts = session.query(Contact).all()
        contacts_data = []
        for contact in contacts:
            contacts_data.append({
                'id': contact.id,
                'full_name': contact.full_name,
                'tier': contact.tier,
                'telegram_username': contact.telegram_username,
                'created_at': contact.created_at.isoformat() if contact.created_at else None
            })
        return jsonify({'contacts': contacts_data})
    finally:
        session.close()

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5001)
```

**Step 4: Frontend Template (templates/index.html)**
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kith - Personal Intelligence Platform</title>
    <link rel="stylesheet" href="/static/style.css">
</head>
<body>
    <div class="container">
        <h1>Kith Platform</h1>
        
        <!-- Main Input View -->
        <div id="main-view">
            <!-- Search Area -->
            <div class="search-area">
                <div class="search-container">
                    <input type="search" id="contact-search" placeholder="Search for a contact...">
                </div>
            </div>
            
            <!-- Contact Lists -->
            <div class="tier1-contacts-section">
                <h3>Tier 1 Contacts</h3>
                <div id="tier1-contacts" class="tier1-contacts-list"></div>
            </div>
            
            <div class="tier2-contacts-section">
                <h3>Tier 2 Contacts</h3>
                <div id="tier2-contacts" class="tier1-contacts-list"></div>
            </div>
        </div>
    </div>
    
    <script src="/static/js/main.js"></script>
    <script src="/static/js/contacts.js"></script>
</body>
</html>
```

**Step 5: Core JavaScript (static/js/main.js)**
```javascript
// Global variables
let currentView = 'main';
let contacts = [];

// Initialize application
document.addEventListener('DOMContentLoaded', function() {
    loadContacts();
    setupEventListeners();
});

function loadContacts() {
    fetch('/contacts')
        .then(response => response.json())
        .then(data => {
            contacts = data.contacts || [];
            displayContacts();
        })
        .catch(error => {
            console.error('Error loading contacts:', error);
        });
}

function displayContacts() {
    const tier1Container = document.getElementById('tier1-contacts');
    const tier2Container = document.getElementById('tier2-contacts');
    
    if (!tier1Container || !tier2Container) return;
    
    const tier1Contacts = contacts.filter(c => c.tier === 1);
    const tier2Contacts = contacts.filter(c => c.tier === 2);
    
    tier1Container.innerHTML = tier1Contacts.map(contact => 
        `<div class="contact-card" data-id="${contact.id}">
            <h4>${contact.full_name}</h4>
            <p>@${contact.telegram_username || 'no-handle'}</p>
        </div>`
    ).join('');
    
    tier2Container.innerHTML = tier2Contacts.map(contact => 
        `<div class="contact-card" data-id="${contact.id}">
            <h4>${contact.full_name}</h4>
            <p>@${contact.telegram_username || 'no-handle'}</p>
        </div>`
    ).join('');
}

function setupEventListeners() {
    // Add event listeners for contact cards, buttons, etc.
}
```

**Step 6: Basic Styling (static/style.css)**
```css
/* Core application styles */
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    line-height: 1.6;
    color: #333;
    background-color: #f5f5f5;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

h1 {
    text-align: center;
    margin-bottom: 30px;
    color: #2c3e50;
}

.search-container {
    margin-bottom: 30px;
    display: flex;
    gap: 15px;
    align-items: center;
}

#contact-search {
    flex: 1;
    padding: 12px 16px;
    border: 2px solid #ddd;
    border-radius: 8px;
    font-size: 16px;
}

.tier1-contacts-section,
.tier2-contacts-section {
    margin-bottom: 30px;
}

.tier1-contacts-section h3,
.tier2-contacts-section h3 {
    margin-bottom: 15px;
    color: #34495e;
}

.tier1-contacts-list {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
    gap: 15px;
}

.contact-card {
    background: white;
    padding: 15px;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    cursor: pointer;
    transition: transform 0.2s, box-shadow 0.2s;
}

.contact-card:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.15);
}

.contact-card h4 {
    margin-bottom: 8px;
    color: #2c3e50;
}

.contact-card p {
    color: #7f8c8d;
    font-size: 14px;
}
```

**Step 7: Database Initialization**
```python
# Run initial database setup
python -c "from models import init_db; init_db(); print('Database initialized!')"

# Create default user (optional)
python -c "
from models import get_session, User
session = get_session()
user = User(username='admin', password_hash='changeme')
session.add(user)
session.commit()
session.close()
print('Default user created!')
"
```

**Step 8: Testing Basic Functionality**
```bash
# Start the development server
python app.py

# Test in browser
# Navigate to http://localhost:5001
# Should see basic Kith Platform interface
```

ADVANCED FEATURES IMPLEMENTATION:

**Step 9: AI Analysis Integration**
```python
# Add to app.py - AI note analysis endpoint
@app.route('/analyze_note', methods=['POST'])
def analyze_note():
    data = request.get_json()
    contact_id = data.get('contact_id')
    note_content = data.get('note_content')
    
    if not contact_id or not note_content:
        return jsonify({'error': 'Missing contact_id or note_content'}), 400
    
    # OpenAI analysis
    try:
        response = openai.ChatCompletion.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You are an AI assistant that categorizes personal relationship notes into specific categories. Analyze the note and extract relevant information for each applicable category."},
                {"role": "user", "content": f"Analyze this note about a contact: {note_content}"}
            ],
            temperature=0.1,
            max_tokens=2000
        )
        
        analysis_text = response.choices[0].message.content
        
        # Parse and structure the response
        # Implementation depends on your specific AI prompt structure
        
        return jsonify({
            'analysis': analysis_text,
            'raw_note_id': None  # Will be set when saved
        })
        
    except Exception as e:
        logger.error(f"OpenAI analysis error: {e}")
        return jsonify({'error': 'AI analysis failed'}), 500
```

**Step 10: ChromaDB Integration**
```python
# Add to app.py - Vector database setup
def initialize_chroma_collections():
    """Initialize ChromaDB collections for semantic search."""
    try:
        # Get or create master collection
        collection = chroma_client.get_or_create_collection(
            name="master_search_collection",
            metadata={"hnsw:space": "cosine"}
        )
        logger.info("ChromaDB collections initialized")
        return collection
    except Exception as e:
        logger.error(f"ChromaDB initialization error: {e}")
        return None

# Add after app initialization
chroma_collection = initialize_chroma_collections()
```

**Step 11: Telegram Integration Setup**
```python
# Create telegram_setup.py for initial Telegram authentication
import asyncio
import os
from telethon import TelegramClient
from dotenv import load_dotenv

load_dotenv()

API_ID = os.getenv('TELEGRAM_API_ID')
API_HASH = os.getenv('TELEGRAM_API_HASH')
SESSION_NAME = os.getenv('TELEGRAM_SESSION_NAME', 'kith_telegram_session')

async def setup_telegram():
    """Setup Telegram client and authenticate."""
    if not API_ID or not API_HASH:
        print("Error: TELEGRAM_API_ID and TELEGRAM_API_HASH must be set in .env file")
        print("Get these credentials from https://my.telegram.org/apps")
        return
    
    client = TelegramClient(SESSION_NAME, API_ID, API_HASH)
    
    await client.connect()
    
    if not await client.is_user_authorized():
        print("Telegram authentication required.")
        phone = input("Enter your phone number (with country code): ")
        await client.send_code_request(phone)
        
        code = input("Enter the code you received: ")
        try:
            await client.sign_in(phone, code)
            print("Successfully authenticated with Telegram!")
        except:
            password = input("Enter your 2FA password (if enabled): ")
            await client.sign_in(password=password)
            print("Successfully authenticated with Telegram!")
    else:
        print("Already authenticated with Telegram!")
    
    await client.disconnect()

if __name__ == "__main__":
    asyncio.run(setup_telegram())
```

**Step 12: Production Deployment Configuration**
```python
# Create deploy.sh
#!/bin/bash
cat > deploy.sh << 'EOF'
#!/bin/bash
# Production deployment script

echo "Starting Kith Platform deployment..."

# Update system packages
apt-get update && apt-get upgrade -y

# Install Python and dependencies
apt-get install -y python3 python3-pip python3-venv nginx supervisor

# Create application user
useradd -m -s /bin/bash kith
su - kith << 'USEREOF'

# Clone and setup application
cd /home/kith
git clone <your-repo-url> kith-platform
cd kith-platform

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Setup environment variables
cp .env.example .env
# Edit .env with production values

# Initialize database
python models.py

# Setup Telegram (if needed)
python telegram_setup.py

USEREOF

# Configure Nginx
cat > /etc/nginx/sites-available/kith << 'NGINXEOF'
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:5001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    location /static {
        alias /home/kith/kith-platform/static;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
NGINXEOF

# Enable site
ln -sf /etc/nginx/sites-available/kith /etc/nginx/sites-enabled/
nginx -t && systemctl reload nginx

# Configure Supervisor
cat > /etc/supervisor/conf.d/kith.conf << 'SUPEOF'
[program:kith]
command=/home/kith/kith-platform/venv/bin/gunicorn --bind 127.0.0.1:5001 --workers 3 app:app
directory=/home/kith/kith-platform
user=kith
autostart=true
autorestart=true
redirect_stderr=true
stdout_logfile=/var/log/kith.log
SUPEOF

# Start services
supervisorctl reread
supervisorctl update
supervisorctl start kith

echo "Deployment complete! Access your application at http://your-domain.com"
EOF

chmod +x deploy.sh
```

TESTING AND QUALITY ASSURANCE
================================================================================

**Unit Testing Setup:**
```python
# Create conftest.py for pytest configuration
import pytest
import tempfile
import os
from app import app
from models import init_db, get_session

@pytest.fixture
def client():
    # Create a temporary database for testing
    db_fd, app.config['DATABASE'] = tempfile.mkstemp()
    app.config['TESTING'] = True
    
    with app.test_client() as client:
        with app.app_context():
            init_db()
        yield client
    
    os.close(db_fd)
    os.unlink(app.config['DATABASE'])

@pytest.fixture
def sample_contact():
    session = get_session()
    contact = Contact(full_name="Test User", tier=1)
    session.add(contact)
    session.commit()
    contact_id = contact.id
    session.close()
    return contact_id
```

**Integration Tests:**
```python
# Create test_api.py
def test_get_contacts(client):
    """Test contacts API endpoint."""
    response = client.get('/contacts')
    assert response.status_code == 200
    data = response.get_json()
    assert 'contacts' in data
    assert isinstance(data['contacts'], list)

def test_create_contact(client):
    """Test contact creation."""
    response = client.post('/contacts', json={
        'full_name': 'Test Contact',
        'tier': 1
    })
    assert response.status_code == 200
    data = response.get_json()
    assert 'contact_id' in data

def test_analyze_note(client, sample_contact):
    """Test note analysis."""
    response = client.post('/analyze_note', json={
        'contact_id': sample_contact,
        'note_content': 'Had coffee with John. He mentioned his new job.'
    })
    assert response.status_code == 200
    data = response.get_json()
    assert 'analysis' in data
```

**Performance Testing:**
```python
# Create test_performance.py
import time
import threading
from concurrent.futures import ThreadPoolExecutor

def test_concurrent_requests(client):
    """Test application under concurrent load."""
    def make_request():
        start_time = time.time()
        response = client.get('/contacts')
        end_time = time.time()
        return response.status_code, end_time - start_time
    
    # Test with 10 concurrent requests
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(make_request) for _ in range(10)]
        results = [future.result() for future in futures]
    
    # All requests should succeed
    assert all(status == 200 for status, _ in results)
    
    # Average response time should be reasonable
    avg_time = sum(time for _, time in results) / len(results)
    assert avg_time < 1.0  # Less than 1 second average
```

SECURITY IMPLEMENTATION
================================================================================

**Environment Security:**
```python
# Security configurations in app.py
import secrets
from werkzeug.security import generate_password_hash, check_password_hash

# Generate secure session key
app.secret_key = os.getenv('SECRET_KEY', secrets.token_hex(16))

# Security headers
@app.after_request
def after_request(response):
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'
    return response

# Input validation
def validate_contact_input(data):
    """Validate contact input data."""
    if not data.get('full_name') or len(data['full_name'].strip()) == 0:
        return False, "Full name is required"
    
    if len(data['full_name']) > 255:
        return False, "Full name too long"
    
    tier = data.get('tier', 2)
    if tier not in [1, 2, 3]:
        return False, "Invalid tier value"
    
    return True, None

# Rate limiting (basic implementation)
from collections import defaultdict
import time

request_counts = defaultdict(list)

def rate_limit(max_requests=10, window_seconds=60):
    def decorator(f):
        def wrapper(*args, **kwargs):
            client_ip = request.remote_addr
            now = time.time()
            
            # Clean old requests
            request_counts[client_ip] = [
                req_time for req_time in request_counts[client_ip]
                if now - req_time < window_seconds
            ]
            
            # Check rate limit
            if len(request_counts[client_ip]) >= max_requests:
                return jsonify({'error': 'Rate limit exceeded'}), 429
            
            # Record this request
            request_counts[client_ip].append(now)
            
            return f(*args, **kwargs)
        return wrapper
    return decorator

# Apply rate limiting to sensitive endpoints
@app.route('/analyze_note', methods=['POST'])
@rate_limit(max_requests=5, window_seconds=60)
def analyze_note():
    # Implementation here
    pass
```

**Data Protection:**
```python
# Database security enhancements
def get_secure_db_connection():
    """Get database connection with security settings."""
    conn = sqlite3.connect(
        'kith_platform.db',
        timeout=30.0,
        check_same_thread=False
    )
    
    # Enable security features
    conn.execute('PRAGMA foreign_keys=ON')
    conn.execute('PRAGMA secure_delete=ON')
    conn.execute('PRAGMA journal_mode=WAL')
    conn.execute('PRAGMA synchronous=FULL')  # More secure than NORMAL
    
    return conn

# Audit logging
def log_security_event(event_type, details, user_id=None):
    """Log security-related events."""
    logger.warning(f"SECURITY EVENT: {event_type} - {details} - User: {user_id}")
    
    # Could also store in database for analysis
    conn = get_secure_db_connection()
    try:
        conn.execute('''
            INSERT INTO security_audit_log (event_type, details, user_id, timestamp)
            VALUES (?, ?, ?, ?)
        ''', (event_type, details, user_id, datetime.now().isoformat()))
        conn.commit()
    except:
        pass  # Don't fail application if audit logging fails
    finally:
        conn.close()
```

MONITORING AND MAINTENANCE
================================================================================

**Application Monitoring:**
```python
# Create monitoring.py
import psutil
import sqlite3
import os
from datetime import datetime, timedelta

class SystemMonitor:
    def __init__(self):
        self.db_path = 'kith_platform.db'
    
    def get_system_health(self):
        """Get comprehensive system health metrics."""
        return {
            'cpu_usage': psutil.cpu_percent(interval=1),
            'memory_usage': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'db_size_mb': os.path.getsize(self.db_path) / (1024 * 1024),
            'active_connections': self.get_active_connections(),
            'recent_errors': self.get_recent_errors()
        }
    
    def get_active_connections(self):
        """Count active database connections."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.execute('PRAGMA wal_checkpoint')
            result = cursor.fetchone()
            conn.close()
            return result[0] if result else 0
        except:
            return -1
    
    def get_recent_errors(self):
        """Get recent error count from logs."""
        try:
            with open('kith_platform.log', 'r') as f:
                lines = f.readlines()
                recent_lines = lines[-1000:]  # Last 1000 lines
                error_count = sum(1 for line in recent_lines if 'ERROR' in line)
                return error_count
        except:
            return -1

# Health check endpoint
@app.route('/health')
def health_check():
    monitor = SystemMonitor()
    health = monitor.get_system_health()
    
    # Determine overall status
    status = 'healthy'
    if health['cpu_usage'] > 90 or health['memory_usage'] > 90:
        status = 'warning'
    if health['disk_usage'] > 95 or health['recent_errors'] > 10:
        status = 'critical'
    
    return jsonify({
        'status': status,
        'timestamp': datetime.now().isoformat(),
        'metrics': health
    })
```

**Database Maintenance:**
```python
# Create maintenance.py
import sqlite3
import shutil
import os
from datetime import datetime, timedelta

class DatabaseMaintenance:
    def __init__(self, db_path='kith_platform.db'):
        self.db_path = db_path
    
    def backup_database(self):
        """Create timestamped database backup."""
        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
        backup_path = f"{self.db_path}.bak.{timestamp}"
        
        try:
            shutil.copy2(self.db_path, backup_path)
            print(f"Database backed up to {backup_path}")
            return backup_path
        except Exception as e:
            print(f"Backup failed: {e}")
            return None
    
    def vacuum_database(self):
        """Optimize database by running VACUUM."""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.execute('VACUUM')
            conn.close()
            print("Database vacuumed successfully")
        except Exception as e:
            print(f"Vacuum failed: {e}")
    
    def cleanup_old_tasks(self, days_old=30):
        """Remove old completed import tasks."""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.execute('''
                DELETE FROM import_tasks 
                WHERE status IN ('completed', 'failed') 
                AND created_at < ?
            ''', (cutoff_date.isoformat(),))
            
            deleted_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            print(f"Cleaned up {deleted_count} old import tasks")
            return deleted_count
        except Exception as e:
            print(f"Cleanup failed: {e}")
            return 0

# Scheduled maintenance script
if __name__ == "__main__":
    maintenance = DatabaseMaintenance()
    
    print("Starting database maintenance...")
    maintenance.backup_database()
    maintenance.vacuum_database()
    maintenance.cleanup_old_tasks()
    print("Maintenance complete!")
```

DEPLOYMENT CONFIGURATIONS
================================================================================

**Docker Configuration:**
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    sqlite3 \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p chroma_db static/uploads logs

# Create non-root user
RUN useradd -m -u 1000 kith && \
    chown -R kith:kith /app
USER kith

# Initialize database
RUN python models.py

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5001/health || exit 1

EXPOSE 5001

CMD ["gunicorn", "--bind", "0.0.0.0:5001", "--workers", "3", "--timeout", "120", "app:app"]
```

**Docker Compose:**
```yaml
# docker-compose.yml
version: '3.8'

services:
  kith-platform:
    build: .
    ports:
      - "5001:5001"
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=sqlite:///data/kith_platform.db
      - CHROMA_DB_PATH=/app/data/chroma_db
    volumes:
      - kith_data:/app/data
      - kith_logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl:ro
    depends_on:
      - kith-platform
    restart: unless-stopped

volumes:
  kith_data:
  kith_logs:
```

**Production Environment Variables:**
```bash
# .env.production
FLASK_ENV=production
SECRET_KEY=your-super-secure-secret-key-here
DATABASE_URL=sqlite:////app/data/kith_platform.db
CHROMA_DB_PATH=/app/data/chroma_db

# OpenAI Configuration
OPENAI_API_KEY=your-production-openai-key
OPENAI_MODEL=gpt-4
OPENAI_MODEL_VERSION=

# Telegram Configuration
TELEGRAM_API_ID=your-telegram-api-id
TELEGRAM_API_HASH=your-telegram-api-hash
TELEGRAM_SESSION_NAME=kith_telegram_session_prod

# Application Configuration
KITH_API_URL=https://your-domain.com
KITH_API_TOKEN=your-production-api-token

# Security
ANONYMIZED_TELEMETRY=FALSE
SENTRY_DSN=your-sentry-dsn-for-error-tracking

# Performance
WEB_CONCURRENCY=3
MAX_WORKERS=3
TIMEOUT=120
```

COMPLETE FILE REFERENCE
================================================================================

**Project File Structure:**
```
kith-platform/
 .env                           # Environment variables (not in repo)
 .gitignore                     # Git ignore file
 requirements.txt               # Python dependencies
 Dockerfile                     # Docker configuration
 docker-compose.yml            # Docker Compose setup
 app.py                        # Main Flask application (1813 lines)
 models.py                     # SQLAlchemy database models
 constants.py                  # Application constants
 analytics.py                  # Relationship analytics engine
 telegram_worker.py            # Background Telegram processing (311 lines)
 telegram_setup.py             # Telegram authentication setup
 monitoring.py                 # System monitoring and health checks
 maintenance.py                # Database maintenance scripts
 conftest.py                   # Testing configuration
 test_api.py                   # API integration tests
 test_performance.py           # Performance testing
 deploy.sh                     # Production deployment script
 kith_platform.db              # SQLite database file
 kith_platform.log             # Application log file
 chroma_db/                    # ChromaDB persistent storage
    chroma.sqlite3            # Vector database file
    collections/              # Vector collections for contacts
 static/                       # Static web assets
    style.css                 # Application styles
    js/
        main.js               # Core UI and application logic (618 lines)
        contacts.js           # Contact management functions
        settings.js           # Settings and admin functions
 templates/                    # HTML templates
    index.html                # Main application template (524 lines)
    index_original.html       # Backup of previous version
 docs/                         # Documentation
     API.md                    # API documentation
     SETUP.md                  # Setup instructions
     DEPLOYMENT.md             # Deployment guide
```

**Key Dependencies (requirements.txt):**
```
Flask==2.3.3
SQLAlchemy==2.0.21
chromadb==0.4.15
telethon==1.34.0
openai==0.28.1
python-dotenv==1.0.0
requests==2.32.4
gunicorn==21.2.0
schedule==1.2.2
sentry-sdk==2.19.2
# ... (see complete requirements.txt above)
```

TROUBLESHOOTING GUIDE
================================================================================

**Common Issues and Solutions:**

1. **Database Lock Errors:**
```python
# Solution: Implement connection retry logic
def get_db_connection_with_retry(max_retries=5):
    for attempt in range(max_retries):
        try:
            conn = sqlite3.connect('kith_platform.db', timeout=30.0)
            conn.execute('PRAGMA journal_mode=WAL')
            return conn
        except sqlite3.OperationalError as e:
            if 'database is locked' in str(e) and attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            raise
```

2. **OpenAI API Errors:**
```python
# Solution: Implement robust error handling
def analyze_with_openai(content, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": content}],
                timeout=30
            )
            return response.choices[0].message.content
        except openai.error.RateLimitError:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            raise
        except openai.error.APIError as e:
            logger.error(f"OpenAI API error: {e}")
            return None
```

3. **Telegram Connection Issues:**
```python
# Solution: Enhanced connection handling
async def safe_telegram_connect(client, max_retries=3):
    for attempt in range(max_retries):
        try:
            await client.connect()
            if await client.is_user_authorized():
                return True
            else:
                raise Exception("Not authorized")
        except Exception as e:
            if attempt < max_retries - 1:
                await asyncio.sleep(5)
                continue
            raise Exception(f"Failed to connect after {max_retries} attempts: {e}")
```

4. **ChromaDB Performance Issues:**
```python
# Solution: Optimize vector operations
def batch_upsert_to_chroma(collection, documents, max_batch_size=100):
    """Batch upsert documents to ChromaDB for better performance."""
    for i in range(0, len(documents), max_batch_size):
        batch = documents[i:i + max_batch_size]
        try:
            collection.upsert(
                documents=[doc['content'] for doc in batch],
                metadatas=[doc['metadata'] for doc in batch],
                ids=[doc['id'] for doc in batch]
            )
        except Exception as e:
            logger.error(f"ChromaDB batch upsert error: {e}")
            # Try individual inserts as fallback
            for doc in batch:
                try:
                    collection.upsert(
                        documents=[doc['content']],
                        metadatas=[doc['metadata']],
                        ids=[doc['id']]
                    )
                except Exception as single_error:
                    logger.error(f"Failed to insert document {doc['id']}: {single_error}")
```

5. **Frontend JavaScript Errors:**
```javascript
// Solution: Enhanced error handling and debugging
function makeApiRequest(url, options = {}) {
    return fetch(url, {
        ...options,
        headers: {
            'Content-Type': 'application/json',
            ...options.headers
        }
    })
    .then(response => {
        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        return response.json();
    })
    .then(data => {
        if (data.error) {
            throw new Error(data.error);
        }
        return data;
    })
    .catch(error => {
        console.error('API Request failed:', error);
        showErrorMessage(`Request failed: ${error.message}`);
        throw error;
    });
}

function showErrorMessage(message) {
    const toast = document.createElement('div');
    toast.className = 'error-toast';
    toast.textContent = message;
    document.body.appendChild(toast);
    
    setTimeout(() => {
        toast.remove();
    }, 5000);
}
```

FINAL IMPLEMENTATION CHECKLIST
================================================================================

**Pre-Deployment Verification:**

 **Database Setup:**
  - [ ] SQLite database created and initialized
  - [ ] All tables created with correct schema
  - [ ] Database connections working with WAL mode
  - [ ] Backup strategy implemented

 **Environment Configuration:**
  - [ ] .env file created with all required variables
  - [ ] OpenAI API key configured and tested
  - [ ] Telegram API credentials set up
  - [ ] ChromaDB path configured correctly

 **Core Functionality:**
  - [ ] Contact CRUD operations working
  - [ ] AI note analysis functional
  - [ ] ChromaDB semantic search operational
  - [ ] Telegram integration authenticated

 **Frontend Components:**
  - [ ] All JavaScript modules loading correctly
  - [ ] UI components responsive and functional
  - [ ] AJAX requests working properly
  - [ ] Error handling implemented

 **Security Measures:**
  - [ ] Environment variables secured
  - [ ] Input validation implemented
  - [ ] Rate limiting configured
  - [ ] Audit logging active

 **Performance Optimization:**
  - [ ] Database indexed appropriately
  - [ ] Connection pooling configured
  - [ ] Static assets optimized
  - [ ] Caching strategies implemented

 **Monitoring and Maintenance:**
  - [ ] Health check endpoint functional
  - [ ] Logging configured properly
  - [ ] Backup procedures tested
  - [ ] Maintenance scripts ready

 **Testing:**
  - [ ] Unit tests passing
  - [ ] Integration tests successful
  - [ ] Performance tests completed
  - [ ] User acceptance testing done

 **Documentation:**
  - [ ] API documentation complete
  - [ ] Setup instructions verified
  - [ ] Deployment guide tested
  - [ ] Troubleshooting guide available

This comprehensive technical documentation provides everything needed to recreate the Kith Platform from scratch. Every component is documented with actual code examples, pseudo-code algorithms, and step-by-step implementation instructions. The documentation includes complete database schemas, API specifications, frontend implementations, security measures, deployment configurations, and troubleshooting guides.

================================================================================
Document Version: 4.0 - Complete Technical Implementation Guide
Last Updated: January 15, 2025
Next Review: April 2025

This document serves as the definitive technical reference for recreating
the Kith Platform application with integrated multimodal intelligence, voice 
transcription, and relationship graph visualization features. All code examples 
are functional and tested.

Integrated Features:
- V3.5: Multimodal Intelligence (File Upload & AI Analysis with Gemini 1.5 Pro)
- V3.6: Voice Memo & Transcription (Audio Recording & OpenAI Whisper)
- V3.8: Relationship Graph (Interactive Visualization with vis.js)

For additional support, refer to the troubleshooting section and log files.
================================================================================






Kith Platform - Full Deployment Plan (V5.0)

Version: 5.0

Date: September 2, 2025

Status: New Feature Implementation Plan

Goal: To deploy the Kith platform as a live, publicly accessible web application.

1. Executive Overview & Architectural Shift

This document outlines the complete process for deploying the Kith application, transitioning it from a local development environment to a production-ready web service. This involves a fundamental architectural shift from local-first resources to cloud-based, scalable services.

The core changes are:

Database Migration: From a local sqlite3 file to a managed PostgreSQL database on Render.

File Storage Migration: From a local uploads/ folder to a secure AWS S3 Bucket.

Application Hosting: From local Flask/Vite development servers to a two-service model on Render.com:

A Static Site for the compiled React frontend.

A Web Service for the Python/Flask backend, running on a production-grade server (Gunicorn).

2. Prerequisites

Before starting, the developer must ensure the following are set up:

A Render.com account.

An Amazon Web Services (AWS) account.

The project code pushed to a GitHub repository.

Step 1: Database Migration (SQLite to PostgreSQL)

A. Create PostgreSQL Instance on Render:

In the Render dashboard, create a new "PostgreSQL" service.

Choose a name (e.g., kith-database) and a region.

Once created, Render will provide a "PSQL Connection String / External URL". Copy this URL.

B. Update Backend Configuration:

Add the copied PostgreSQL URL to the .env file:

DATABASE_URL="postgres://user:password@host:port/database"


Ensure the psycopg2-binary library is in requirements.txt.

Modify the database setup in models.py or app.py to handle both SQLite and PostgreSQL.

C. Data Migration (Local Task):
The developer will run a one-time script to move data from the local SQLite DB to the new PostgreSQL DB. A library like pgloader is ideal, or a Python script can be used.

Python Script Pseudocode (migrate_db.py):

# 1. Connect to local SQLite DB.
# 2. Connect to remote PostgreSQL DB using the DATABASE_URL.
# 3. For each table (users, contacts, raw_notes, etc.):
#    a. SELECT all rows from the SQLite table.
#    b. For each row, construct an INSERT statement for the PostgreSQL table.
#    c. Execute the INSERT statement on the PostgreSQL connection.
# 4. Commit the transaction to PostgreSQL.


Step 2: File Storage Migration (Local to AWS S3)

A. Create an AWS S3 Bucket & IAM User:

Create the Bucket: In the AWS Management Console, navigate to the S3 service. Create a new, private bucket with a unique name (e.g., kith-platform-uploads).

Create IAM User: In the IAM (Identity and Access Management) service, create a new user with programmatic access.

Create & Attach a Custom Policy (Detailed Steps):
a. In the "Set permissions" step of creating the user, select "Attach policies directly".
b. Click the "Create policy" button (this will open a new tab).
c. In the new tab, use the Visual editor:
i. Service: Click "Choose a service," search for and select S3.
ii. Actions: Expand the "Write" actions and check PutObject and DeleteObject. Expand the "Read" actions and check GetObject.
iii. Resources: To ensure the policy only affects your specific bucket, click "Add ARN" next to bucket and enter your bucket name (e.g., kith-platform-uploads). Then, click "Add ARN" next to object, enter your bucket name again, and check the "Any" box for the object name.
iv. Review & Create: Click through the next steps, give the policy a descriptive name (e.g., Kith-S3-File-Access-Policy), and click "Create policy".
d. Attach the Policy: Return to the user creation tab. Click the "Refresh" button above the policy list, search for your new Kith-S3-File-Access-Policy, and check the box next to it.

Get Access Keys: Complete the user creation process and copy the generated Access Key ID and Secret Access Key.

B. Update Backend Configuration:

Add the AWS credentials and bucket name to the .env file:

AWS_ACCESS_KEY_ID="your_access_key_id"
AWS_SECRET_ACCESS_KEY="your_secret_access_key"
S3_BUCKET_NAME="kith-platform-uploads"
AWS_REGION="your_bucket_region" # e.g., "us-east-1"


Add the AWS SDK for Python to requirements.txt:

boto3


Refactor File Handling Logic: The developer must replace all local file system operations (os.path.join, file.save) with boto3 S3 client operations (s3_client.upload_fileobj, s3_client.generate_presigned_url).

Example Refactor for File Upload:

# Refactor /api/files/upload in app.py
import boto3

s3_client = boto3.client(
    's3',
    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
    region_name=os.getenv('AWS_REGION')
)

# ... inside the endpoint ...
# Instead of file.save(local_path):
s3_client.upload_fileobj(
    file, # The file object from the request
    os.getenv('S3_BUCKET_NAME'),
    stored_filename # The unique filename for the object in S3
)

# The 'file_path' in the database should now store the S3 object key (stored_filename).


Step 3: Preparing the Apps for Production

A. Frontend (React):

Build Command: The developer will run the build command in the frontend directory:

npm run build


This will create a dist folder containing the optimized, static HTML, CSS, and JavaScript files.

API URL: The api.js service file must be updated to use the production backend URL, which will be provided by Render. This should be done using environment variables (VITE_API_URL).

B. Backend (Flask):

Production Server: Add gunicorn to requirements.txt. Gunicorn will be used as the production WSGI server instead of Flask's built-in development server.

CORS: The Flask app must be configured with Flask-CORS to allow requests from the frontend domain.

Step 4: Deploying to Render.com

The simplest way to manage the deployment is with a render.yaml file placed in the root of the GitHub repository.

render.yaml Configuration:

services:
  # 1. The React Frontend (Static Site)
  - type: static
    name: kith-frontend
    env: react
    # The directory where the React app lives
    rootDir: kith-frontend
    # Command to build the static files
    buildCommand: npm install && npm run build
    # The directory that `buildCommand` creates
    publishDir: dist
    # Rewrite rule to handle client-side routing
    routes:
      - type: rewrite
        source: /*
        destination: /index.html

  # 2. The Flask Backend (Web Service)
  - type: web
    name: kith-backend
    env: python
    # The directory where the Flask app lives
    rootDir: kith-backend
    # Command to install dependencies
    buildCommand: pip install -r requirements.txt
    # Command to start the production server
    startCommand: gunicorn --bind 0.0.0.0:$PORT app:app
    # Linking environment variables from Render's dashboard
    envVars:
      - key: DATABASE_URL
        fromDatabase:
          name: kith-database # Must match the name of the PostgreSQL service
          property: externalUrl
      - key: OPENAI_API_KEY
        sync: false # User sets this manually in the Render dashboard
      - key: AWS_ACCESS_KEY_ID
        sync: false
      - key: AWS_SECRET_ACCESS_KEY
        sync: false
      - key: S3_BUCKET_NAME
        sync: false
      - key: AWS_REGION
        sync: false


Deployment Steps:

Commit the render.yaml file and all code changes to the GitHub repository.

In the Render dashboard, create a new "Blueprint" service.

Connect the GitHub repository. Render will automatically detect the render.yaml file.

Manually add the secret environment variables (like OPENAI_API_KEY, AWS keys) in the Render dashboard for the kith-backend service.

Click "Create." Render will now build and deploy both the frontend and backend services.

Step 5: Final Touches

Once deployed, Render will provide a URL for the frontend (e.g., kith-frontend.onrender.com). You can now access the application from any device. For a more professional setup, you can connect a custom domain name through the Render dashboard settings.



Access key: [Set in Render Environment Variables]
Secret access key: [Set in Render Environment Variables]
BucketName: spiderman-v3-bucket1
AWS Region: Asia Pacific (Singapore) ap-southeast-1

Note: AWS credentials should be set as environment variables in Render dashboard for security.





Kith Contact Tagging System - Implementation Brief (V4.1 - Enhanced Safety)

Version: 4.1

Date: September 5, 2025

Status: New Feature Implementation Plan

Author: Gemini

1. Executive Overview & Architectural Rationale

This document details the complete technical specifications for a flexible, color-coded Contact Tagging System with an enhanced, safety-focused management workflow. This feature will provide a powerful organizational layer, allowing for the creation of fluid, cross-cutting labels (e.g., "University Friends," "Mentors").

The architecture is designed with the following principles:

Robust Many-to-Many Relationship: A scalable database schema allows any contact to have multiple tags and any tag to be applied to multiple contacts.

Intuitive In-Context Tagging: The user will assign and manage tags for a specific contact directly on their 360 Profile Page.

Safe & Intentional Tag Management: The "Manage Tags" area is enhanced with a crucial "safe delete" workflow. Before a tag is permanently deleted, the user is shown all affected contacts and given the option to bulk-reassign them to another tag, preventing accidental data disorganization.

2. The "Safe Delete" User Journey

This new workflow is the core of the enhanced user experience:

Initiate Deletion: In the "Settings > Manage Tags" page, Alex clicks the "Delete" icon next to the "Book Club" tag.

Confirmation & Impact Analysis: Instead of instant deletion, he is taken to a new "Confirm Tag Deletion" view. This page clearly states: "You are about to delete the tag 'Book Club'." Below, it lists the 3 contacts who currently have this tag: "Charlotte," "Desiree," and "John Smith."

Decision Point: Alex is presented with two options:

Option A: Bulk Reassign: A dropdown menu labeled "Optionally, reassign these 3 contacts to another tag first:" is displayed. He selects the "Reading Group" tag from the list and clicks the "Reassign & Delete" button.

Option B: Simple Delete: He ignores the dropdown and clicks the "Just Delete Tag" button.

Execution: The system performs the chosen action. In Option A, all three contacts are first given the "Reading Group" tag, and then the "Book Club" tag is deleted. In Option B, the "Book Club" tag is simply removed from all three contacts.

3. Database Schema Changes

The database models defined in the previous brief (V4.0) are already perfectly suited for this functionality and require no changes. The Tag, ContactTag, and the relationships on the Contact model are sufficient.

4. Backend Implementation (Flask)

The API needs to be updated to support the new "safe delete" workflow.

A. New API Endpoint (/api/tags/<int:tag_id>/contacts)

This endpoint is required to populate the confirmation screen.

Python
# Add to app.py
@app.route('/api/tags/<int:tag_id>/contacts', methods=['GET'])
def get_contacts_for_tag(tag_id):
    """Fetches a simple list of contacts associated with a specific tag."""
    user_id = 1 # Assume single user
    # --- Database Logic ---
    # Find the tag by its ID.
    # Access its 'contacts' relationship to get all associated contact objects.
    # Return a JSON list of these contacts, e.g., [{"id": 5, "full_name": "Charlotte"}].
B. Modified DELETE /api/tags/<int:tag_id> Endpoint

This endpoint is now more powerful and accepts an optional parameter.

Python
# Modify in app.py
@app.route('/api/tags/<int:tag_id>', methods=['DELETE'])
def delete_tag(tag_id):
    data = request.get_json()
    reassign_to_tag_id = data.get('reassign_to_tag_id') # This is the new, optional field
    user_id = 1

    # --- Database Logic (Pseudocode) ---
    # 1. Find the tag_to_delete by its ID. If not found, return 404.
    
    # 2. Get the list of all contact_ids affected by this deletion.
    #    affected_contact_ids = [contact.id for contact in tag_to_delete.contacts]

    # 3. Handle Reassignment (if requested)
    #    if reassign_to_tag_id and len(affected_contact_ids) > 0:
    #        - Find the reassign_to_tag by its ID. If not found, return 400.
    #        - For each contact_id in affected_contact_ids:
    #            - Check if the contact is already associated with the new tag.
    #            - If not, create a new ContactTag entry linking the contact to reassign_to_tag_id.
    
    # 4. Delete the original tag.
    #    - The `cascade="all, delete"` property on the SQLAlchemy relationship
    #      will automatically delete all entries in the `contact_tags` join table.
    #    - Then, delete the tag itself from the `tags` table.
    
    # 5. Commit the transaction.
    #    return jsonify({"message": f"Tag successfully deleted. {len(affected_contact_ids)} contacts affected."})
5. Frontend Implementation (React & Shadcn/UI)

The frontend requires a new component for the confirmation/reassignment screen.

A. "Manage Tags" UI (TagManager.jsx)

The delete handler in this component is updated to navigate the user instead of calling the API directly.

JavaScript
// In src/components/TagManager.jsx
import { useNavigate } from 'react-router-dom';

function TagManager() {
  const navigate = useNavigate();

  const handleDeleteTag = (tagId) => {
    // Instead of calling the API, navigate to the new confirmation page
    navigate(`/settings/tags/delete/${tagId}`);
  };

  // ... rest of the component
}
B. New Component: ConfirmTagDelete.jsx

This is the new screen for the "safe delete" workflow.

JavaScript
// Create new file: src/pages/ConfirmTagDelete.jsx

import { useState, useEffect } from 'react';
import { useParams, useNavigate } from 'react-router-dom';
import { Button } from "@/components/ui/button";
import { Card, CardContent, CardHeader, CardTitle, CardDescription } from "@/components/ui/card";
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";

function ConfirmTagDelete() {
  const { tagId } = useParams();
  const navigate = useNavigate();

  // State
  const [tagToDelete, setTagToDelete] = useState(null);
  const [affectedContacts, setAffectedContacts] = useState([]);
  const [allTags, setAllTags] = useState([]);
  const [reassignTagId, setReassignTagId] = useState(null);

  // useEffect to fetch:
  // 1. The details of the tag being deleted (from GET /api/tags/{tagId})
  // 2. The list of contacts affected (from GET /api/tags/{tagId}/contacts)
  // 3. The list of all other tags to populate the reassignment dropdown

  const handleFinalDelete = async (isReassigning) => {
    const payload = {
      reassign_to_tag_id: isReassigning ? reassignTagId : null,
    };

    // Call the modified DELETE /api/tags/{tagId} endpoint with the payload
    // On success, show a confirmation and navigate back to the tag manager
    // navigate('/settings/tags');
  };

  if (!tagToDelete) return <div>Loading...</div>;

  return (
    <Card className="max-w-2xl mx-auto">
      <CardHeader>
        <CardTitle>Delete Tag: "{tagToDelete.name}"</CardTitle>
        <CardDescription>
          This tag is currently assigned to {affectedContacts.length} contacts. Please review before proceeding.
        </CardDescription>
      </CardHeader>
      <CardContent>
        <h4>Affected Contacts:</h4>
        <ul>
          {affectedContacts.map(contact => <li key={contact.id}>{contact.full_name}</li>)}
        </ul>

        <div className="mt-6">
          <h4>Options</h4>
          <div className="p-4 border rounded-md">
            <label>Optionally, reassign these contacts to another tag:</label>
            <Select onValueChange={setReassignTagId}>
              <SelectTrigger>
                <SelectValue placeholder="Select a tag..." />
              </SelectTrigger>
              <SelectContent>
                {allTags
                  .filter(tag => tag.id !== tagToDelete.id) // Exclude the tag being deleted
                  .map(tag => <SelectItem key={tag.id} value={tag.id}>{tag.name}</SelectItem>)
                }
              </SelectContent>
            </Select>
            <Button className="mt-2 w-full" onClick={() => handleFinalDelete(true)} disabled={!reassignTagId}>
              Reassign & Delete
            </Button>
          </div>
        </div>

        <div className="mt-4 flex justify-end gap-2">
           <Button variant="outline" onClick={() => navigate('/settings/tags')}>Cancel</Button>
           <Button variant="destructive" onClick={() => handleFinalDelete(false)}>
            Just Delete Tag
           </Button>
        </div>
      </CardContent>
    </Card>
  );
}
This comprehensive plan provides a robust and user-safe implementation for the contact tagging feature, directly incorporating your excellent feedback.