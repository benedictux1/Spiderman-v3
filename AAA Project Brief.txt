================================================================================
KITH PLATFORM - COMPREHENSIVE TECHNICAL IMPLEMENTATION GUIDE
Project Brief - Advanced Personal Intelligence & Relationship Management Platform
Version: 5.0 - Complete Feature Documentation (September 2025)
================================================================================

PROJECT OVERVIEW
================================================================================

Project Name: Kith Platform
Version: 5.0 (Production Ready with Admin Dashboard & Advanced Analytics)
Status: Production Deployed with Full Feature Set
Type: Personal Intelligence & Relationship Management Platform
Primary Languages: Python (Flask), JavaScript (Vanilla), HTML5, CSS3
Database: SQLite (Development) / PostgreSQL (Production)
Deployment: Render.com with automated CI/CD

EXECUTIVE SUMMARY
================================================================================

The Kith Platform is a sophisticated personal intelligence system designed to help users organize, analyze, and derive insights from their personal relationships and social interactions. It transforms unstructured notes and conversation data into actionable relationship intelligence through AI-powered analysis.

CORE CAPABILITIES:
- **Multi-User Authentication System**: Role-based access with admin dashboard
- **AI-Powered Note Analysis**: Gemini 1.5 Pro integration for intelligent content processing
- **Voice Recording & Transcription**: OpenAI Whisper integration for voice memos
- **File Upload & Analysis**: Support for images, PDFs, and documents with AI extraction
- **Interactive Relationship Graph**: Advanced visualization with vis.js for network mapping
- **Telegram Integration**: Direct message import and analysis from Telegram chats
- **Advanced Analytics Engine**: Relationship health scoring and trend analysis
- **Calendar Integration**: Smart event creation from actionable items
- **Admin Dashboard**: Comprehensive user management and data administration
- **Tag Management System**: Organizational tools with color-coded categories
- **CSV Import/Export**: Bulk data management and backup functionality

TECHNICAL ARCHITECTURE BY FEATURE
================================================================================

1. DATABASE SCHEMA & DATA MODELS
================================================================================

The platform uses a sophisticated multi-table relational database design supporting multi-user architecture with comprehensive relationship tracking.

**Core Tables Structure:**

```python
# User Management (models.py:12-24)
class User(Base, UserMixin):
    __tablename__ = 'users'

    id = Column(Integer, primary_key=True)
    username = Column(String(255), unique=True, nullable=False)
    password_hash = Column(String(255), nullable=False)
    password_plaintext = Column(String(255), nullable=True)  # Admin viewing
    role = Column(String(50), nullable=False, default='user')  # 'user' or 'admin'
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    contacts = relationship("Contact", back_populates="user")
```

**Contact Management:**

```python
# Contact Entity with Telegram Integration (models.py:25-50)
class Contact(Base):
    __tablename__ = 'contacts'

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    full_name = Column(String(255), nullable=False)
    tier = Column(Integer, default=2, nullable=False)  # 1=inner circle, 2=outer
    vector_collection_id = Column(String(255), unique=True)

    # Telegram Integration Fields
    telegram_id = Column(String(255))               # Telegram user ID
    telegram_username = Column(String(255))         # @username handle
    telegram_phone = Column(String(255))            # Phone number
    telegram_handle = Column(String(255))           # User-provided identifier
    is_verified = Column(Boolean, default=False)    # Verified account status
    is_premium = Column(Boolean, default=False)     # Premium account status
    telegram_last_sync = Column(DateTime)           # Last sync timestamp

    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

**Note Storage System:**

```python
# Raw Notes Storage (models.py:51-62)
class RawNote(Base):
    __tablename__ = 'raw_notes'

    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'))
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    tags = Column(String)  # JSON string for SQLite compatibility

    contact = relationship("Contact", back_populates="raw_notes")

# AI-Processed Intelligence (models.py:63-75)
class SynthesizedEntry(Base):
    __tablename__ = 'synthesized_entries'

    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'))
    category = Column(String(255), nullable=False)  # e.g., "Goals", "Actionable"
    content = Column(Text, nullable=False)
    confidence_score = Column(Float)  # AI confidence rating
    created_at = Column(DateTime, default=datetime.utcnow)
```

**Advanced Features Tables:**

```python
# File Upload Management (models.py:94-114)
class UploadedFile(Base):
    __tablename__ = 'uploaded_files'

    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'))
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'))
    original_filename = Column(String(255), nullable=False)
    stored_filename = Column(String(255), nullable=False)
    file_path = Column(String(500), nullable=False)
    file_type = Column(String(100), nullable=False)
    file_size_bytes = Column(Integer, nullable=False)
    analysis_task_id = Column(String(255), ForeignKey('import_tasks.id'))
    generated_raw_note_id = Column(Integer, ForeignKey('raw_notes.id'))

# Task Management for Async Operations (models.py:76-93)
class ImportTask(Base):
    __tablename__ = 'import_tasks'

    id = Column(String(255), primary_key=True)  # UUID string
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    contact_id = Column(Integer, ForeignKey('contacts.id'))
    task_type = Column(String(50), default='telegram_import')
    status = Column(String(50), default='pending')  # pending, processing, completed, failed
    progress = Column(Integer, default=0)
    status_message = Column(Text)
    error_details = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
```

**Graph Visualization & Tag System:**

```python
# Contact Grouping (models.py:115-128)
class ContactGroup(Base):
    __tablename__ = 'contact_groups'
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'))
    name = Column(String(255), nullable=False)
    color = Column(String(7), default='#97C2FC')  # Hex color for visualization

# Relationship Mapping (models.py:129-138)
class ContactRelationship(Base):
    __tablename__ = 'contact_relationships'
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    source_contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'))
    target_contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'))
    label = Column(String(100))

# Tag Management System (models.py:139-166)
class Tag(Base):
    __tablename__ = 'tags'

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'))
    name = Column(String(255), nullable=False)
    color = Column(String(7), default='#97C2FC')
    description = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    contacts = relationship("Contact", secondary="contact_tags", back_populates="tags")
```

2. AUTHENTICATION & USER MANAGEMENT SYSTEM
================================================================================

The platform implements a comprehensive authentication system with role-based access control supporting both regular users and administrators.

**User Registration & Login:**

```python
# User Registration (app.py:110-138)
@app.route('/api/register', methods=['POST'])
def register():
    data = request.get_json()
    username = data.get('username', '').strip()
    password = data.get('password', '')

    # Validation
    if not username or not password:
        return jsonify({'error': 'Username and password required'}), 400

    if len(password) < 6:
        return jsonify({'error': 'Password must be at least 6 characters'}), 400

    session = get_session()
    try:
        # Check if user exists
        existing_user = session.query(User).filter_by(username=username).first()
        if existing_user:
            return jsonify({'error': 'Username already exists'}), 400

        # Create new user
        hashed_password = generate_password_hash(password, method='pbkdf2:sha256')
        user = User(
            username=username,
            password_hash=hashed_password,
            password_plaintext=password,  # Store for admin access
            role='user'  # Default role
        )
        session.add(user)
        session.commit()

        # Auto-login after registration
        login_user(user, remember=True)
        return jsonify({'message': 'Registration successful', 'user_id': user.id})
    finally:
        session.close()
```

**Admin Authorization System:**

```python
# Admin Access Control Decorator (app.py:102-109)
def admin_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not current_user.is_authenticated or getattr(current_user, 'role', 'user') != 'admin':
            return jsonify({
                'error': 'Admin access required',
                'redirect': '/login'
            }), 403
        return f(*args, **kwargs)
    return decorated_function
```

**Admin User Management:**

```python
# Admin User Administration (app.py:177-190)
@app.route('/admin/api/users', methods=['GET'])
@login_required
@admin_required
def admin_get_all_users():
    session = get_session()
    try:
        users = session.query(User).order_by(User.id.asc()).all()
        user_data = []
        for user in users:
            user_data.append({
                'id': user.id,
                'username': user.username,
                'role': getattr(user, 'role', 'user'),
                'created_at': user.created_at.isoformat() if user.created_at else None
            })
        return jsonify({'users': user_data})
    finally:
        session.close()
```

**Auto-Admin Seeding:**

```python
# Default Admin Creation (app.py:1251-1264)
# Automatic admin user creation on first startup
try:
    cur = conn.execute('SELECT COUNT(1) FROM users')
    (ucount,) = cur.fetchone()
    if ucount == 0:
        default_admin_user = os.getenv('DEFAULT_ADMIN_USER', 'admin')
        default_admin_pass = os.getenv('DEFAULT_ADMIN_PASS', 'admin123')
        hashed = generate_password_hash(default_admin_pass, method='pbkdf2:sha256')
        conn.execute('INSERT INTO users (username, password_hash, password_plaintext, role, created_at) VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)',
                    (default_admin_user, hashed, default_admin_pass, 'admin'))
        conn.commit()
        logger.info('‚úÖ Seeded default admin user')
```

3. ADMIN DASHBOARD SYSTEM
================================================================================

The admin dashboard provides comprehensive user management, data administration, and system monitoring capabilities.

**Dashboard Features:**
- Complete user management (create, edit, delete, role assignment)
- Contact data viewing and management for all users
- CSV import/export functionality for bulk operations
- Relationship graph visualization for any user
- Password viewing for support purposes
- System health monitoring

**Admin Dashboard Implementation:**

```python
# Admin Dashboard Route (app.py:288-293)
@app.route('/admin/dashboard', methods=['GET'])
@login_required
@admin_required
def admin_dashboard():
    return render_template('admin_dashboard.html')

# User Data Export (app.py:481-584)
@app.route('/admin/api/users/<int:user_id>/export/csv', methods=['GET'])
@login_required
@admin_required
def admin_export_user_csv(user_id):
    """Export a specific user's data as CSV (admin only)."""
    def generate_csv():
        output = StringIO()
        writer = csv.writer(output)

        # Write header
        writer.writerow([
            'Contact ID', 'Contact Name', 'Tier', 'Category', 'Content',
            'AI Confidence', 'Created At', 'Telegram ID', 'Telegram Username'
        ])

        session = get_session()
        try:
            # Get all contacts for user
            contacts = session.query(Contact).filter_by(user_id=user_id).all()

            for contact in contacts:
                # Get synthesized entries for each contact
                entries = session.query(SynthesizedEntry).filter_by(contact_id=contact.id).all()

                if not entries:
                    # Write contact without entries
                    writer.writerow([
                        contact.id, contact.full_name, contact.tier, '', '',
                        '', contact.created_at.isoformat(),
                        contact.telegram_id or '', contact.telegram_username or ''
                    ])
                else:
                    # Write contact with all entries
                    for entry in entries:
                        writer.writerow([
                            contact.id, contact.full_name, contact.tier,
                            entry.category, entry.content,
                            entry.confidence_score or '',
                            entry.created_at.isoformat(),
                            contact.telegram_id or '', contact.telegram_username or ''
                        ])

        finally:
            session.close()

        output.seek(0)
        return output.getvalue()

    csv_data = generate_csv()
    response = Response(csv_data, mimetype='text/csv')
    response.headers['Content-Disposition'] = f'attachment; filename=user_{user_id}_export.csv'
    return response
```

**Admin Frontend Dashboard:**

```html
<!-- Admin Dashboard Template (templates/admin_dashboard.html) -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Admin Dashboard - Kith</title>
    <link rel="stylesheet" href="/static/style.css" />
    <style>
        body {
            background:#0b1020;
            color:#fff;
            font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, sans-serif;
        }
        .container { max-width: 1200px; margin: 40px auto; padding: 0 16px; }
        .tabs { display: flex; gap: 8px; margin-bottom: 24px; border-bottom: 1px solid #2a366b; }
        .tab {
            padding: 12px 16px; background: transparent; border: 0;
            color: #9fb3ff; cursor: pointer; border-bottom: 2px solid transparent;
        }
        .tab.active { color: #4f6cff; border-bottom-color: #4f6cff; }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 16px; margin-bottom: 24px;
        }
        .stat-card {
            background: #111832; padding: 16px; border-radius: 8px;
            border: 1px solid #2a366b;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Admin Dashboard</h1>

        <!-- User Selection -->
        <div class="user-selector">
            <select id="user-select">
                <option value="">Select a user...</option>
            </select>
            <button onclick="loadUserData()">Load User Data</button>
        </div>

        <!-- Tab Navigation -->
        <div class="tabs">
            <button class="tab active" onclick="showTab('overview')">Overview</button>
            <button class="tab" onclick="showTab('kith-view')">Kith View</button>
            <button class="tab" onclick="showTab('graph-view')">Graph View</button>
            <button class="tab" onclick="showTab('csv-operations')">CSV Operations</button>
        </div>

        <!-- Overview Tab -->
        <div id="overview" class="tab-content active">
            <div class="stats-grid">
                <div class="stat-card">
                    <h3>Total Contacts</h3>
                    <div class="value" id="total-contacts">-</div>
                </div>
                <div class="stat-card">
                    <h3>Total Notes</h3>
                    <div class="value" id="total-notes">-</div>
                </div>
                <div class="stat-card">
                    <h3>Recent Activity</h3>
                    <div class="value" id="recent-activity">-</div>
                </div>
            </div>
        </div>

        <!-- Kith View Tab -->
        <div id="kith-view" class="tab-content">
            <div id="contacts-data"></div>
        </div>

        <!-- Graph View Tab -->
        <div id="graph-view" class="tab-content">
            <div class="graph-container">
                <h3>Relationship Network</h3>
                <div id="network"></div>
            </div>
        </div>

        <!-- CSV Operations Tab -->
        <div id="csv-operations" class="tab-content">
            <!-- Export/Import functionality -->
        </div>
    </div>

    <script src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <script>/* Dashboard JavaScript functionality */</script>
</body>
</html>
```

4. AI-POWERED NOTE ANALYSIS ENGINE
================================================================================

The core intelligence of the platform lies in its AI-powered note analysis system that transforms unstructured text into organized, actionable insights.

**AI Analysis Workflow:**

```python
# Note Processing Pipeline (app.py:2951-3037)
@app.route('/api/process-note', methods=['POST'])
@login_required
def process_note():
    data = request.get_json()
    contact_id = data.get('contact_id')
    raw_content = data.get('content', '').strip()

    if not contact_id or not raw_content:
        return jsonify({'error': 'Missing contact_id or content'}), 400

    session = get_session()
    try:
        # Get contact information
        contact = session.query(Contact).filter_by(
            id=contact_id,
            user_id=current_user.id
        ).first()

        if not contact:
            return jsonify({'error': 'Contact not found'}), 404

        # Store raw note
        raw_note = RawNote(contact_id=contact_id, content=raw_content)
        session.add(raw_note)
        session.flush()  # Get the ID

        # AI Analysis using Gemini 1.5 Pro
        analysis_prompt = f"""
        Analyze these personal notes about {contact.full_name} and organize the information into structured categories.

        Raw Notes: {raw_content}

        Please organize the information into these categories:
        - **Personal Details**: Basic information, demographics, background
        - **Interests**: Hobbies, preferences, things they enjoy
        - **Goals**: Aspirations, objectives, things they want to achieve
        - **Challenges**: Problems, difficulties, obstacles they face
        - **Actionable**: Follow-ups, things to remember, action items
        - **Professional**: Work-related information, career details
        - **Relationships**: Information about their connections to others
        - **Communication**: How they prefer to communicate, contact preferences

        For each category that has relevant information:
        1. Extract and organize the key points
        2. Provide a confidence score (1-10) for the accuracy
        3. Keep the tone professional but personal
        4. If no information exists for a category, skip it entirely

        Format as JSON with this structure:
        {
            "category_name": {
                "content": "organized information",
                "confidence": 8
            }
        }
        """

        try:
            # Call Gemini AI
            genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
            model = genai.GenerativeModel('gemini-1.5-pro-latest')
            response = model.generate_content(analysis_prompt)

            # Parse AI response
            ai_text = response.text.strip()

            # Extract JSON from AI response
            json_match = re.search(r'\{[\s\S]*\}', ai_text)
            if json_match:
                analysis_data = json.loads(json_match.group())
            else:
                # Fallback parsing
                analysis_data = {"General": {"content": ai_text, "confidence": 5}}

            # Save synthesized entries
            synthesis_results = []
            for category, data in analysis_data.items():
                content = data.get('content', '').strip()
                confidence = data.get('confidence', 5)

                if content and len(content) > 10:  # Only save substantial content
                    entry = SynthesizedEntry(
                        contact_id=contact_id,
                        category=category,
                        content=content,
                        confidence_score=float(confidence)
                    )
                    session.add(entry)
                    synthesis_results.append({
                        'category': category,
                        'content': content,
                        'confidence': confidence
                    })

            session.commit()

            return jsonify({
                'success': True,
                'raw_note_id': raw_note.id,
                'synthesis': synthesis_results,
                'contact_name': contact.full_name
            })

        except Exception as ai_error:
            logger.error(f"AI analysis error: {ai_error}")
            session.rollback()
            return jsonify({'error': f'AI analysis failed: {str(ai_error)}'}), 500

    finally:
        session.close()
```

**AI Model Configuration:**

```python
# Gemini AI Setup (app.py:45-65)
import google.generativeai as genai
from google.ai.generativelanguage_v1beta.types import content

# Configure Gemini AI
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

    # Safety settings for content generation
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]

    # Model configuration
    generation_config = {
        "temperature": 0.7,
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": 8192,
    }
```

5. VOICE RECORDING & TRANSCRIPTION SYSTEM
================================================================================

The platform includes browser-based voice recording with automatic transcription using OpenAI's Whisper API.

**Voice Recording Frontend:**

```javascript
// Voice Recording Implementation (static/js/voice-recording.js)
class VoiceRecorder {
    constructor() {
        this.mediaRecorder = null;
        this.audioChunks = [];
        this.isRecording = false;
        this.stream = null;
    }

    async startRecording(buttonElement) {
        try {
            // Request microphone access
            this.stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });

            // Create MediaRecorder
            this.mediaRecorder = new MediaRecorder(this.stream, {
                mimeType: 'audio/webm;codecs=opus'
            });

            this.audioChunks = [];

            // Handle data available
            this.mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    this.audioChunks.push(event.data);
                }
            };

            // Handle recording stop
            this.mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
                await this.transcribeAudio(audioBlob);

                // Clean up
                if (this.stream) {
                    this.stream.getTracks().forEach(track => track.stop());
                    this.stream = null;
                }
            };

            // Start recording
            this.mediaRecorder.start();
            this.isRecording = true;

            // Update UI
            buttonElement.textContent = 'üî¥ Stop';
            buttonElement.style.backgroundColor = '#dc3545';

        } catch (error) {
            console.error('Error starting recording:', error);
            showToast('Error: Could not access microphone', 'error');
        }
    }

    stopRecording(buttonElement) {
        if (this.mediaRecorder && this.isRecording) {
            this.mediaRecorder.stop();
            this.isRecording = false;

            // Update UI
            buttonElement.textContent = 'üé§';
            buttonElement.style.backgroundColor = '#28a745';
        }
    }

    async transcribeAudio(audioBlob) {
        const formData = new FormData();
        formData.append('audio', audioBlob, 'recording.webm');

        try {
            const response = await fetch('/api/transcribe-audio', {
                method: 'POST',
                body: formData
            });

            if (!response.ok) {
                throw new Error(`HTTP ${response.status}`);
            }

            const data = await response.json();

            if (data.success && data.transcript) {
                // Insert transcribed text into note input
                const noteInput = document.getElementById('note-input') ||
                                document.getElementById('profile-note-input');

                if (noteInput) {
                    const currentValue = noteInput.value;
                    const newValue = currentValue +
                        (currentValue && !currentValue.endsWith('\n') ? '\n\n' : '') +
                        data.transcript;

                    noteInput.value = newValue;
                    noteInput.focus();

                    showToast('Voice recording transcribed successfully!', 'success');
                }
            } else {
                throw new Error(data.error || 'Transcription failed');
            }

        } catch (error) {
            console.error('Transcription error:', error);
            showToast(`Transcription failed: ${error.message}`, 'error');
        }
    }
}

// Initialize voice recorder globally
const voiceRecorder = new VoiceRecorder();
```

**Audio Transcription Backend:**

```python
# OpenAI Whisper Integration (app.py:5255-5334)
@app.route('/api/transcribe-audio', methods=['POST'])
@login_required
def transcribe_audio():
    if 'audio' not in request.files:
        return jsonify({'error': 'No audio file provided'}), 400

    audio_file = request.files['audio']
    if audio_file.filename == '':
        return jsonify({'error': 'No audio file selected'}), 400

    try:
        # Read audio data
        audio_data = audio_file.read()

        # Create a temporary file for OpenAI API
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
            temp_file.write(audio_data)
            temp_file_path = temp_file.name

        try:
            # Call OpenAI Whisper API
            import openai
            openai.api_key = os.getenv('OPENAI_API_KEY')

            with open(temp_file_path, 'rb') as audio:
                transcript = openai.Audio.transcribe(
                    model="whisper-1",
                    file=audio,
                    language="en"  # Auto-detect if None
                )

            return jsonify({
                'success': True,
                'transcript': transcript.text.strip()
            })

        finally:
            # Clean up temporary file
            import os
            try:
                os.unlink(temp_file_path)
            except:
                pass

    except Exception as e:
        logger.error(f"Transcription error: {e}")
        return jsonify({
            'error': f'Transcription failed: {str(e)}'
        }), 500
```

6. FILE UPLOAD & ANALYSIS SYSTEM
================================================================================

The platform supports uploading images, PDFs, and documents with AI-powered content extraction and analysis.

**File Upload Implementation:**

```python
# File Upload with AI Analysis (app.py:4799-4897)
@app.route('/api/files/upload', methods=['POST'])
@login_required
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400

    file = request.files['file']
    contact_id = request.form.get('contact_id')

    if not contact_id:
        return jsonify({'error': 'Contact ID required'}), 400

    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400

    try:
        contact_id = int(contact_id)

        # Verify contact belongs to current user
        session = get_session()
        contact = session.query(Contact).filter_by(
            id=contact_id,
            user_id=current_user.id
        ).first()

        if not contact:
            session.close()
            return jsonify({'error': 'Contact not found'}), 404

        # Generate unique filename
        original_filename = secure_filename(file.filename)
        file_ext = os.path.splitext(original_filename)[1].lower()
        unique_filename = f"{uuid.uuid4().hex}_{original_filename}"

        # Create uploads directory if it doesn't exist
        upload_dir = os.path.join(os.getcwd(), 'uploads')
        os.makedirs(upload_dir, exist_ok=True)

        # Save file
        file_path = os.path.join(upload_dir, unique_filename)
        file.save(file_path)

        # Get file info
        file_size = os.path.getsize(file_path)
        file_type = file.content_type or 'application/octet-stream'

        # Create file record
        uploaded_file = UploadedFile(
            contact_id=contact_id,
            user_id=current_user.id,
            original_filename=original_filename,
            stored_filename=unique_filename,
            file_path=file_path,
            file_type=file_type,
            file_size_bytes=file_size
        )
        session.add(uploaded_file)
        session.commit()

        # Start background analysis task
        task_id = str(uuid.uuid4())

        # Create import task
        import_task = ImportTask(
            id=task_id,
            user_id=current_user.id,
            contact_id=contact_id,
            task_type='file_analysis',
            status='pending'
        )
        session.add(import_task)

        # Link file to task
        uploaded_file.analysis_task_id = task_id
        session.commit()

        # Start background analysis
        import threading
        analysis_thread = threading.Thread(
            target=analyze_file_content,
            args=(file_path, file_type, task_id, contact_id, uploaded_file.id)
        )
        analysis_thread.daemon = True
        analysis_thread.start()

        session.close()

        return jsonify({
            'success': True,
            'task_id': task_id,
            'file_id': uploaded_file.id,
            'message': 'File uploaded successfully. Analysis in progress...'
        })

    except Exception as e:
        logger.error(f"File upload error: {e}")
        if 'session' in locals():
            session.rollback()
            session.close()
        return jsonify({'error': str(e)}), 500

# Background File Analysis
def analyze_file_content(file_path, file_type, task_id, contact_id, file_id):
    """Analyze uploaded file content in background thread."""
    session = get_session()

    try:
        # Update task status
        task = session.query(ImportTask).filter_by(id=task_id).first()
        if not task:
            return

        task.status = 'processing'
        task.status_message = 'Analyzing file content...'
        task.progress = 25
        session.commit()

        # Extract content based on file type
        extracted_content = ""

        if file_type.startswith('image/'):
            # Image analysis with Google Vision API
            extracted_content = analyze_image_content(file_path)
        elif file_type == 'application/pdf':
            # PDF text extraction
            extracted_content = extract_pdf_content(file_path)
        elif file_type.startswith('text/'):
            # Text file reading
            with open(file_path, 'r', encoding='utf-8') as f:
                extracted_content = f.read()

        if extracted_content.strip():
            # Create raw note from extracted content
            contact = session.query(Contact).filter_by(id=contact_id).first()

            raw_note = RawNote(
                contact_id=contact_id,
                content=f"[File Analysis] {os.path.basename(file_path)}\n\n{extracted_content}"
            )
            session.add(raw_note)
            session.flush()

            # Update uploaded file record
            uploaded_file = session.query(UploadedFile).filter_by(id=file_id).first()
            if uploaded_file:
                uploaded_file.generated_raw_note_id = raw_note.id

            # Update task completion
            task.status = 'completed'
            task.status_message = f'Successfully extracted content from file'
            task.progress = 100
            task.completed_at = datetime.utcnow()

            session.commit()

        else:
            task.status = 'completed'
            task.status_message = 'No extractable content found in file'
            task.progress = 100
            session.commit()

    except Exception as e:
        logger.error(f"File analysis error: {e}")
        task.status = 'failed'
        task.error_details = str(e)
        task.status_message = f'Analysis failed: {str(e)}'
        session.commit()

    finally:
        session.close()
```

7. INTERACTIVE RELATIONSHIP GRAPH SYSTEM
================================================================================

The platform features an advanced relationship graph visualization using vis.js for network mapping and relationship management.

**Graph Visualization Frontend:**

```javascript
// Relationship Graph Implementation (static/js/graph-visualization.js)
class RelationshipGraph {
    constructor(containerId) {
        this.containerId = containerId;
        this.network = null;
        this.nodes = new vis.DataSet([]);
        this.edges = new vis.DataSet([]);
        this.selectedNodes = new Set();

        this.options = {
            nodes: {
                shape: 'circle',
                font: {
                    color: '#ffffff',
                    size: 14,
                    face: 'system-ui'
                },
                borderWidth: 2,
                borderWidthSelected: 4,
                shadow: {
                    enabled: true,
                    color: 'rgba(0,0,0,0.3)',
                    size: 10,
                    x: 2,
                    y: 2
                }
            },
            edges: {
                width: 2,
                color: {
                    color: '#848484',
                    highlight: '#4f6cff',
                    hover: '#4f6cff'
                },
                font: {
                    color: '#ffffff',
                    size: 12,
                    strokeWidth: 3,
                    strokeColor: '#0b1020'
                },
                arrows: {
                    to: { enabled: true, scaleFactor: 1, type: 'arrow' }
                },
                smooth: {
                    enabled: true,
                    type: 'continuous',
                    roundness: 0.5
                }
            },
            physics: {
                enabled: true,
                stabilization: { iterations: 100 },
                barnesHut: {
                    gravitationalConstant: -2000,
                    centralGravity: 0.3,
                    springLength: 200,
                    springConstant: 0.05,
                    damping: 0.09
                }
            },
            interaction: {
                selectConnectedEdges: false,
                hover: true,
                multiselect: true,
                dragNodes: true,
                dragView: true,
                zoomView: true
            }
        };
    }

    async loadData() {
        try {
            const response = await fetch('/api/graph-data');
            if (!response.ok) throw new Error(`HTTP ${response.status}`);

            const data = await response.json();

            // Process nodes (contacts)
            const processedNodes = data.contacts.map(contact => ({
                id: contact.id,
                label: contact.full_name,
                title: `${contact.full_name}\nTier: ${contact.tier}\nNotes: ${contact.note_count || 0}`,
                color: {
                    background: contact.tier === 1 ? '#4f6cff' : '#97C2FC',
                    border: contact.tier === 1 ? '#3d54cc' : '#7ea8f8',
                    highlight: {
                        background: contact.tier === 1 ? '#6b7fff' : '#b3d2ff',
                        border: contact.tier === 1 ? '#5865f2' : '#9fc3ff'
                    }
                },
                size: contact.tier === 1 ? 30 : 20,
                font: {
                    color: '#ffffff',
                    size: contact.tier === 1 ? 16 : 14
                },
                data: contact
            }));

            // Process edges (relationships)
            const processedEdges = data.relationships.map(rel => ({
                id: `${rel.source_contact_id}-${rel.target_contact_id}`,
                from: rel.source_contact_id,
                to: rel.target_contact_id,
                label: rel.label || '',
                title: rel.label || 'Connected'
            }));

            // Update datasets
            this.nodes.clear();
            this.edges.clear();
            this.nodes.add(processedNodes);
            this.edges.add(processedEdges);

            // Create or update network
            if (!this.network) {
                const container = document.getElementById(this.containerId);
                const data = { nodes: this.nodes, edges: this.edges };
                this.network = new vis.Network(container, data, this.options);

                // Add event listeners
                this.setupEventListeners();
            }

            return { nodes: processedNodes.length, edges: processedEdges.length };

        } catch (error) {
            console.error('Error loading graph data:', error);
            throw error;
        }
    }

    setupEventListeners() {
        // Node selection
        this.network.on('selectNode', (params) => {
            const nodeIds = params.nodes;
            this.selectedNodes = new Set(nodeIds);

            // Update UI for selected nodes
            this.updateSelectionUI();
        });

        // Node deselection
        this.network.on('deselectNode', (params) => {
            this.selectedNodes.clear();
            this.updateSelectionUI();
        });

        // Double click to view contact
        this.network.on('doubleClick', (params) => {
            if (params.nodes.length === 1) {
                const nodeId = params.nodes[0];
                const node = this.nodes.get(nodeId);
                if (node && node.data) {
                    // Navigate to contact profile
                    window.viewContactProfile(node.data.id);
                }
            }
        });

        // Context menu (right click)
        this.network.on('oncontext', (params) => {
            params.event.preventDefault();

            if (params.nodes.length === 1) {
                const nodeId = params.nodes[0];
                this.showContextMenu(nodeId, params.pointer.DOM);
            }
        });
    }

    updateSelectionUI() {
        const selectedCount = this.selectedNodes.size;
        const selectionInfo = document.getElementById('selection-info');

        if (selectionInfo) {
            if (selectedCount === 0) {
                selectionInfo.textContent = 'No contacts selected';
            } else if (selectedCount === 1) {
                const nodeId = Array.from(this.selectedNodes)[0];
                const node = this.nodes.get(nodeId);
                selectionInfo.textContent = `Selected: ${node.label}`;
            } else {
                selectionInfo.textContent = `Selected: ${selectedCount} contacts`;
            }
        }

        // Enable/disable relationship creation button
        const createRelBtn = document.getElementById('create-relationship-btn');
        if (createRelBtn) {
            createRelBtn.disabled = selectedCount !== 2;
        }
    }

    async createRelationship(label = '') {
        if (this.selectedNodes.size !== 2) {
            showToast('Please select exactly 2 contacts to create a relationship', 'error');
            return;
        }

        const [sourceId, targetId] = Array.from(this.selectedNodes);

        try {
            const response = await fetch('/api/relationships', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    source_contact_id: sourceId,
                    target_contact_id: targetId,
                    label: label
                })
            });

            if (!response.ok) throw new Error(`HTTP ${response.status}`);

            const result = await response.json();

            if (result.success) {
                // Add new edge to graph
                const newEdge = {
                    id: `${sourceId}-${targetId}`,
                    from: sourceId,
                    to: targetId,
                    label: label,
                    title: label || 'Connected'
                };

                this.edges.add(newEdge);

                showToast('Relationship created successfully!', 'success');
                this.selectedNodes.clear();
                this.network.unselectAll();
                this.updateSelectionUI();
            }

        } catch (error) {
            console.error('Error creating relationship:', error);
            showToast('Error creating relationship', 'error');
        }
    }

    showContextMenu(nodeId, position) {
        const node = this.nodes.get(nodeId);
        if (!node) return;

        // Remove existing context menu
        const existingMenu = document.getElementById('graph-context-menu');
        if (existingMenu) {
            existingMenu.remove();
        }

        // Create context menu
        const menu = document.createElement('div');
        menu.id = 'graph-context-menu';
        menu.className = 'context-menu';
        menu.style.left = `${position.x}px`;
        menu.style.top = `${position.y}px`;

        menu.innerHTML = `
            <div class="context-menu-item" onclick="window.viewContactProfile(${nodeId})">
                <i class="icon">üëÅÔ∏è</i> View Profile
            </div>
            <div class="context-menu-item" onclick="graphManager.toggleTier(${nodeId})">
                <i class="icon">‚≠ê</i> Toggle Tier
            </div>
            <div class="context-menu-item" onclick="graphManager.deleteNode(${nodeId})">
                <i class="icon">üóëÔ∏è</i> Delete Contact
            </div>
        `;

        document.body.appendChild(menu);

        // Remove menu on click outside
        setTimeout(() => {
            document.addEventListener('click', () => {
                if (menu.parentNode) {
                    menu.remove();
                }
            }, { once: true });
        }, 10);
    }
}

// Initialize graph manager globally
const graphManager = new RelationshipGraph('network');
```

**Graph Data Backend:**

```python
# Graph Data API (app.py:5335-5397)
@app.route('/api/graph-data', methods=['GET'])
@login_required
def get_graph_data():
    session = get_session()
    try:
        # Get all contacts for current user
        contacts = session.query(Contact).filter_by(user_id=current_user.id).all()

        # Get note counts for each contact
        contact_data = []
        for contact in contacts:
            note_count = session.query(SynthesizedEntry).filter_by(contact_id=contact.id).count()

            contact_data.append({
                'id': contact.id,
                'full_name': contact.full_name,
                'tier': contact.tier,
                'note_count': note_count,
                'created_at': contact.created_at.isoformat() if contact.created_at else None,
                'telegram_username': contact.telegram_username
            })

        # Get relationships
        relationships = session.query(ContactRelationship).filter_by(user_id=current_user.id).all()

        relationship_data = []
        for rel in relationships:
            relationship_data.append({
                'id': rel.id,
                'source_contact_id': rel.source_contact_id,
                'target_contact_id': rel.target_contact_id,
                'label': rel.label
            })

        return jsonify({
            'contacts': contact_data,
            'relationships': relationship_data
        })

    except Exception as e:
        logger.error(f"Graph data error: {e}")
        return jsonify({'error': str(e)}), 500
    finally:
        session.close()
```

8. TELEGRAM INTEGRATION SYSTEM
================================================================================

The platform provides comprehensive Telegram integration for automated message import and analysis.

**Telegram Authentication & Connection:**

```python
# Telegram Authentication (app.py:1658-1746)
@app.route('/api/telegram/auth/start', methods=['POST'])
@login_required
def telegram_auth_start():
    """Start Telegram authentication process"""
    data = request.get_json()
    phone_number = data.get('phone_number', '').strip()

    if not phone_number:
        return jsonify({'error': 'Phone number is required'}), 400

    try:
        # Initialize Telegram client
        session_name = f"user_{current_user.id}_telegram"
        client = TelegramClient(session_name, api_id, api_hash)

        # Start client and request code
        client.connect()
        result = client.send_code_request(phone_number)

        # Store session info for verification
        telegram_sessions[current_user.id] = {
            'client': client,
            'phone_number': phone_number,
            'phone_code_hash': result.phone_code_hash
        }

        return jsonify({
            'success': True,
            'message': 'Verification code sent to your phone',
            'phone_number_masked': phone_number[:3] + '*' * (len(phone_number) - 6) + phone_number[-3:]
        })

    except Exception as e:
        logger.error(f"Telegram auth start error: {e}")
        return jsonify({'error': f'Authentication failed: {str(e)}'}), 500

@app.route('/api/telegram/auth/verify', methods=['POST'])
@login_required
def telegram_auth_verify():
    """Verify Telegram authentication code"""
    data = request.get_json()
    verification_code = data.get('verification_code', '').strip()

    if not verification_code:
        return jsonify({'error': 'Verification code is required'}), 400

    session_info = telegram_sessions.get(current_user.id)
    if not session_info:
        return jsonify({'error': 'No active authentication session'}), 400

    try:
        client = session_info['client']
        phone_number = session_info['phone_number']
        phone_code_hash = session_info['phone_code_hash']

        # Sign in with verification code
        result = client.sign_in(
            phone=phone_number,
            code=verification_code,
            phone_code_hash=phone_code_hash
        )

        # Get user info
        me = client.get_me()

        # Clean up session
        client.disconnect()
        del telegram_sessions[current_user.id]

        return jsonify({
            'success': True,
            'message': 'Telegram authentication successful',
            'user_info': {
                'id': me.id,
                'username': me.username,
                'first_name': me.first_name,
                'last_name': me.last_name,
                'phone': me.phone,
                'is_verified': me.verified,
                'is_premium': getattr(me, 'premium', False)
            }
        })

    except Exception as e:
        logger.error(f"Telegram verification error: {e}")
        return jsonify({'error': f'Verification failed: {str(e)}'}), 500
```

**Telegram Message Import:**

```python
# Telegram Chat Import (app.py:3321-3486)
@app.route('/api/telegram/start-import', methods=['POST'])
@login_required
def telegram_start_import():
    """Start importing Telegram chat history for a contact"""
    data = request.get_json()
    contact_id = data.get('contact_id')

    if not contact_id:
        return jsonify({'error': 'Contact ID is required'}), 400

    try:
        contact_id = int(contact_id)

        session = get_session()
        contact = session.query(Contact).filter_by(
            id=contact_id,
            user_id=current_user.id
        ).first()

        if not contact:
            session.close()
            return jsonify({'error': 'Contact not found'}), 404

        # Check if contact has Telegram handle
        if not contact.telegram_handle and not contact.telegram_username:
            session.close()
            return jsonify({'error': 'Contact must have Telegram handle configured'}), 400

        # Create import task
        task_id = str(uuid.uuid4())
        import_task = ImportTask(
            id=task_id,
            user_id=current_user.id,
            contact_id=contact_id,
            task_type='telegram_import',
            status='pending',
            status_message='Starting Telegram import...'
        )

        session.add(import_task)
        session.commit()
        session.close()

        # Start background import
        import threading
        import_thread = threading.Thread(
            target=run_telegram_import,
            args=(task_id, current_user.id, contact_id)
        )
        import_thread.daemon = True
        import_thread.start()

        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'Telegram import started'
        })

    except Exception as e:
        logger.error(f"Telegram import start error: {e}")
        return jsonify({'error': str(e)}), 500

def run_telegram_import(task_id, user_id, contact_id):
    """Background task to import Telegram messages"""
    session = get_session()

    try:
        # Get task and contact info
        task = session.query(ImportTask).filter_by(id=task_id).first()
        contact = session.query(Contact).filter_by(id=contact_id).first()

        if not task or not contact:
            return

        # Update task status
        task.status = 'connecting'
        task.status_message = 'Connecting to Telegram...'
        task.progress = 10
        session.commit()

        # Initialize Telegram client
        session_name = f"user_{user_id}_import"
        client = TelegramClient(session_name, api_id, api_hash)

        client.connect()

        if not client.is_user_authorized():
            task.status = 'failed'
            task.error_details = 'Telegram authorization required'
            session.commit()
            return

        # Update progress
        task.status = 'fetching'
        task.status_message = 'Fetching messages...'
        task.progress = 30
        session.commit()

        # Find chat by handle or username
        target_handle = contact.telegram_handle or contact.telegram_username
        if target_handle.startswith('@'):
            target_handle = target_handle[1:]

        # Get chat entity
        entity = client.get_entity(target_handle)

        # Fetch messages (limit to last 500 for performance)
        messages = []
        for message in client.iter_messages(entity, limit=500):
            if message.text:
                messages.append({
                    'date': message.date,
                    'text': message.text,
                    'from_me': message.from_id and message.from_id.user_id == client.get_me().id
                })

        # Update progress
        task.status = 'processing'
        task.status_message = f'Processing {len(messages)} messages...'
        task.progress = 60
        session.commit()

        # Process messages into raw notes
        if messages:
            # Group messages by date
            from collections import defaultdict
            messages_by_date = defaultdict(list)

            for msg in messages:
                date_key = msg['date'].strftime('%Y-%m-%d')
                messages_by_date[date_key].append(msg)

            # Create raw notes for each date
            for date_str, date_messages in messages_by_date.items():
                # Combine messages for the date
                combined_text = f"[Telegram Import - {date_str}]\n\n"

                for msg in sorted(date_messages, key=lambda x: x['date']):
                    sender = "Me" if msg['from_me'] else contact.full_name
                    time_str = msg['date'].strftime('%H:%M')
                    combined_text += f"{time_str} {sender}: {msg['text']}\n"

                # Create raw note
                raw_note = RawNote(
                    contact_id=contact_id,
                    content=combined_text
                )
                session.add(raw_note)

        # Update contact sync info
        contact.telegram_last_sync = datetime.utcnow()
        if not contact.telegram_id and hasattr(entity, 'id'):
            contact.telegram_id = str(entity.id)

        # Complete task
        task.status = 'completed'
        task.status_message = f'Successfully imported {len(messages)} messages'
        task.progress = 100
        task.completed_at = datetime.utcnow()

        session.commit()
        client.disconnect()

    except Exception as e:
        logger.error(f"Telegram import error: {e}")
        task.status = 'failed'
        task.error_details = str(e)
        task.status_message = f'Import failed: {str(e)}'
        session.commit()

    finally:
        session.close()
```

9. ADVANCED ANALYTICS ENGINE
================================================================================

The platform includes sophisticated analytics for relationship health scoring, trend analysis, and actionable recommendations.

**Relationship Health Scoring:**

```python
# Analytics Implementation (analytics.py - Referenced from PHASE4_SUMMARY.md)
class RelationshipAnalytics:
    def __init__(self):
        self.session = get_session()

    def calculate_relationship_health_score(self, contact_id):
        """Calculate comprehensive relationship health score"""
        try:
            # Get contact and entries
            contact = self.session.query(Contact).filter_by(id=contact_id).first()
            if not contact:
                return None

            entries = self.session.query(SynthesizedEntry).filter_by(contact_id=contact_id).all()
            raw_notes = self.session.query(RawNote).filter_by(contact_id=contact_id).all()

            if not entries and not raw_notes:
                return {
                    'health_score': 0,
                    'insights': ['No interaction data available'],
                    'total_interactions': 0,
                    'last_interaction': None,
                    'days_since_last': None
                }

            # Calculate metrics
            total_interactions = len(entries) + len(raw_notes)

            # Recency score (30% weight)
            last_interaction = None
            if entries:
                last_entry = max(entries, key=lambda x: x.created_at)
                last_interaction = last_entry.created_at
            if raw_notes:
                last_raw = max(raw_notes, key=lambda x: x.created_at)
                if not last_interaction or last_raw.created_at > last_interaction:
                    last_interaction = last_raw.created_at

            days_since_last = (datetime.utcnow() - last_interaction).days if last_interaction else 365
            recency_score = max(0, 100 - (days_since_last * 2))  # Decreases by 2 per day

            # Engagement score (30% weight)
            weeks_active = min(12, total_interactions)  # Cap at 12 weeks
            engagement_score = min(100, (total_interactions / weeks_active) * 20)

            # Quality score (20% weight)
            if entries:
                avg_confidence = sum(e.confidence_score or 5 for e in entries) / len(entries)
                quality_score = avg_confidence * 10  # Scale to 100
            else:
                quality_score = 50  # Neutral score for raw notes only

            # Diversity score (20% weight)
            categories = set(e.category for e in entries)
            diversity_score = min(100, len(categories) * 25)  # 25 points per category

            # Calculate weighted health score
            health_score = (
                recency_score * 0.3 +
                engagement_score * 0.3 +
                quality_score * 0.2 +
                diversity_score * 0.2
            )

            # Generate insights
            insights = []
            if health_score >= 80:
                insights.append("Excellent relationship health!")
            elif health_score >= 60:
                insights.append("Good relationship health. Consider more frequent interactions.")
            elif health_score >= 40:
                insights.append("Moderate relationship health. Increase engagement frequency.")
            else:
                insights.append("Low relationship health. Schedule immediate follow-up.")

            if days_since_last > 30:
                insights.append(f"No interaction in {days_since_last} days. Consider reaching out.")

            return {
                'health_score': round(health_score, 1),
                'total_interactions': total_interactions,
                'last_interaction': last_interaction.isoformat() if last_interaction else None,
                'days_since_last': days_since_last,
                'category_distribution': dict(Counter(e.category for e in entries)),
                'confidence_avg': round(sum(e.confidence_score or 5 for e in entries) / len(entries), 1) if entries else 0,
                'insights': insights
            }

        except Exception as e:
            logger.error(f"Health calculation error: {e}")
            return None

    def get_relationship_trends(self, contact_id, days=90):
        """Analyze relationship interaction trends"""
        try:
            # Get entries within time range
            cutoff_date = datetime.utcnow() - timedelta(days=days)

            entries = self.session.query(SynthesizedEntry).filter(
                SynthesizedEntry.contact_id == contact_id,
                SynthesizedEntry.created_at >= cutoff_date
            ).all()

            # Group by week
            weekly_data = defaultdict(lambda: {"count": 0, "categories": [], "confidence": []})

            for entry in entries:
                week_key = entry.created_at.strftime("%Y-W%U")
                weekly_data[week_key]["count"] += 1
                weekly_data[week_key]["categories"].append(entry.category)
                if entry.confidence_score:
                    weekly_data[week_key]["confidence"].append(entry.confidence_score)

            # Calculate trend metrics
            weeks = sorted(weekly_data.keys())
            if len(weeks) < 2:
                return {"trend": "insufficient_data", "weekly_data": {}}

            recent_avg = sum(weekly_data[w]["count"] for w in weeks[-4:]) / min(4, len(weeks))
            earlier_avg = sum(weekly_data[w]["count"] for w in weeks[:-4]) / max(1, len(weeks) - 4)

            if recent_avg > earlier_avg * 1.2:
                trend = "increasing"
            elif recent_avg < earlier_avg * 0.8:
                trend = "decreasing"
            else:
                trend = "stable"

            return {
                "trend": trend,
                "recent_average": round(recent_avg, 1),
                "earlier_average": round(earlier_avg, 1),
                "total_weeks": len(weeks),
                "weekly_data": dict(weekly_data)
            }

        except Exception as e:
            logger.error(f"Trend analysis error: {e}")
            return {"error": str(e)}

    def get_actionable_recommendations(self, contact_id):
        """Generate personalized relationship recommendations"""
        try:
            health_data = self.calculate_relationship_health_score(contact_id)
            if not health_data:
                return []

            recommendations = []
            health_score = health_data['health_score']
            days_since_last = health_data['days_since_last']

            # Health-based recommendations
            if health_score < 50:
                recommendations.append({
                    'priority': 'high',
                    'type': 'follow_up',
                    'title': 'Schedule Immediate Follow-up',
                    'description': 'Relationship health is low. Schedule a call or meeting this week.',
                    'action': 'schedule_meeting'
                })

            # Recency-based recommendations
            if days_since_last and days_since_last > 14:
                recommendations.append({
                    'priority': 'medium',
                    'type': 'reconnect',
                    'title': 'Reconnect Soon',
                    'description': f'No interaction in {days_since_last} days. Send a quick message.',
                    'action': 'send_message'
                })

            # Category-based recommendations
            categories = set(health_data['category_distribution'].keys())
            if 'Goals' not in categories and health_data['total_interactions'] > 3:
                recommendations.append({
                    'priority': 'medium',
                    'type': 'deeper_connection',
                    'title': 'Learn About Their Goals',
                    'description': 'Ask about their aspirations and long-term objectives.',
                    'action': 'ask_about_goals'
                })

            return recommendations[:5]  # Return top 5 recommendations

        except Exception as e:
            logger.error(f"Recommendations error: {e}")
            return []
```

**Analytics API Endpoints:**

```python
# Analytics API Routes (Referenced from Flask app analysis)
@app.route('/api/analytics/contact/<int:contact_id>/health', methods=['GET'])
@login_required
def get_contact_health(contact_id):
    """Get relationship health score for a contact"""
    # Verify contact belongs to current user
    session = get_session()
    try:
        contact = session.query(Contact).filter_by(id=contact_id, user_id=current_user.id).first()
        if not contact:
            return jsonify({'error': 'Contact not found'}), 404

        analytics = RelationshipAnalytics()
        health_data = analytics.calculate_relationship_health_score(contact_id)

        if health_data:
            return jsonify(health_data)
        else:
            return jsonify({'error': 'Unable to calculate health score'}), 500

    finally:
        session.close()

@app.route('/api/analytics/contact/<int:contact_id>/trends', methods=['GET'])
@login_required
def get_contact_trends(contact_id):
    """Get relationship trends for a contact"""
    days = request.args.get('days', 90, type=int)

    session = get_session()
    try:
        contact = session.query(Contact).filter_by(id=contact_id, user_id=current_user.id).first()
        if not contact:
            return jsonify({'error': 'Contact not found'}), 404

        analytics = RelationshipAnalytics()
        trends = analytics.get_relationship_trends(contact_id, days)

        return jsonify(trends)

    finally:
        session.close()

@app.route('/api/analytics/contact/<int:contact_id>/recommendations', methods=['GET'])
@login_required
def get_contact_recommendations(contact_id):
    """Get actionable recommendations for a contact"""
    session = get_session()
    try:
        contact = session.query(Contact).filter_by(id=contact_id, user_id=current_user.id).first()
        if not contact:
            return jsonify({'error': 'Contact not found'}), 404

        analytics = RelationshipAnalytics()
        recommendations = analytics.get_actionable_recommendations(contact_id)

        return jsonify({'recommendations': recommendations})

    finally:
        session.close()
```

10. CALENDAR INTEGRATION SYSTEM
================================================================================

The platform includes smart calendar integration with automatic event creation from actionable items.

**Calendar Event Creation:**

```python
# Calendar Integration (calendar_integration.py - Referenced from PHASE4_SUMMARY.md)
class CalendarIntegration:
    def __init__(self):
        self.calendar_file = 'kith_calendar_events.json'
        self.date_patterns = [
            r'tomorrow\s+(?:at\s+)?(\d{1,2})(?::(\d{2}))?\s*(am|pm)?',
            r'next\s+week\s+(?:on\s+)?(monday|tuesday|wednesday|thursday|friday|saturday|sunday)',
            r'next\s+month\s+(?:on\s+the\s+)?(\d{1,2})(?:st|nd|rd|th)?',
            r'(\d{1,2})/(\d{1,2})/(\d{2,4})\s*(?:at\s+)?(\d{1,2})(?::(\d{2}))?\s*(am|pm)?',
            r'(january|february|march|april|may|june|july|august|september|october|november|december)\s+(\d{1,2})(?:st|nd|rd|th)?\s*(?:at\s+)?(\d{1,2})(?::(\d{2}))?\s*(am|pm)?'
        ]

    def extract_date_time_from_text(self, text):
        """Extract date and time information from natural language text"""
        text = text.lower().strip()

        for pattern in self.date_patterns:
            match = re.search(pattern, text)
            if match:
                return self._parse_date_match(match, pattern)

        # Default fallback - schedule for tomorrow
        tomorrow = datetime.now() + timedelta(days=1)
        return {
            'date': tomorrow.strftime('%Y-%m-%d'),
            'time': '09:00',
            'confidence': 'low',
            'extracted_text': 'No specific date found, defaulting to tomorrow'
        }

    def create_events_from_entries(self, contact_id, entries):
        """Create calendar events from synthesized entries marked as actionable"""
        events_created = []

        for entry in entries:
            if entry.category.lower() == 'actionable':
                date_time_info = self.extract_date_time_from_text(entry.content)

                # Get contact name
                session = get_session()
                try:
                    contact = session.query(Contact).filter_by(id=contact_id).first()
                    contact_name = contact.full_name if contact else f"Contact {contact_id}"
                finally:
                    session.close()

                # Create event
                event = {
                    'id': str(uuid.uuid4()),
                    'title': f"Follow up with {contact_name}",
                    'description': entry.content[:200] + ('...' if len(entry.content) > 200 else ''),
                    'date': date_time_info['date'],
                    'time': date_time_info['time'],
                    'confidence': date_time_info.get('confidence', 'low'),
                    'contact_id': contact_id,
                    'contact_name': contact_name,
                    'created_at': datetime.now().isoformat(),
                    'entry_id': entry.id
                }

                events_created.append(event)

        # Save events to local calendar file
        self._save_events(events_created)

        return events_created

    def _save_events(self, new_events):
        """Save events to local JSON calendar file"""
        try:
            # Load existing events
            existing_events = []
            if os.path.exists(self.calendar_file):
                with open(self.calendar_file, 'r') as f:
                    existing_events = json.load(f)

            # Add new events
            existing_events.extend(new_events)

            # Save back to file
            with open(self.calendar_file, 'w') as f:
                json.dump(existing_events, f, indent=2, default=str)

        except Exception as e:
            logger.error(f"Calendar save error: {e}")

    def get_upcoming_events(self, days_ahead=30):
        """Get upcoming calendar events"""
        try:
            if not os.path.exists(self.calendar_file):
                return []

            with open(self.calendar_file, 'r') as f:
                all_events = json.load(f)

            # Filter for upcoming events
            cutoff_date = datetime.now() + timedelta(days=days_ahead)
            upcoming_events = []

            for event in all_events:
                event_date = datetime.strptime(event['date'], '%Y-%m-%d')
                if event_date <= cutoff_date and event_date >= datetime.now():
                    upcoming_events.append(event)

            # Sort by date/time
            upcoming_events.sort(key=lambda x: (x['date'], x['time']))

            return upcoming_events

        except Exception as e:
            logger.error(f"Calendar read error: {e}")
            return []
```

**Calendar API Endpoints:**

```python
# Calendar API Routes (Referenced from Flask app)
@app.route('/api/calendar/events', methods=['GET'])
@login_required
def get_calendar_events():
    """Get upcoming calendar events"""
    days = request.args.get('days', 30, type=int)

    calendar = CalendarIntegration()
    events = calendar.get_upcoming_events(days)

    return jsonify({'events': events})

@app.route('/api/calendar/create-from-actionable', methods=['POST'])
@login_required
def create_calendar_from_actionable():
    """Create calendar events from actionable items for a contact"""
    data = request.get_json()
    contact_id = data.get('contact_id')

    if not contact_id:
        return jsonify({'error': 'Contact ID required'}), 400

    session = get_session()
    try:
        # Get actionable entries for contact
        entries = session.query(SynthesizedEntry).filter_by(
            contact_id=contact_id,
            category='Actionable'
        ).all()

        if not entries:
            return jsonify({'message': 'No actionable items found', 'events': []})

        # Create calendar events
        calendar = CalendarIntegration()
        events = calendar.create_events_from_entries(contact_id, entries)

        return jsonify({
            'success': True,
            'events_created': len(events),
            'events': events
        })

    finally:
        session.close()

@app.route('/api/calendar/extract-datetime', methods=['POST'])
@login_required
def extract_datetime_from_text():
    """Extract date/time information from natural language text"""
    data = request.get_json()
    text = data.get('text', '')

    if not text:
        return jsonify({'error': 'Text is required'}), 400

    calendar = CalendarIntegration()
    date_time_info = calendar.extract_date_time_from_text(text)

    return jsonify(date_time_info)
```

11. TAG MANAGEMENT SYSTEM
================================================================================

The platform includes a comprehensive tag management system for organizing and categorizing contacts.

**Tag System Implementation:**

```python
# Tag Management API (app.py:5529-5890)
@app.route('/api/tags', methods=['GET'])
@login_required
def get_tags():
    """Get all tags for current user"""
    session = get_session()
    try:
        tags = session.query(Tag).filter_by(user_id=current_user.id).order_by(Tag.name).all()

        tag_data = []
        for tag in tags:
            # Get contact count for each tag
            contact_count = session.query(ContactTag).filter_by(tag_id=tag.id).count()

            tag_data.append({
                'id': tag.id,
                'name': tag.name,
                'color': tag.color,
                'description': tag.description,
                'contact_count': contact_count,
                'created_at': tag.created_at.isoformat() if tag.created_at else None
            })

        return jsonify({'tags': tag_data})

    finally:
        session.close()

@app.route('/api/tags', methods=['POST'])
@login_required
def create_tag():
    """Create a new tag"""
    data = request.get_json()
    name = data.get('name', '').strip()
    color = data.get('color', '#97C2FC').strip()
    description = data.get('description', '').strip()

    if not name:
        return jsonify({'error': 'Tag name is required'}), 400

    # Validate color format
    if not re.match(r'^#[0-9A-Fa-f]{6}$', color):
        color = '#97C2FC'  # Default color

    session = get_session()
    try:
        # Check if tag already exists for user
        existing_tag = session.query(Tag).filter_by(
            user_id=current_user.id,
            name=name
        ).first()

        if existing_tag:
            return jsonify({'error': 'Tag with this name already exists'}), 400

        # Create new tag
        tag = Tag(
            user_id=current_user.id,
            name=name,
            color=color,
            description=description if description else None
        )

        session.add(tag)
        session.commit()

        return jsonify({
            'success': True,
            'tag': {
                'id': tag.id,
                'name': tag.name,
                'color': tag.color,
                'description': tag.description,
                'contact_count': 0
            }
        })

    finally:
        session.close()

@app.route('/api/contacts/<int:contact_id>/tags', methods=['POST'])
@login_required
def assign_tag_to_contact(contact_id):
    """Assign a tag to a contact"""
    data = request.get_json()
    tag_id = data.get('tag_id')

    if not tag_id:
        return jsonify({'error': 'Tag ID is required'}), 400

    session = get_session()
    try:
        # Verify contact belongs to user
        contact = session.query(Contact).filter_by(
            id=contact_id,
            user_id=current_user.id
        ).first()

        if not contact:
            return jsonify({'error': 'Contact not found'}), 404

        # Verify tag belongs to user
        tag = session.query(Tag).filter_by(
            id=tag_id,
            user_id=current_user.id
        ).first()

        if not tag:
            return jsonify({'error': 'Tag not found'}), 404

        # Check if assignment already exists
        existing_assignment = session.query(ContactTag).filter_by(
            contact_id=contact_id,
            tag_id=tag_id
        ).first()

        if existing_assignment:
            return jsonify({'error': 'Tag already assigned to contact'}), 400

        # Create assignment
        contact_tag = ContactTag(
            contact_id=contact_id,
            tag_id=tag_id
        )

        session.add(contact_tag)
        session.commit()

        return jsonify({
            'success': True,
            'message': f'Tag "{tag.name}" assigned to contact'
        })

    finally:
        session.close()
```

**Tag Frontend Management:**

```javascript
// Tag Management Frontend (static/js/tag-management.js)
class TagManager {
    constructor() {
        this.tags = [];
        this.selectedContact = null;
    }

    async loadTags() {
        try {
            const response = await fetch('/api/tags');
            if (!response.ok) throw new Error(`HTTP ${response.status}`);

            const data = await response.json();
            this.tags = data.tags;

            return this.tags;
        } catch (error) {
            console.error('Error loading tags:', error);
            showToast('Error loading tags', 'error');
            return [];
        }
    }

    async createTag(name, color = '#97C2FC', description = '') {
        try {
            const response = await fetch('/api/tags', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    name: name.trim(),
                    color: color,
                    description: description.trim()
                })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(errorData.error || 'Failed to create tag');
            }

            const result = await response.json();

            if (result.success) {
                this.tags.push(result.tag);
                showToast(`Tag "${name}" created successfully!`, 'success');
                await this.refreshTagUI();
                return result.tag;
            }

        } catch (error) {
            console.error('Error creating tag:', error);
            showToast(`Error creating tag: ${error.message}`, 'error');
            return null;
        }
    }

    async assignTagToContact(contactId, tagId) {
        try {
            const response = await fetch(`/api/contacts/${contactId}/tags`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ tag_id: tagId })
            });

            if (!response.ok) {
                const errorData = await response.json();
                throw new Error(errorData.error || 'Failed to assign tag');
            }

            const result = await response.json();

            if (result.success) {
                showToast(result.message, 'success');
                await this.loadContactTags(contactId);
                return true;
            }

        } catch (error) {
            console.error('Error assigning tag:', error);
            showToast(`Error assigning tag: ${error.message}`, 'error');
            return false;
        }
    }

    async loadContactTags(contactId) {
        try {
            const response = await fetch(`/api/contacts/${contactId}/tags`);
            if (!response.ok) throw new Error(`HTTP ${response.status}`);

            const data = await response.json();
            return data.tags || [];

        } catch (error) {
            console.error('Error loading contact tags:', error);
            return [];
        }
    }

    renderTagSelector(containerId, onSelect) {
        const container = document.getElementById(containerId);
        if (!container) return;

        container.innerHTML = `
            <div class="tag-selector">
                <select id="tag-select-dropdown" class="tag-select">
                    <option value="">Select a tag...</option>
                    ${this.tags.map(tag => `
                        <option value="${tag.id}" style="color: ${tag.color}">
                            ${tag.name} (${tag.contact_count} contacts)
                        </option>
                    `).join('')}
                </select>
                <button onclick="tagManager.showCreateTagForm()" class="secondary-btn">
                    + New Tag
                </button>
            </div>

            <div id="create-tag-form" style="display: none;" class="create-tag-form">
                <h4>Create New Tag</h4>
                <input type="text" id="new-tag-name" placeholder="Tag name" maxlength="50" />
                <input type="color" id="new-tag-color" value="#97C2FC" title="Tag color" />
                <textarea id="new-tag-description" placeholder="Description (optional)" rows="2"></textarea>
                <div class="form-actions">
                    <button onclick="tagManager.submitNewTag()" class="primary-btn">Create</button>
                    <button onclick="tagManager.hideCreateTagForm()" class="secondary-btn">Cancel</button>
                </div>
            </div>
        `;

        // Set up event listener
        const dropdown = document.getElementById('tag-select-dropdown');
        if (dropdown && onSelect) {
            dropdown.addEventListener('change', (e) => {
                if (e.target.value) {
                    onSelect(e.target.value);
                }
            });
        }
    }

    renderContactTags(contactId, containerId) {
        const container = document.getElementById(containerId);
        if (!container) return;

        this.loadContactTags(contactId).then(contactTags => {
            container.innerHTML = contactTags.map(tag => `
                <span class="tag-badge" style="background-color: ${tag.color};" title="${tag.description || ''}">
                    ${tag.name}
                    <button class="tag-remove" onclick="tagManager.removeTagFromContact(${contactId}, ${tag.id})" title="Remove tag">
                        √ó
                    </button>
                </span>
            `).join('');
        });
    }

    showCreateTagForm() {
        const form = document.getElementById('create-tag-form');
        if (form) {
            form.style.display = 'block';
            document.getElementById('new-tag-name').focus();
        }
    }

    hideCreateTagForm() {
        const form = document.getElementById('create-tag-form');
        if (form) {
            form.style.display = 'none';
            // Clear form
            document.getElementById('new-tag-name').value = '';
            document.getElementById('new-tag-color').value = '#97C2FC';
            document.getElementById('new-tag-description').value = '';
        }
    }

    async submitNewTag() {
        const name = document.getElementById('new-tag-name').value.trim();
        const color = document.getElementById('new-tag-color').value;
        const description = document.getElementById('new-tag-description').value.trim();

        if (!name) {
            showToast('Tag name is required', 'error');
            return;
        }

        const newTag = await this.createTag(name, color, description);
        if (newTag) {
            this.hideCreateTagForm();
        }
    }

    async refreshTagUI() {
        await this.loadTags();
        // Refresh any visible tag selectors or lists
        const event = new CustomEvent('tagsUpdated', { detail: this.tags });
        document.dispatchEvent(event);
    }
}

// Initialize global tag manager
const tagManager = new TagManager();

// Load tags on page load
document.addEventListener('DOMContentLoaded', () => {
    tagManager.loadTags();
});
```

12. CSV IMPORT/EXPORT SYSTEM
================================================================================

The platform provides comprehensive CSV import/export functionality for bulk data management and backups.

**CSV Export Implementation:**

```python
# CSV Export (app.py:3550-3643)
@app.route('/api/export/csv', methods=['GET'])
@login_required
def export_csv():
    """Export user's data as CSV"""
    def generate_csv():
        output = StringIO()
        writer = csv.writer(output)

        # Write header
        writer.writerow([
            'Contact ID', 'Contact Name', 'Tier', 'Category', 'Content',
            'AI Confidence', 'Created At', 'Telegram ID', 'Telegram Username'
        ])

        session = get_session()
        try:
            # Get all contacts for current user
            contacts = session.query(Contact).filter_by(user_id=current_user.id).all()

            for contact in contacts:
                # Get synthesized entries for each contact
                entries = session.query(SynthesizedEntry).filter_by(contact_id=contact.id).all()

                if not entries:
                    # Write contact without entries
                    writer.writerow([
                        contact.id, contact.full_name, contact.tier, '', '',
                        '', contact.created_at.isoformat() if contact.created_at else '',
                        contact.telegram_id or '', contact.telegram_username or ''
                    ])
                else:
                    # Write contact with all entries
                    for entry in entries:
                        writer.writerow([
                            contact.id, contact.full_name, contact.tier,
                            entry.category, entry.content,
                            entry.confidence_score or '',
                            entry.created_at.isoformat() if entry.created_at else '',
                            contact.telegram_id or '', contact.telegram_username or ''
                        ])

        finally:
            session.close()

        output.seek(0)
        return output.getvalue()

    # Generate CSV data
    csv_data = generate_csv()

    # Create response
    response = Response(csv_data, mimetype='text/csv')

    # Set filename with timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'kith_platform_export_{timestamp}.csv'
    response.headers['Content-Disposition'] = f'attachment; filename={filename}'

    return response
```

**CSV Import with Merge Logic:**

```python
# CSV Import (app.py:3758-4142)
@app.route('/api/import/merge-from-csv', methods=['POST'])
@login_required
def import_merge_from_csv():
    """Import CSV data with smart merging logic"""
    try:
        if 'backup_file' not in request.files:
            return jsonify({'error': 'No backup file provided'}), 400

        file = request.files['backup_file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400

        if not file.filename.lower().endswith('.csv'):
            return jsonify({'error': 'File must be a CSV'}), 400

        # Parse options
        dry_run = request.form.get('dry_run', '').lower() == 'true'
        merge_strategy = request.form.get('merge_strategy', 'skip_existing')
        confidence_threshold = float(request.form.get('confidence_threshold', '0'))

        # Read CSV content
        csv_content = file.read().decode('utf-8')

        # Process the CSV
        result = run_merge_process(csv_content, {
            'dry_run': dry_run,
            'merge_strategy': merge_strategy,
            'confidence_threshold': confidence_threshold
        })

        return jsonify(result)

    except Exception as e:
        logger.error(f"CSV import error: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'contacts_processed': 0,
            'entries_created': 0
        }), 500

def run_merge_process(csv_text: str, options: dict = None) -> dict:
    """Process CSV import with merge logic"""
    options = options or {}
    dry_run = bool(options.get('dry_run', False))
    merge_strategy = options.get('merge_strategy', 'skip_existing')
    confidence_threshold = float(options.get('confidence_threshold', 0))

    session = get_session()
    stats = {
        'contacts_processed': 0,
        'contacts_created': 0,
        'contacts_updated': 0,
        'entries_created': 0,
        'entries_skipped': 0,
        'errors': []
    }

    try:
        # Parse CSV
        csv_reader = csv.DictReader(StringIO(csv_text))

        # Group rows by contact
        contacts_data = {}
        for row in csv_reader:
            contact_id = row.get('Contact ID', '').strip()
            contact_name = row.get('Contact Name', '').strip()

            if not contact_name:
                continue

            key = contact_name.lower()
            if key not in contacts_data:
                contacts_data[key] = {
                    'name': contact_name,
                    'tier': int(row.get('Tier', 2)),
                    'telegram_id': row.get('Telegram ID', '').strip(),
                    'telegram_username': row.get('Telegram Username', '').strip(),
                    'entries': []
                }

            # Add entry if it has content
            category = row.get('Category', '').strip()
            content = row.get('Content', '').strip()
            confidence = row.get('AI Confidence', '').strip()

            if category and content:
                entry_data = {
                    'category': category,
                    'content': content,
                    'confidence': float(confidence) if confidence.replace('.', '').isdigit() else None
                }

                # Apply confidence filter
                if confidence_threshold > 0 and entry_data['confidence'] and entry_data['confidence'] < confidence_threshold:
                    stats['entries_skipped'] += 1
                    continue

                contacts_data[key]['entries'].append(entry_data)

        # Process each contact
        for contact_key, contact_data in contacts_data.items():
            stats['contacts_processed'] += 1

            try:
                # Find existing contact
                existing_contact = session.query(Contact).filter(
                    Contact.user_id == current_user.id,
                    Contact.full_name.ilike(f"%{contact_data['name']}%")
                ).first()

                contact = None
                if existing_contact:
                    # Update existing contact
                    if merge_strategy in ['update_existing', 'merge_all']:
                        if not dry_run:
                            existing_contact.tier = contact_data['tier']
                            if contact_data['telegram_id']:
                                existing_contact.telegram_id = contact_data['telegram_id']
                            if contact_data['telegram_username']:
                                existing_contact.telegram_username = contact_data['telegram_username']
                            existing_contact.updated_at = datetime.utcnow()

                        stats['contacts_updated'] += 1
                        contact = existing_contact
                    else:
                        # Skip existing
                        contact = existing_contact
                else:
                    # Create new contact
                    if not dry_run:
                        contact = Contact(
                            user_id=current_user.id,
                            full_name=contact_data['name'],
                            tier=contact_data['tier'],
                            telegram_id=contact_data['telegram_id'] if contact_data['telegram_id'] else None,
                            telegram_username=contact_data['telegram_username'] if contact_data['telegram_username'] else None
                        )
                        session.add(contact)
                        session.flush()  # Get ID

                    stats['contacts_created'] += 1

                # Process entries
                for entry_data in contact_data['entries']:
                    if not dry_run and contact:
                        # Check for duplicate entry
                        if merge_strategy == 'skip_existing':
                            existing_entry = session.query(SynthesizedEntry).filter(
                                SynthesizedEntry.contact_id == contact.id,
                                SynthesizedEntry.category == entry_data['category'],
                                SynthesizedEntry.content == entry_data['content']
                            ).first()

                            if existing_entry:
                                stats['entries_skipped'] += 1
                                continue

                        # Create new entry
                        entry = SynthesizedEntry(
                            contact_id=contact.id,
                            category=entry_data['category'],
                            content=entry_data['content'],
                            confidence_score=entry_data['confidence']
                        )
                        session.add(entry)

                    stats['entries_created'] += 1

            except Exception as contact_error:
                logger.error(f"Error processing contact {contact_data['name']}: {contact_error}")
                stats['errors'].append(f"Contact '{contact_data['name']}': {str(contact_error)}")

        if not dry_run:
            session.commit()

        return {
            'success': True,
            'dry_run': dry_run,
            'merge_strategy': merge_strategy,
            'confidence_threshold': confidence_threshold,
            **stats
        }

    except Exception as e:
        session.rollback()
        logger.error(f"CSV merge process error: {e}")
        return {
            'success': False,
            'error': str(e),
            **stats
        }
    finally:
        session.close()
```

13. DEPLOYMENT & PRODUCTION CONFIGURATION
================================================================================

The platform is production-ready with comprehensive deployment configuration for Render.com and other hosting platforms.

**Production Dependencies:**

```python
# Production Requirements (requirements.txt:70-129)
# Core Flask and Web Framework
Flask==2.3.3
Flask-CORS==4.0.0                # CORS support for production
Flask-APScheduler==1.13.0         # Background task scheduling
Flask-Caching==2.3.0             # Caching layer (Redis/SimpleCache)
Flask-Login==0.6.3               # Session auth management
gunicorn==21.2.0                 # Production WSGI server
gevent==23.9.1                   # Async support for Gunicorn

# Database Support
SQLAlchemy==2.0.21               # ORM
psycopg2-binary==2.9.7           # PostgreSQL adapter for production

# AI & Machine Learning
google-generativeai==0.8.5       # Gemini AI integration
openai==0.28.1                   # OpenAI Whisper for transcription
google-cloud-vision==3.10.2      # Google Vision API for image analysis

# File Processing
PyPDF2==3.0.1                    # PDF processing
pdfplumber==0.11.7               # Advanced PDF text extraction
boto3>=1.35.0,<1.36.0           # AWS SDK for S3 file storage

# Communication & Integration
telethon==1.34.0                 # Telegram API client
redis==5.0.4                     # Redis client for Flask-Caching

# Monitoring & Logging
sentry-sdk==2.19.2               # Error tracking and monitoring

# Security & Authentication
bcrypt==4.3.0                    # Password hashing
```

**Production Deployment Script:**

```bash
#!/bin/bash
# Production Deployment Script (deploy.sh)

set -e  # Exit on any error

echo "üöÄ Starting Kith Platform deployment process..."

# Check if virtual environment is activated
if [[ "$VIRTUAL_ENV" == "" ]]; then
    echo "‚ùå Virtual environment not activated. Please run: source venv/bin/activate"
    exit 1
fi

# Run automated tests
echo "üß™ Running automated tests..."
python -m pytest test_app.py -v --tb=short

if [ $? -eq 0 ]; then
    echo "‚úÖ All tests passed!"
else
    echo "‚ùå Tests failed. Please fix issues before deployment."
    exit 1
fi

# Check code coverage
echo "üìä Checking code coverage..."
python -m pytest test_app.py --cov=app --cov-report=term-missing --cov-fail-under=65

# Validate environment configuration
echo "üîß Checking environment configuration..."
if [ -z "$GEMINI_API_KEY" ]; then
    echo "‚ö†Ô∏è  GEMINI_API_KEY not set in environment"
else
    echo "‚úÖ GEMINI_API_KEY is configured"
fi

if [ -z "$FLASK_SECRET_KEY" ]; then
    echo "‚ö†Ô∏è  FLASK_SECRET_KEY not set in environment"
else
    echo "‚úÖ FLASK_SECRET_KEY is configured"
fi

# Validate dependencies
echo "üì¶ Validating dependencies..."
pip check

if [ $? -eq 0 ]; then
    echo "‚úÖ Dependencies are compatible"
else
    echo "‚ùå Dependency conflicts found. Please resolve before deployment."
    exit 1
fi

# Production readiness checks
echo "üîß Production readiness checks..."

# Check if render.yaml exists
if [ -f "render.yaml" ]; then
    echo "‚úÖ Render.com configuration found"
else
    echo "‚ùå render.yaml not found. Please create deployment configuration."
    exit 1
fi

# Database migration check
echo "üóÑÔ∏è Checking database schema..."
python -c "from models import init_db; init_db(); print('‚úÖ Database schema validated')"

echo ""
echo "üéâ Deployment preparation complete!"
echo ""
echo "Next steps for production deployment:"
echo "1. Commit your changes: git add . && git commit -m 'Production ready'"
echo "2. Push to GitHub: git push origin main"
echo "3. Deploy on Render.com using the render.yaml configuration"
echo ""
echo "Required environment variables for production:"
echo "- GEMINI_API_KEY: Your Google AI Studio API key"
echo "- OPENAI_API_KEY: Your OpenAI API key for Whisper transcription"
echo "- FLASK_SECRET_KEY: Secret key for session management"
echo "- DATABASE_URL: PostgreSQL connection string (auto-provided by Render)"
echo "- DEFAULT_ADMIN_USER: Admin username (default: admin)"
echo "- DEFAULT_ADMIN_PASS: Admin password (default: admin123)"
```

**Render.com Configuration:**

```yaml
# render.yaml - Production deployment configuration
services:
  - type: web
    name: kith-platform
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: gunicorn --worker-class gevent --workers 2 --bind 0.0.0.0:$PORT app:app
    envVars:
      - key: PYTHON_VERSION
        value: 3.11.0
      - key: FLASK_ENV
        value: production
      - key: FLASK_SECRET_KEY
        generateValue: true
      - key: DEFAULT_ADMIN_USER
        value: admin
      - key: DEFAULT_ADMIN_PASS
        generateValue: true
    healthCheckPath: /health

databases:
  - name: kith-platform-db
    databaseName: kith_platform
    user: kith_user
```

**Production Flask Configuration:**

```python
# Production Configuration (app.py:1240-1280)
# Production-specific settings
if os.getenv('FLASK_ENV') == 'production':
    # Use PostgreSQL in production
    DATABASE_URL = os.getenv('DATABASE_URL')
    if DATABASE_URL and DATABASE_URL.startswith('postgres://'):
        DATABASE_URL = DATABASE_URL.replace('postgres://', 'postgresql://', 1)

    # Configure Gunicorn-compatible logging
    import logging
    from logging.handlers import RotatingFileHandler

    if not app.debug:
        # File logging for production
        if not os.path.exists('logs'):
            os.mkdir('logs')

        file_handler = RotatingFileHandler('logs/kith_platform.log',
                                         maxBytes=10240000, backupCount=10)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
        ))
        file_handler.setLevel(logging.INFO)
        app.logger.addHandler(file_handler)

        app.logger.setLevel(logging.INFO)
        app.logger.info('Kith Platform startup - Production Mode')

# Health check endpoint for monitoring
@app.route('/health')
def health_check():
    """Health check endpoint for production monitoring"""
    try:
        # Test database connection
        session = get_session()
        session.execute(text("SELECT 1"))
        session.close()

        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.utcnow().isoformat(),
            'version': '5.0',
            'database': 'connected'
        }), 200

    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500
```

**Database Migration & Schema Management:**

```python
# Database Migration Support (models.py:177-257)
def init_db():
    """Initialize the database and create tables with production support."""
    database_url = get_database_url()

    # Production vs Development detection
    is_production = database_url and database_url.startswith('postgresql://')

    try:
        engine = create_engine(database_url)
        Base.metadata.create_all(engine)

        # For PostgreSQL, ensure sequences are properly set up
        if is_production:
            try:
                from sqlalchemy import text
                with engine.connect() as conn:
                    # Fix sequences for all tables with auto-incrementing IDs
                    tables_with_sequences = ['users', 'contacts', 'raw_notes', 'synthesized_entries', 'tags']

                    for table_name in tables_with_sequences:
                        try:
                            # Get max ID from table
                            result = conn.execute(text(f"SELECT COALESCE(MAX(id), 0) FROM {table_name}"))
                            max_id = result.scalar()

                            # Set sequence to max_id + 1
                            sequence_name = f"{table_name}_id_seq"
                            conn.execute(text(f"SELECT setval('{sequence_name}', {max_id + 1}, false)"))
                            print(f"‚úÖ Ensured {sequence_name} is set to {max_id + 1}")

                        except Exception as seq_error:
                            print(f"‚ö†Ô∏è  Could not fix sequence for {table_name}: {seq_error}")
                            continue

                    conn.commit()
            except Exception as seq_fix_error:
                print(f"‚ö†Ô∏è  Sequence fix warning: {seq_fix_error}")

        print(f"‚úÖ Database initialized successfully: {'PostgreSQL' if is_production else 'SQLite'}")
        return engine

    except Exception as e:
        print(f"‚ùå Error initializing database: {e}")

        if is_production:
            # In production, don't fall back to SQLite - prevents data loss!
            print("üö® CRITICAL: PostgreSQL connection failed in production.")
            print("üîß Check your DATABASE_URL and PostgreSQL service status.")
            raise e  # Re-raise the exception to prevent silent fallback
        else:
            # Only fall back to SQLite in development
            print("üí° Development mode: Falling back to SQLite")
            engine = create_engine('sqlite:///kith_platform.db')
            Base.metadata.create_all(engine)
            return engine

def get_session():
    """Get a database session with connection retry logic."""
    database_url = get_database_url()

    # Add connection pooling and retry logic for production
    if database_url and database_url.startswith('postgresql://'):
        # PostgreSQL connection with connection pooling
        engine = create_engine(
            database_url,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,  # Validate connections before use
            pool_recycle=3600,   # Recycle connections after 1 hour
            echo=False
        )
    else:
        # SQLite connection for development
        engine = create_engine(database_url)

    Session = sessionmaker(bind=engine)
    return Session()
```

COMPREHENSIVE API REFERENCE
================================================================================

The Kith Platform provides over 60 RESTful API endpoints organized into functional categories:

**Authentication & User Management:**
- `POST /api/register` - User registration with auto-login
- `POST /api/login` - User authentication with session management
- `POST /api/logout` - Secure logout with session cleanup
- `GET /api/session` - Current session status and user info

**Admin Dashboard (Admin-only endpoints):**
- `GET /admin/dashboard` - Admin dashboard interface
- `GET /admin/api/users` - Get all users with statistics
- `GET /admin/api/users/<id>/contacts` - Get contacts for specific user
- `POST /admin/api/users/<id>/role` - Update user role (admin/user)
- `DELETE /admin/api/users/<id>/delete` - Delete user and all data
- `GET /admin/api/users/<id>/password` - View user password (support)
- `GET /admin/api/users/<id>/export/csv` - Export user data as CSV
- `POST /admin/api/users/<id>/import/csv` - Import CSV data for user

**Contact Management:**
- `GET /api/contacts` - List all user contacts with pagination
- `POST /api/contacts` - Create new contact with validation
- `GET /api/contact/<id>` - Get detailed contact information
- `PATCH /api/contact/<id>` - Update contact details
- `DELETE /api/contacts/<id>` - Delete single contact
- `POST /api/contacts/bulk-delete` - Delete multiple contacts
- `POST /api/import-vcard` - Import contacts from vCard format

**AI-Powered Note Processing:**
- `POST /api/process-note` - Analyze raw notes with Gemini AI
- `POST /api/save-synthesis` - Save AI analysis results
- `GET /api/contact/<id>/raw-logs` - View raw note history
- `PUT /api/contact/<id>/categories` - Update synthesized categories
- `GET /api/contact/<id>/audit-log` - View contact interaction history

**Voice & Audio Processing:**
- `POST /api/transcribe-audio` - Transcribe voice recordings to text
- `POST /api/notes` - Create note from transcribed audio

**File Upload & Analysis:**
- `POST /api/files/upload` - Upload files with AI content extraction
- `GET /api/files/status/<task_id>` - Check file analysis progress

**Telegram Integration:**
- `GET /api/telegram/status` - Check Telegram connection status
- `POST /api/telegram/auth/start` - Begin Telegram authentication
- `POST /api/telegram/auth/verify` - Verify Telegram auth code
- `POST /api/telegram/auth/password` - Handle 2FA password
- `POST /api/telegram/auth/cancel` - Cancel authentication process
- `POST /api/telegram/start-import` - Import chat history for contact
- `GET /api/telegram/import-status/<task_id>` - Check import progress
- `POST /api/telegram/direct-import` - Direct message import

**Relationship Graph & Visualization:**
- `GET /api/graph-data` - Get contacts and relationships for visualization
- `POST /api/relationships` - Create relationship between contacts
- `POST /api/groups` - Create contact groups
- `POST /api/groups/<id>/members` - Add members to group

**Advanced Analytics:**
- `GET /api/analytics/contact/<id>/health` - Relationship health score
- `GET /api/analytics/contact/<id>/trends` - Interaction trends analysis
- `GET /api/analytics/contact/<id>/recommendations` - AI recommendations
- `GET /api/analytics/network` - Overall network health insights

**Calendar Integration:**
- `GET /api/calendar/events` - Get upcoming calendar events
- `POST /api/calendar/create-from-actionable` - Create events from action items
- `POST /api/calendar/extract-datetime` - Extract dates from natural language

**Tag Management:**
- `GET /api/tags` - List all user tags with contact counts
- `POST /api/tags` - Create new tag with color and description
- `GET /api/tags/<id>` - Get tag details and associated contacts
- `PATCH /api/tags/<id>` - Update tag properties
- `DELETE /api/tags/<id>` - Delete tag and associations
- `GET /api/contacts/<id>/tags` - Get tags assigned to contact
- `POST /api/contacts/<id>/tags` - Assign tag to contact
- `DELETE /api/contacts/<id>/tags/<tag_id>` - Remove tag from contact

**Data Import/Export:**
- `GET /api/export/csv` - Export user data as CSV with timestamp
- `POST /api/import/merge-from-csv` - Import CSV with smart merge logic
- `GET /api/admin/api/export/all-users-csv` - Export all users data (admin)
- `POST /api/admin/api/import/all-users-csv` - Import for all users (admin)

**System Health & Monitoring:**
- `GET /health` - Basic health check for monitoring
- `GET /api/health` - Detailed health status with database connection
- `GET /api/ready` - Readiness probe for container orchestration
- `GET /debug/routes` - Debug endpoint listing all available routes

**Search & Discovery:**
- `GET /api/search` - Global search across contacts and notes
- `POST /api/contacts/seed` - Seed dummy data for testing

**Background Tasks:**
- `POST /api/reindex/start` - Start background reindexing task
- `GET /api/reindex/status/<task_id>` - Check reindexing progress

FRONTEND UI COMPONENTS & ARCHITECTURE
================================================================================

The frontend is built with vanilla JavaScript, HTML5, and CSS3, providing a modern, responsive interface with advanced interactive components.

**Main Interface Layout:**

```html
<!-- Core Layout Structure (templates/index.html:11-70) -->
<div class="container">
    <!-- Navigation System -->
    <div class="navigation">
        <button id="main-view-btn" class="nav-btn active">Contacts</button>
        <button id="graph-view-btn" class="nav-btn">Relationship Graph</button>
        <button id="manage-graph-btn" class="nav-btn">Manage Graph</button>
        <button id="settings-btn" class="nav-btn">Settings</button>
    </div>

    <!-- Main Contact Management View -->
    <div id="main-view">
        <div class="search-area">
            <input type="search" id="contact-search" placeholder="Search for a contact...">
            <label class="tier-filter">
                <input type="checkbox" id="tier-filter"> Tier 1 Only
            </label>
        </div>

        <!-- Note Input with Voice Recording -->
        <div id="input-area">
            <div class="note-container">
                <textarea id="note-input" placeholder="Enter your unstructured notes..."></textarea>
                <button id="record-btn" class="mic-btn" title="Start Voice Recording">üé§</button>
            </div>
            <button id="analyze-btn" disabled>Analyze Note</button>
        </div>

        <!-- Tiered Contact Lists -->
        <div class="tier1-contacts-section">
            <h3>Tier 1 Contacts</h3>
            <div id="tier1-contacts" class="tier1-contacts-list"></div>
        </div>

        <div class="tier2-contacts-section">
            <h3>Tier 2 Contacts</h3>
            <div id="tier2-contacts" class="tier1-contacts-list"></div>
        </div>
    </div>

    <!-- Individual Contact Profile View -->
    <div id="profile-view" style="display:none;">
        <div class="profile-actions-bar card">
            <button id="profile-add-note-btn">Add Note</button>
            <button id="profile-sync-telegram-btn">Sync Telegram Chat</button>
            <button id="profile-upload-file-btn">Upload File</button>
            <button id="edit-contact-profile-btn">Edit Profile Details</button>
            <button id="delete-contact-btn" class="danger-btn">Delete Contact</button>
        </div>

        <!-- Tags Management -->
        <div class="tags-section card">
            <div class="tags-header">
                <h3>Tags</h3>
                <button id="manage-tags-btn" class="secondary-btn">Manage Tags</button>
            </div>
            <div id="contact-tags-container" class="tags-container"></div>
        </div>

        <!-- Dynamic Category Sections -->
        <div id="contact-profile-content" class="profile-sections"></div>
    </div>
</div>
```

**Interactive Components:**

1. **Voice Recording Interface**: Browser-based audio capture with visual feedback
2. **File Upload with Progress**: Drag-and-drop file upload with AI analysis progress
3. **Relationship Graph**: Interactive network visualization using vis.js
4. **Tag Management**: Color-coded tag system with assignment interface
5. **Search & Filter**: Real-time search with tier-based filtering
6. **CSV Import/Export**: Bulk data management with merge options
7. **Telegram Integration**: Step-by-step authentication and import process

**Responsive Design System:**

```css
/* Modern CSS Architecture (static/style.css) */
:root {
    --primary-color: #4f6cff;
    --secondary-color: #97C2FC;
    --background-dark: #0b1020;
    --card-background: #111832;
    --border-color: #2a366b;
    --text-primary: #ffffff;
    --text-secondary: #9fb3ff;
    --success-color: #28a745;
    --error-color: #dc3545;
    --warning-color: #ffc107;
}

/* Responsive grid system */
.stats-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
    gap: 16px;
    margin-bottom: 24px;
}

/* Interactive elements */
.nav-btn {
    padding: 12px 20px;
    background: transparent;
    border: 2px solid var(--border-color);
    color: var(--text-secondary);
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
}

.nav-btn.active {
    background: var(--primary-color);
    color: var(--text-primary);
    border-color: var(--primary-color);
}

/* Card-based layout */
.card {
    background: var(--card-background);
    border: 1px solid var(--border-color);
    border-radius: 10px;
    padding: 20px;
    margin-bottom: 20px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.2);
}
```

SECURITY & PERFORMANCE CONSIDERATIONS
================================================================================

**Authentication Security:**
- PBKDF2-SHA256 password hashing with salt
- Session-based authentication with secure cookies
- Role-based access control (admin/user roles)
- CSRF protection on all state-changing operations
- Input validation and sanitization on all endpoints

**Data Protection:**
- SQL injection prevention through SQLAlchemy ORM
- File upload validation and sandboxing
- Secure file storage with generated filenames
- Environment variable management for secrets
- Rate limiting on authentication endpoints

**Performance Optimizations:**
- Database connection pooling for production
- Lazy loading of relationship data
- Pagination for large datasets
- Background processing for heavy operations
- Caching layer ready (Redis support)
- Optimized database queries with proper indexing

**Production Monitoring:**
- Comprehensive logging with rotation
- Health check endpoints for monitoring
- Error tracking with Sentry integration
- Performance metrics collection
- Background task status monitoring

DEVELOPMENT & TESTING FRAMEWORK
================================================================================

**Test Coverage:**
- 36+ comprehensive test cases covering all major features
- Unit tests for AI analysis functionality
- Integration tests for database operations
- API endpoint testing with authentication
- Error condition and edge case testing

**Quality Assurance:**
- Automated deployment script with validation checks
- Code coverage requirements (65%+ minimum)
- Dependency conflict checking
- Environment configuration validation
- Database schema migration testing

**Development Workflow:**
- Local SQLite for development
- PostgreSQL for production
- Environment-based configuration
- Hot reload for frontend development
- Comprehensive error handling and logging

CONCLUSION
================================================================================

The Kith Platform represents a sophisticated, production-ready personal intelligence and relationship management system. With its comprehensive feature set including:

‚úÖ **Multi-user authentication** with admin dashboard
‚úÖ **AI-powered analysis** using Gemini 1.5 Pro
‚úÖ **Voice transcription** with OpenAI Whisper
‚úÖ **File upload and analysis** for images, PDFs, and documents
‚úÖ **Interactive relationship graphs** with vis.js visualization
‚úÖ **Telegram integration** for message import and analysis
‚úÖ **Advanced analytics engine** with health scoring
‚úÖ **Calendar integration** with smart event creation
‚úÖ **Tag management system** for organization
‚úÖ **CSV import/export** for bulk data operations
‚úÖ **Production deployment** with monitoring and health checks

The platform is designed to be easily recreatable using this comprehensive documentation, with every major component, API endpoint, database schema, and configuration detail thoroughly documented with actual code examples and implementation specifics.

**Key Technical Achievements:**
- 60+ RESTful API endpoints
- Comprehensive database schema with 12+ tables
- Real-time background task processing
- Secure multi-user architecture
- Production-ready deployment configuration
- Extensive test coverage and quality assurance
- Modern responsive web interface
- Advanced AI integration with multiple providers

This documentation serves as both a complete technical specification and implementation guide, enabling any developer to understand, deploy, modify, or extend the Kith Platform according to their specific requirements.

**Repository Structure:**
```
kith-platform/
‚îú‚îÄ‚îÄ app.py                 # Main Flask application (5900+ lines)
‚îú‚îÄ‚îÄ models.py              # Database models and schemas
‚îú‚îÄ‚îÄ requirements.txt       # Production dependencies
‚îú‚îÄ‚îÄ deploy.sh              # Deployment automation script
‚îú‚îÄ‚îÄ render.yaml           # Production deployment config
‚îú‚îÄ‚îÄ templates/            # HTML templates
‚îÇ   ‚îú‚îÄ‚îÄ index.html        # Main application interface
‚îÇ   ‚îú‚îÄ‚îÄ admin_dashboard.html # Admin interface
‚îÇ   ‚îî‚îÄ‚îÄ login.html        # Authentication pages
‚îú‚îÄ‚îÄ static/               # Frontend assets
‚îÇ   ‚îú‚îÄ‚îÄ style.css         # Comprehensive CSS framework
‚îÇ   ‚îî‚îÄ‚îÄ js/               # JavaScript components
‚îú‚îÄ‚îÄ uploads/              # File storage directory
‚îú‚îÄ‚îÄ logs/                 # Application logs
‚îî‚îÄ‚îÄ tests/                # Test suite
```

**File Locations:**
- Main Application: `/Users/benedictleong/Library/Mobile Documents/com~apple~CloudDocs/2. Archive/2. Pet Projects /Spiderman-v3-main/kith-platform/app.py`
- Database Models: `/Users/benedictleong/Library/Mobile Documents/com~apple~CloudDocs/2. Archive/2. Pet Projects /Spiderman-v3-main/kith-platform/models.py`
- Requirements: `/Users/benedictleong/Library/Mobile Documents/com~apple~CloudDocs/2. Archive/2. Pet Projects /Spiderman-v3-main/kith-platform/requirements.txt`
- Deployment Script: `/Users/benedictleong/Library/Mobile Documents/com~apple~CloudDocs/2. Archive/2. Pet Projects /Spiderman-v3-main/kith-platform/deploy.sh`

The Kith Platform is now a fully-featured, production-ready personal intelligence system ready for deployment and real-world use.








Comprehensive Kith Platform Refactoring Implementation Plan


The implementation plan provides:

Line-by-line code changes with specific files to modify/remove
Complete new file structures with full implementations
Database migration scripts for zero-downtime deployment
Comprehensive testing framework with realistic coverage targets
Production-ready containerization with monitoring and health checks
Detailed deployment procedures for development and production



Phase 1: Database Architecture Modernization
1.1 Replace Mixed Database Strategy with Unified PostgreSQL
Scope: Eliminate SQLite/PostgreSQL switching logic and standardize on PostgreSQL for all environments.
Files to Modify:

models.py (lines 177-257)
app.py (database initialization sections)

OLD CODE TO REMOVE from models.py:
pythondef get_database_url():
    """Get database URL with fallback logic."""
    database_url = os.getenv('DATABASE_URL')
    if database_url:
        if database_url.startswith('postgres://'):
            database_url = database_url.replace('postgres://', 'postgresql://', 1)
        return database_url
    return 'sqlite:///kith_platform.db'

def init_db():
    """Initialize the database and create tables with production support."""
    database_url = get_database_url()
    
    # Production vs Development detection
    is_production = database_url and database_url.startswith('postgresql://')
    
    try:
        engine = create_engine(database_url)
        # ... complex fallback logic
NEW CODE for config/database.py:
pythonimport os
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class DatabaseConfig:
    @staticmethod
    def get_database_url():
        """Get PostgreSQL database URL for all environments."""
        database_url = os.getenv('DATABASE_URL')
        if not database_url:
            # Default development PostgreSQL connection
            database_url = os.getenv(
                'DEV_DATABASE_URL', 
                'postgresql://postgres:postgres@localhost:5432/kith_dev'
            )
        
        if database_url.startswith('postgres://'):
            database_url = database_url.replace('postgres://', 'postgresql://', 1)
        
        return database_url
    
    @staticmethod
    def create_engine():
        """Create SQLAlchemy engine with optimized settings."""
        database_url = DatabaseConfig.get_database_url()
        
        return create_engine(
            database_url,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=os.getenv('SQLALCHEMY_ECHO', '').lower() == 'true'
        )
1.2 Implement Proper Database Migrations with Alembic
Scope: Replace manual table creation with proper migration system.
NEW FILE: alembic.ini
ini[alembic]
script_location = migrations
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = 

[post_write_hooks]

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
NEW FILE: migrations/env.py
pythonfrom logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from models import Base
from config.database import DatabaseConfig

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata

def get_url():
    return DatabaseConfig.get_database_url()

def run_migrations_offline() -> None:
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    configuration = config.get_section(config.config_ini_section)
    configuration["sqlalchemy.url"] = get_url()
    
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
COMMAND SEQUENCE for initial migration setup:
bash# Initialize Alembic
alembic init migrations

# Create initial migration
alembic revision --autogenerate -m "Initial database schema"

# Apply migrations
alembic upgrade head
1.3 Replace JSON String Storage with Proper JSON Columns
OLD CODE TO REPLACE in models.py (line 61):
pythontags = Column(String)  # JSON string for SQLite compatibility
NEW CODE for models.py:
pythonfrom sqlalchemy.dialects.postgresql import JSON

class RawNote(Base):
    __tablename__ = 'raw_notes'

    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'))
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    metadata_tags = Column(JSON)  # Proper JSON column
    
    contact = relationship("Contact", back_populates="raw_notes")

class Contact(Base):
    __tablename__ = 'contacts'
    
    # ... existing fields ...
    
    # Replace any JSON string fields with proper JSON
    telegram_metadata = Column(JSON)  # For storing complex Telegram data
    custom_fields = Column(JSON)      # For extensible contact fields
MIGRATION FILE migrations/versions/002_convert_json_fields.py:
python"""Convert string JSON fields to proper JSON columns

Revision ID: 002
Revises: 001
Create Date: 2024-01-01 12:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSON

# revision identifiers
revision = '002'
down_revision = '001'
branch_labels = None
depends_on = None

def upgrade() -> None:
    # Add new JSON columns
    op.add_column('raw_notes', sa.Column('metadata_tags', JSON))
    op.add_column('contacts', sa.Column('telegram_metadata', JSON))
    op.add_column('contacts', sa.Column('custom_fields', JSON))
    
    # Migrate data from string fields
    op.execute("""
        UPDATE raw_notes 
        SET metadata_tags = CASE 
            WHEN tags IS NOT NULL AND tags != '' THEN tags::json
            ELSE NULL 
        END
    """)
    
    # Drop old string columns
    op.drop_column('raw_notes', 'tags')

def downgrade() -> None:
    op.add_column('raw_notes', sa.Column('tags', sa.String))
    
    # Convert JSON back to string
    op.execute("""
        UPDATE raw_notes 
        SET tags = CASE 
            WHEN metadata_tags IS NOT NULL THEN metadata_tags::text
            ELSE NULL 
        END
    """)
    
    op.drop_column('contacts', 'custom_fields')
    op.drop_column('contacts', 'telegram_metadata')
    op.drop_column('raw_notes', 'metadata_tags')
1.4 Implement Comprehensive Indexing Strategy
NEW FILE: migrations/versions/003_add_performance_indexes.py
python"""Add performance indexes

Revision ID: 003
Revises: 002
Create Date: 2024-01-01 12:30:00.000000

"""
from alembic import op
import sqlalchemy as sa

revision = '003'
down_revision = '002'

def upgrade() -> None:
    # User-based queries
    op.create_index('idx_contacts_user_id', 'contacts', ['user_id'])
    op.create_index('idx_contacts_user_tier', 'contacts', ['user_id', 'tier'])
    op.create_index('idx_contacts_user_name', 'contacts', ['user_id', 'full_name'])
    
    # Contact-based queries
    op.create_index('idx_raw_notes_contact', 'raw_notes', ['contact_id'])
    op.create_index('idx_synthesized_contact', 'synthesized_entries', ['contact_id'])
    op.create_index('idx_synthesized_category', 'synthesized_entries', ['contact_id', 'category'])
    
    # Time-based queries
    op.create_index('idx_raw_notes_created', 'raw_notes', ['created_at'])
    op.create_index('idx_synthesized_created', 'synthesized_entries', ['created_at'])
    op.create_index('idx_contacts_updated', 'contacts', ['updated_at'])
    
    # Search optimization
    op.create_index('idx_contacts_name_gin', 'contacts', ['full_name'], postgresql_using='gin', postgresql_ops={'full_name': 'gin_trgm_ops'})
    op.create_index('idx_synthesized_content_gin', 'synthesized_entries', ['content'], postgresql_using='gin', postgresql_ops={'content': 'gin_trgm_ops'})
    
    # Telegram integration
    op.create_index('idx_contacts_telegram_id', 'contacts', ['telegram_id'])
    op.create_index('idx_contacts_telegram_username', 'contacts', ['telegram_username'])
    
    # Task management
    op.create_index('idx_import_tasks_user_status', 'import_tasks', ['user_id', 'status'])
    op.create_index('idx_import_tasks_created', 'import_tasks', ['created_at'])

def downgrade() -> None:
    op.drop_index('idx_import_tasks_created')
    op.drop_index('idx_import_tasks_user_status')
    op.drop_index('idx_contacts_telegram_username')
    op.drop_index('idx_contacts_telegram_id')
    op.drop_index('idx_synthesized_content_gin')
    op.drop_index('idx_contacts_name_gin')
    op.drop_index('idx_contacts_updated')
    op.drop_index('idx_synthesized_created')
    op.drop_index('idx_raw_notes_created')
    op.drop_index('idx_synthesized_category')
    op.drop_index('idx_synthesized_contact')
    op.drop_index('idx_raw_notes_contact')
    op.drop_index('idx_contacts_user_name')
    op.drop_index('idx_contacts_user_tier')
    op.drop_index('idx_contacts_user_id')
Phase 2: Modular Architecture Implementation
2.1 Break Down Monolithic app.py
OLD CODE TO REMOVE from app.py:

All route handlers (keep only app initialization)
All business logic functions
All database session management

NEW DIRECTORY STRUCTURE:
kith-platform/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contacts.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ notes.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ telegram.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ admin.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analytics.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ telegram_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analytics_service.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contact.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ note.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ task.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validators.py
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ settings.py
‚îÇ       ‚îî‚îÄ‚îÄ database.py
‚îú‚îÄ‚îÄ migrations/
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ main.py
NEW FILE: app/__init__.py
pythonfrom flask import Flask
from flask_login import LoginManager
from flask_cors import CORS
from config.settings import Config
from config.database import DatabaseConfig
import logging

def create_app(config_class=Config):
    app = Flask(__name__)
    app.config.from_object(config_class)
    
    # Initialize extensions
    login_manager = LoginManager()
    login_manager.init_app(app)
    login_manager.login_view = 'auth.login'
    
    CORS(app)
    
    # Configure logging
    if not app.debug:
        configure_logging(app)
    
    # Register blueprints
    from app.api.auth import auth_bp
    from app.api.contacts import contacts_bp
    from app.api.notes import notes_bp
    from app.api.telegram import telegram_bp
    from app.api.admin import admin_bp
    from app.api.analytics import analytics_bp
    
    app.register_blueprint(auth_bp, url_prefix='/api/auth')
    app.register_blueprint(contacts_bp, url_prefix='/api/contacts')
    app.register_blueprint(notes_bp, url_prefix='/api/notes')
    app.register_blueprint(telegram_bp, url_prefix='/api/telegram')
    app.register_blueprint(admin_bp, url_prefix='/api/admin')
    app.register_blueprint(analytics_bp, url_prefix='/api/analytics')
    
    @login_manager.user_loader
    def load_user(user_id):
        from app.services.auth_service import AuthService
        return AuthService.get_user_by_id(user_id)
    
    return app

def configure_logging(app):
    import os
    from logging.handlers import RotatingFileHandler
    
    if not os.path.exists('logs'):
        os.mkdir('logs')
    
    file_handler = RotatingFileHandler(
        'logs/kith_platform.log',
        maxBytes=10240000,
        backupCount=10
    )
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
    ))
    file_handler.setLevel(logging.INFO)
    app.logger.addHandler(file_handler)
    app.logger.setLevel(logging.INFO)
NEW FILE: config/settings.py
pythonimport os
from datetime import timedelta

class Config:
    SECRET_KEY = os.getenv('FLASK_SECRET_KEY', 'development-key-change-in-production')
    
    # Database
    SQLALCHEMY_DATABASE_URI = os.getenv('DATABASE_URL')
    SQLALCHEMY_TRACK_MODIFICATIONS = False
    
    # Session
    PERMANENT_SESSION_LIFETIME = timedelta(days=7)
    SESSION_COOKIE_SECURE = os.getenv('FLASK_ENV') == 'production'
    SESSION_COOKIE_HTTPONLY = True
    SESSION_COOKIE_SAMESITE = 'Lax'
    
    # File uploads
    MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB max file size
    UPLOAD_FOLDER = os.path.join(os.getcwd(), 'uploads')
    
    # External APIs
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
    
    # Celery configuration
    CELERY_BROKER_URL = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
    CELERY_RESULT_BACKEND = os.getenv('REDIS_URL', 'redis://localhost:6379/0')

class DevelopmentConfig(Config):
    DEBUG = True
    SQLALCHEMY_DATABASE_URI = os.getenv(
        'DEV_DATABASE_URL',
        'postgresql://postgres:postgres@localhost:5432/kith_dev'
    )

class ProductionConfig(Config):
    DEBUG = False
    # Production-specific settings

class TestingConfig(Config):
    TESTING = True
    SQLALCHEMY_DATABASE_URI = 'postgresql://postgres:postgres@localhost:5432/kith_test'
2.2 Implement Dependency Injection Pattern
NEW FILE: app/utils/dependencies.py
pythonfrom functools import lru_cache
from app.services.ai_service import AIService
from app.services.telegram_service import TelegramService
from app.services.file_service import FileService
from app.services.analytics_service import AnalyticsService
from app.utils.database import DatabaseManager

class Container:
    """Dependency injection container"""
    
    def __init__(self):
        self._database_manager = None
        self._ai_service = None
        self._telegram_service = None
        self._file_service = None
        self._analytics_service = None
    
    @property
    def database_manager(self) -> DatabaseManager:
        if self._database_manager is None:
            self._database_manager = DatabaseManager()
        return self._database_manager
    
    @property
    def ai_service(self) -> AIService:
        if self._ai_service is None:
            self._ai_service = AIService()
        return self._ai_service
    
    @property
    def telegram_service(self) -> TelegramService:
        if self._telegram_service is None:
            self._telegram_service = TelegramService()
        return self._telegram_service
    
    @property
    def file_service(self) -> FileService:
        if self._file_service is None:
            self._file_service = FileService()
        return self._file_service
    
    @property
    def analytics_service(self) -> AnalyticsService:
        if self._analytics_service is None:
            self._analytics_service = AnalyticsService(self.database_manager)
        return self._analytics_service

# Global container instance
container = Container()
NEW FILE: app/utils/database.py
pythonfrom contextlib import contextmanager
from sqlalchemy.orm import sessionmaker
from config.database import DatabaseConfig

class DatabaseManager:
    def __init__(self):
        self.engine = DatabaseConfig.create_engine()
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    @contextmanager
    def get_session(self):
        """Context manager for database sessions"""
        session = self.SessionLocal()
        try:
            yield session
            session.commit()
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()
    
    def get_session_sync(self):
        """Get session for synchronous operations"""
        return self.SessionLocal()
    
    def close_session(self, session):
        """Properly close a session"""
        try:
            session.close()
        except Exception:
            pass
2.3 Separate Business Logic from API Handlers
OLD CODE TO REMOVE from app.py (process_note function, lines 2951-3037):
python@app.route('/api/process-note', methods=['POST'])
@login_required
def process_note():
    # ... 86 lines of mixed API handling and business logic
NEW FILE: app/api/notes.py
pythonfrom flask import Blueprint, request, jsonify
from flask_login import login_required, current_user
from app.services.note_service import NoteService
from app.utils.dependencies import container
from app.utils.validators import validate_note_input
import logging

notes_bp = Blueprint('notes', __name__)
logger = logging.getLogger(__name__)

@notes_bp.route('/process', methods=['POST'])
@login_required
def process_note():
    """Process raw note with AI analysis"""
    try:
        # Validate input
        data = request.get_json()
        validation_result = validate_note_input(data)
        if not validation_result.is_valid:
            return jsonify({'error': validation_result.error}), 400
        
        # Delegate to service
        note_service = NoteService(container.database_manager, container.ai_service)
        result = note_service.process_note(
            contact_id=data['contact_id'],
            content=data['content'],
            user_id=current_user.id
        )
        
        return jsonify(result)
        
    except ValueError as e:
        logger.warning(f"Invalid note processing request: {e}")
        return jsonify({'error': str(e)}), 400
    except Exception as e:
        logger.error(f"Error processing note: {e}")
        return jsonify({'error': 'Internal server error'}), 500

@notes_bp.route('/<int:contact_id>/raw', methods=['GET'])
@login_required
def get_raw_notes(contact_id):
    """Get raw notes for a contact"""
    try:
        note_service = NoteService(container.database_manager, container.ai_service)
        notes = note_service.get_raw_notes(contact_id, current_user.id)
        return jsonify({'notes': notes})
    except Exception as e:
        logger.error(f"Error getting raw notes: {e}")
        return jsonify({'error': 'Internal server error'}), 500
NEW FILE: app/services/note_service.py
pythonfrom datetime import datetime
from typing import Dict, List, Any, Optional
from app.models.note import RawNote, SynthesizedEntry
from app.models.contact import Contact
from app.services.ai_service import AIService
from app.utils.database import DatabaseManager
import logging

logger = logging.getLogger(__name__)

class NoteService:
    def __init__(self, db_manager: DatabaseManager, ai_service: AIService):
        self.db_manager = db_manager
        self.ai_service = ai_service
    
    def process_note(self, contact_id: int, content: str, user_id: int) -> Dict[str, Any]:
        """Process a raw note with AI analysis"""
        with self.db_manager.get_session() as session:
            # Verify contact ownership
            contact = self._get_user_contact(session, contact_id, user_id)
            if not contact:
                raise ValueError("Contact not found")
            
            # Save raw note
            raw_note = RawNote(
                contact_id=contact_id,
                content=content.strip(),
                created_at=datetime.utcnow()
            )
            session.add(raw_note)
            session.flush()
            
            # Process with AI
            try:
                analysis_result = self.ai_service.analyze_note(
                    content=content,
                    contact_name=contact.full_name
                )
                
                # Save synthesized entries
                synthesis_results = []
                for category, data in analysis_result.categories.items():
                    if data.content and len(data.content.strip()) > 10:
                        entry = SynthesizedEntry(
                            contact_id=contact_id,
                            category=category,
                            content=data.content,
                            confidence_score=data.confidence,
                            created_at=datetime.utcnow()
                        )
                        session.add(entry)
                        synthesis_results.append({
                            'category': category,
                            'content': data.content,
                            'confidence': data.confidence
                        })
                
                return {
                    'success': True,
                    'raw_note_id': raw_note.id,
                    'synthesis': synthesis_results,
                    'contact_name': contact.full_name
                }
                
            except Exception as ai_error:
                logger.error(f"AI analysis failed: {ai_error}")
                # Still save the raw note even if AI analysis fails
                return {
                    'success': True,
                    'raw_note_id': raw_note.id,
                    'synthesis': [],
                    'ai_error': str(ai_error),
                    'contact_name': contact.full_name
                }
    
    def get_raw_notes(self, contact_id: int, user_id: int) -> List[Dict[str, Any]]:
        """Get raw notes for a contact"""
        with self.db_manager.get_session() as session:
            contact = self._get_user_contact(session, contact_id, user_id)
            if not contact:
                raise ValueError("Contact not found")
            
            notes = session.query(RawNote).filter_by(contact_id=contact_id).order_by(RawNote.created_at.desc()).all()
            
            return [{
                'id': note.id,
                'content': note.content,
                'created_at': note.created_at.isoformat(),
                'metadata_tags': note.metadata_tags
            } for note in notes]
    
    def _get_user_contact(self, session, contact_id: int, user_id: int) -> Optional[Contact]:
        """Get contact if it belongs to the user"""
        return session.query(Contact).filter_by(
            id=contact_id,
            user_id=user_id
        ).first()
Phase 3: Background Job Processing with Celery
3.1 Replace Threading with Celery
OLD CODE TO REMOVE from app.py:
pythonimport threading
analysis_thread = threading.Thread(
    target=analyze_file_content,
    args=(file_path, file_type, task_id, contact_id, uploaded_file.id)
)
analysis_thread.daemon = True
analysis_thread.start()
NEW FILE: app/tasks/__init__.py
pythonfrom celery import Celery
from config.settings import Config

def create_celery_app(app=None):
    celery = Celery(app.import_name if app else 'kith-platform')
    
    if app:
        celery.conf.update(
            broker_url=app.config['CELERY_BROKER_URL'],
            result_backend=app.config['CELERY_RESULT_BACKEND'],
            task_serializer='json',
            accept_content=['json'],
            result_serializer='json',
            timezone='UTC',
            enable_utc=True,
            task_track_started=True,
            task_time_limit=30 * 60,  # 30 minutes
            task_soft_time_limit=25 * 60,  # 25 minutes
        )
        
        class ContextTask(celery.Task):
            """Make celery tasks work with Flask app context."""
            def __call__(self, *args, **kwargs):
                with app.app_context():
                    return self.run(*args, **kwargs)
        
        celery.Task = ContextTask
    
    return celery

celery = create_celery_app()
NEW FILE: app/tasks/file_tasks.py
pythonfrom app.tasks import celery
from app.services.file_service import FileService
from app.utils.dependencies import container
from app.models.task import ImportTask
import logging

logger = logging.getLogger(__name__)

@celery.task(bind=True)
def analyze_file_content(self, file_path: str, file_type: str, task_id: str, contact_id: int, file_id: int):
    """Analyze uploaded file content in background"""
    try:
        # Update task status
        self.update_state(state='PROGRESS', meta={'progress': 25, 'status': 'Analyzing file content...'})
        
        # Process file
        file_service = FileService(container.database_manager, container.ai_service)
        result = file_service.analyze_file(file_path, file_type, contact_id, file_id)
        
        # Update task completion
        with container.database_manager.get_session() as session:
            task = session.query(ImportTask).filter_by(id=task_id).first()
            if task:
                task.status = 'completed'
                task.progress = 100
                task.status_message = 'File analysis completed successfully'
                task.completed_at = datetime.utcnow()
        
        return {
            'status': 'completed',
            'result': result,
            'progress': 100
        }
        
    except Exception as e:
        logger.error(f"File analysis error: {e}")
        
        # Update task failure
        with container.database_manager.get_session() as session:
            task = session.query(ImportTask).filter_by(id=task_id).first()
            if task:
                task.status = 'failed'
                task.error_details = str(e)
                task.status_message = f'Analysis failed: {str(e)}'
        
        self.update_state(
            state='FAILURE',
            meta={'error': str(e)}
        )
        raise

@celery.task(bind=True)
def import_telegram_messages(self, task_id: str, user_id: int, contact_id: int):
    """Import Telegram messages in background"""
    try:
        self.update_state(state='PROGRESS', meta={'progress': 10, 'status': 'Connecting to Telegram...'})
        
        telegram_service = container.telegram_service
        result = telegram_service.import_messages(user_id, contact_id, task_callback=self.update_state)
        
        return {
            'status': 'completed',
            'result': result,
            'progress': 100
        }
        
    except Exception as e:
        logger.error(f"Telegram import error: {e}")
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
NEW FILE: celery_app.py (Worker startup file):
pythonfrom app import create_app
from app.tasks import create_celery_app

app = create_app()
celery = create_celery_app(app)

if __name__ == '__main__':
    celery.start()
3.2 Update API Endpoints to Use Celery Tasks
REPLACE file upload endpoint in app/api/files.py:
pythonfrom flask import Blueprint, request, jsonify
from flask_login import login_required, current_user
from app.tasks.file_tasks import analyze_file_content
from app.utils.dependencies import container

files_bp = Blueprint('files', __name__)

@files_bp.route('/upload', methods=['POST'])
@login_required
def upload_file():
    """Upload file and start background analysis"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400

    file = request.files['file']
    contact_id = request.form.get('contact_id')

    if not file.filename:
        return jsonify({'error': 'No file selected'}), 400

    try:
        # Save file and create database records
        file_service = container.file_service
        upload_result = file_service.save_uploaded_file(
            file, contact_id, current_user.id


3.2 Update API Endpoints to Use Celery Tasks (Continued)
REPLACE file upload endpoint in app/api/files.py:
pythonfrom flask import Blueprint, request, jsonify
from flask_login import login_required, current_user
from app.tasks.file_tasks import analyze_file_content
from app.utils.dependencies import container

files_bp = Blueprint('files', __name__)

@files_bp.route('/upload', methods=['POST'])
@login_required
def upload_file():
    """Upload file and start background analysis"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400

    file = request.files['file']
    contact_id = request.form.get('contact_id')

    if not file.filename:
        return jsonify({'error': 'No file selected'}), 400

    try:
        # Save file and create database records
        file_service = container.file_service
        upload_result = file_service.save_uploaded_file(
            file, contact_id, current_user.id
        )
        
        # Start background analysis task
        task = analyze_file_content.delay(
            upload_result['file_path'],
            upload_result['file_type'],
            upload_result['task_id'],
            upload_result['contact_id'],
            upload_result['file_id']
        )
        
        return jsonify({
            'success': True,
            'task_id': upload_result['task_id'],
            'celery_task_id': task.id,
            'file_id': upload_result['file_id'],
            'message': 'File uploaded successfully. Analysis in progress...'
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@files_bp.route('/status/<task_id>', methods=['GET'])
@login_required
def get_file_analysis_status(task_id):
    """Get file analysis task status"""
    try:
        # Get Celery task status
        from app.tasks.file_tasks import analyze_file_content
        task = analyze_file_content.AsyncResult(task_id)
        
        if task.state == 'PENDING':
            response = {'state': task.state, 'progress': 0, 'status': 'Task pending...'}
        elif task.state == 'PROGRESS':
            response = {
                'state': task.state,
                'progress': task.info.get('progress', 0),
                'status': task.info.get('status', 'Processing...')
            }
        elif task.state == 'SUCCESS':
            response = {
                'state': task.state,
                'progress': 100,
                'result': task.info
            }
        else:  # FAILURE
            response = {
                'state': task.state,
                'error': str(task.info)
            }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500
Phase 4: Consistent Error Handling
4.1 Implement Global Error Handling System
NEW FILE: app/utils/exceptions.py
pythonclass KithPlatformException(Exception):
    """Base exception for Kith Platform"""
    def __init__(self, message, status_code=500, error_code=None):
        self.message = message
        self.status_code = status_code
        self.error_code = error_code
        super().__init__(self.message)

class ValidationError(KithPlatformException):
    """Input validation error"""
    def __init__(self, message, field=None):
        self.field = field
        super().__init__(message, status_code=400, error_code='VALIDATION_ERROR')

class AuthenticationError(KithPlatformException):
    """Authentication error"""
    def __init__(self, message="Authentication required"):
        super().__init__(message, status_code=401, error_code='AUTH_ERROR')

class AuthorizationError(KithPlatformException):
    """Authorization error"""
    def __init__(self, message="Access denied"):
        super().__init__(message, status_code=403, error_code='AUTHORIZATION_ERROR')

class ResourceNotFoundError(KithPlatformException):
    """Resource not found error"""
    def __init__(self, message="Resource not found"):
        super().__init__(message, status_code=404, error_code='NOT_FOUND')

class ExternalServiceError(KithPlatformException):
    """External service error"""
    def __init__(self, service_name, message):
        self.service_name = service_name
        super().__init__(f"{service_name} error: {message}", status_code=502, error_code='EXTERNAL_SERVICE_ERROR')
NEW FILE: app/utils/error_handlers.py
pythonfrom flask import jsonify, request
import logging
from app.utils.exceptions import KithPlatformException, ValidationError

logger = logging.getLogger(__name__)

def register_error_handlers(app):
    """Register global error handlers"""
    
    @app.errorhandler(KithPlatformException)
    def handle_kith_platform_exception(error):
        logger.warning(f"Application error: {error.message} (Code: {error.error_code})")
        return jsonify({
            'error': error.message,
            'error_code': error.error_code,
            'status_code': error.status_code
        }), error.status_code
    
    @app.errorhandler(ValidationError)
    def handle_validation_error(error):
        logger.warning(f"Validation error: {error.message} (Field: {error.field})")
        return jsonify({
            'error': error.message,
            'error_code': error.error_code,
            'field': error.field,
            'status_code': error.status_code
        }), error.status_code
    
    @app.errorhandler(404)
    def not_found_error(error):
        return jsonify({
            'error': 'Resource not found',
            'error_code': 'NOT_FOUND',
            'status_code': 404
        }), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        logger.error(f"Internal server error: {error}", exc_info=True)
        return jsonify({
            'error': 'Internal server error',
            'error_code': 'INTERNAL_ERROR',
            'status_code': 500
        }), 500
    
    @app.errorhandler(Exception)
    def handle_unexpected_error(error):
        logger.error(f"Unexpected error: {error}", exc_info=True)
        return jsonify({
            'error': 'An unexpected error occurred',
            'error_code': 'UNEXPECTED_ERROR',
            'status_code': 500
        }), 500
4.2 Implement Consistent Input Validation
NEW FILE: app/utils/validators.py
pythonfrom dataclasses import dataclass
from typing import Optional, Dict, Any
import re
from app.utils.exceptions import ValidationError

@dataclass
class ValidationResult:
    is_valid: bool
    error: Optional[str] = None
    field: Optional[str] = None

class BaseValidator:
    """Base class for input validators"""
    
    @staticmethod
    def validate_required_fields(data: Dict[str, Any], required_fields: list) -> ValidationResult:
        """Validate that all required fields are present and non-empty"""
        for field in required_fields:
            if field not in data or not str(data[field]).strip():
                return ValidationResult(
                    is_valid=False,
                    error=f"{field} is required",
                    field=field
                )
        return ValidationResult(is_valid=True)
    
    @staticmethod
    def validate_email(email: str) -> ValidationResult:
        """Validate email format"""
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(pattern, email):
            return ValidationResult(
                is_valid=False,
                error="Invalid email format",
                field="email"
            )
        return ValidationResult(is_valid=True)
    
    @staticmethod
    def validate_string_length(value: str, field: str, min_length: int = 0, max_length: int = None) -> ValidationResult:
        """Validate string length"""
        if len(value) < min_length:
            return ValidationResult(
                is_valid=False,
                error=f"{field} must be at least {min_length} characters",
                field=field
            )
        
        if max_length and len(value) > max_length:
            return ValidationResult(
                is_valid=False,
                error=f"{field} must not exceed {max_length} characters",
                field=field
            )
        
        return ValidationResult(is_valid=True)

class NoteValidator(BaseValidator):
    """Validator for note-related inputs"""
    
    @classmethod
    def validate_note_input(cls, data: Dict[str, Any]) -> ValidationResult:
        """Validate note processing input"""
        # Check required fields
        required_result = cls.validate_required_fields(data, ['contact_id', 'content'])
        if not required_result.is_valid:
            return required_result
        
        # Validate contact_id is integer
        try:
            contact_id = int(data['contact_id'])
            if contact_id <= 0:
                return ValidationResult(
                    is_valid=False,
                    error="contact_id must be a positive integer",
                    field="contact_id"
                )
        except (ValueError, TypeError):
            return ValidationResult(
                is_valid=False,
                error="contact_id must be an integer",
                field="contact_id"
            )
        
        # Validate content length
        content = str(data['content']).strip()
        length_result = cls.validate_string_length(content, "content", min_length=10, max_length=10000)
        if not length_result.is_valid:
            return length_result
        
        return ValidationResult(is_valid=True)

class ContactValidator(BaseValidator):
    """Validator for contact-related inputs"""
    
    @classmethod
    def validate_contact_creation(cls, data: Dict[str, Any]) -> ValidationResult:
        """Validate contact creation input"""
        required_result = cls.validate_required_fields(data, ['full_name'])
        if not required_result.is_valid:
            return required_result
        
        # Validate name length
        name = str(data['full_name']).strip()
        name_result = cls.validate_string_length(name, "full_name", min_length=2, max_length=255)
        if not name_result.is_valid:
            return name_result
        
        # Validate tier if provided
        if 'tier' in data:
            try:
                tier = int(data['tier'])
                if tier not in [1, 2]:
                    return ValidationResult(
                        is_valid=False,
                        error="tier must be 1 or 2",
                        field="tier"
                    )
            except (ValueError, TypeError):
                return ValidationResult(
                    is_valid=False,
                    error="tier must be an integer",
                    field="tier"
                )
        
        return ValidationResult(is_valid=True)

# Convenience functions
def validate_note_input(data: Dict[str, Any]) -> ValidationResult:
    return NoteValidator.validate_note_input(data)

def validate_contact_creation(data: Dict[str, Any]) -> ValidationResult:
    return ContactValidator.validate_contact_creation(data)
4.3 Update All API Endpoints with Consistent Error Handling
UPDATE app/api/contacts.py to use new error handling:
pythonfrom flask import Blueprint, request, jsonify
from flask_login import login_required, current_user
from app.services.contact_service import ContactService
from app.utils.dependencies import container
from app.utils.validators import validate_contact_creation
from app.utils.exceptions import ValidationError, ResourceNotFoundError
import logging

contacts_bp = Blueprint('contacts', __name__)
logger = logging.getLogger(__name__)

@contacts_bp.route('/', methods=['POST'])
@login_required
def create_contact():
    """Create a new contact"""
    data = request.get_json()
    if not data:
        raise ValidationError("Request body is required")
    
    # Validate input
    validation_result = validate_contact_creation(data)
    if not validation_result.is_valid:
        raise ValidationError(validation_result.error, field=validation_result.field)
    
    # Create contact
    contact_service = ContactService(container.database_manager)
    contact = contact_service.create_contact(
        user_id=current_user.id,
        full_name=data['full_name'].strip(),
        tier=data.get('tier', 2),
        telegram_handle=data.get('telegram_handle', '').strip() or None
    )
    
    return jsonify({
        'success': True,
        'contact': contact.to_dict()
    }), 201

@contacts_bp.route('/<int:contact_id>', methods=['GET'])
@login_required
def get_contact(contact_id):
    """Get contact details"""
    contact_service = ContactService(container.database_manager)
    contact = contact_service.get_user_contact(contact_id, current_user.id)
    
    if not contact:
        raise ResourceNotFoundError("Contact not found")
    
    return jsonify({
        'contact': contact.to_dict()
    })

@contacts_bp.route('/<int:contact_id>', methods=['DELETE'])
@login_required
def delete_contact(contact_id):
    """Delete a contact"""
    contact_service = ContactService(container.database_manager)
    success = contact_service.delete_contact(contact_id, current_user.id)
    
    if not success:
        raise ResourceNotFoundError("Contact not found")
    
    return jsonify({
        'success': True,
        'message': 'Contact deleted successfully'
    })
Phase 5: Proper Testing Implementation
5.1 Replace Questionable Test Suite with Comprehensive Testing
REMOVE ALL OLD TEST FILES - The current test coverage claims are unreliable.
NEW FILE: tests/conftest.py
pythonimport pytest
import os
import tempfile
from app import create_app
from app.models import Base
from config.database import DatabaseConfig
from config.settings import TestingConfig
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

@pytest.fixture(scope='session')
def app():
    """Create application for testing"""
    app = create_app(TestingConfig)
    
    with app.app_context():
        yield app

@pytest.fixture(scope='session')
def test_engine(app):
    """Create test database engine"""
    engine = create_engine(app.config['SQLALCHEMY_DATABASE_URI'])
    Base.metadata.create_all(engine)
    yield engine
    Base.metadata.drop_all(engine)

@pytest.fixture
def db_session(test_engine):
    """Create database session for testing"""
    Session = sessionmaker(bind=test_engine)
    session = Session()
    
    yield session
    
    session.rollback()
    session.close()

@pytest.fixture
def client(app):
    """Create test client"""
    return app.test_client()

@pytest.fixture
def runner(app):
    """Create test CLI runner"""
    return app.test_cli_runner()

@pytest.fixture
def test_user(db_session):
    """Create test user"""
    from app.models.user import User
    from werkzeug.security import generate_password_hash
    
    user = User(
        username='testuser',
        password_hash=generate_password_hash('testpass123'),
        role='user'
    )
    db_session.add(user)
    db_session.commit()
    
    return user

@pytest.fixture
def admin_user(db_session):
    """Create admin user"""
    from app.models.user import User
    from werkzeug.security import generate_password_hash
    
    user = User(
        username='admin',
        password_hash=generate_password_hash('adminpass123'),
        role='admin'
    )
    db_session.add(user)
    db_session.commit()
    
    return user

@pytest.fixture
def test_contact(db_session, test_user):
    """Create test contact"""
    from app.models.contact import Contact
    
    contact = Contact(
        user_id=test_user.id,
        full_name='John Doe',
        tier=2
    )
    db_session.add(contact)
    db_session.commit()
    
    return contact
NEW FILE: tests/test_models.py
pythonimport pytest
from datetime import datetime
from app.models.user import User
from app.models.contact import Contact
from app.models.note import RawNote, SynthesizedEntry

class TestUserModel:
    def test_user_creation(self, db_session):
        """Test user model creation"""
        user = User(
            username='testuser',
            password_hash='hashed_password',
            role='user'
        )
        db_session.add(user)
        db_session.commit()
        
        assert user.id is not None
        assert user.username == 'testuser'
        assert user.role == 'user'
        assert user.created_at is not None
    
    def test_user_relationships(self, db_session, test_user):
        """Test user-contact relationships"""
        contact = Contact(
            user_id=test_user.id,
            full_name='Test Contact',
            tier=1
        )
        db_session.add(contact)
        db_session.commit()
        
        assert len(test_user.contacts) == 1
        assert test_user.contacts[0].full_name == 'Test Contact'

class TestContactModel:
    def test_contact_creation(self, db_session, test_user):
        """Test contact model creation"""
        contact = Contact(
            user_id=test_user.id,
            full_name='Jane Smith',
            tier=1,
            telegram_id='123456789'
        )
        db_session.add(contact)
        db_session.commit()
        
        assert contact.id is not None
        assert contact.full_name == 'Jane Smith'
        assert contact.tier == 1
        assert contact.telegram_id == '123456789'
        assert contact.user_id == test_user.id
    
    def test_contact_to_dict(self, test_contact):
        """Test contact serialization"""
        data = test_contact.to_dict()
        
        assert data['id'] == test_contact.id
        assert data['full_name'] == test_contact.full_name
        assert data['tier'] == test_contact.tier
        assert 'created_at' in data

class TestNoteModel:
    def test_raw_note_creation(self, db_session, test_contact):
        """Test raw note creation"""
        note = RawNote(
            contact_id=test_contact.id,
            content='This is a test note about the contact.',
            metadata_tags={'category': 'personal', 'importance': 'high'}
        )
        db_session.add(note)
        db_session.commit()
        
        assert note.id is not None
        assert note.content == 'This is a test note about the contact.'
        assert note.metadata_tags == {'category': 'personal', 'importance': 'high'}
        assert note.contact_id == test_contact.id
    
    def test_synthesized_entry_creation(self, db_session, test_contact):
        """Test synthesized entry creation"""
        entry = SynthesizedEntry(
            contact_id=test_contact.id,
            category='Goals',
            content='Wants to learn Python programming.',
            confidence_score=8.5
        )
        db_session.add(entry)
        db_session.commit()
        
        assert entry.id is not None
        assert entry.category == 'Goals'
        assert entry.confidence_score == 8.5
        assert entry.contact_id == test_contact.id
NEW FILE: tests/test_api_auth.py
pythonimport pytest
import json
from flask import url_for

class TestAuthAPI:
    def test_register_success(self, client):
        """Test successful user registration"""
        response = client.post('/api/auth/register', 
            json={
                'username': 'newuser',
                'password': 'password123'
            }
        )
        
        assert response.status_code == 201
        data = json.loads(response.data)
        assert data['success'] is True
        assert 'user_id' in data
    
    def test_register_invalid_data(self, client):
        """Test registration with invalid data"""
        response = client.post('/api/auth/register', 
            json={
                'username': '',
                'password': '123'
            }
        )
        
        assert response.status_code == 400
        data = json.loads(response.data)
        assert 'error' in data
    
    def test_login_success(self, client, test_user):
        """Test successful login"""
        response = client.post('/api/auth/login',
            json={
                'username': test_user.username,
                'password': 'testpass123'
            }
        )
        
        assert response.status_code == 200
        data = json.loads(response.data)
        assert data['success'] is True
    
    def test_login_invalid_credentials(self, client):
        """Test login with invalid credentials"""
        response = client.post('/api/auth/login',
            json={
                'username': 'nonexistent',
                'password': 'wrongpass'
            }
        )
        
        assert response.status_code == 401
        data = json.loads(response.data)
        assert 'error' in data
NEW FILE: tests/test_services.py
pythonimport pytest
from unittest.mock import Mock, patch
from app.services.note_service import NoteService
from app.services.contact_service import ContactService
from app.utils.exceptions import ValidationError, ResourceNotFoundError

class TestNoteService:
    @pytest.fixture
    def note_service(self):
        """Create note service with mocked dependencies"""
        db_manager = Mock()
        ai_service = Mock()
        return NoteService(db_manager, ai_service)
    
    def test_process_note_success(self, note_service, test_contact):
        """Test successful note processing"""
        # Mock database session
        mock_session = Mock()
        note_service.db_manager.get_session.return_value.__enter__.return_value = mock_session
        mock_session.query.return_value.filter_by.return_value.first.return_value = test_contact
        
        # Mock AI service
        mock_analysis = Mock()
        mock_analysis.categories = {
            'Goals': Mock(content='Learn Python', confidence=8.5)
        }
        note_service.ai_service.analyze_note.return_value = mock_analysis
        
        result = note_service.process_note(
            contact_id=test_contact.id,
            content='He wants to learn Python programming',
            user_id=test_contact.user_id
        )
        
        assert result['success'] is True
        assert len(result['synthesis']) == 1
        assert result['synthesis'][0]['category'] == 'Goals'
    
    def test_process_note_contact_not_found(self, note_service):
        """Test note processing with non-existent contact"""
        mock_session = Mock()
        note_service.db_manager.get_session.return_value.__enter__.return_value = mock_session
        mock_session.query.return_value.filter_by.return_value.first.return_value = None
        
        with pytest.raises(ValueError, match="Contact not found"):
            note_service.process_note(
                contact_id=999,
                content='Test content',
                user_id=1
            )

class TestContactService:
    @pytest.fixture
    def contact_service(self):
        """Create contact service with mocked dependencies"""
        db_manager = Mock()
        return ContactService(db_manager)
    
    def test_create_contact_success(self, contact_service):
        """Test successful contact creation"""
        mock_session = Mock()
        contact_service.db_manager.get_session.return_value.__enter__.return_value = mock_session
        
        result = contact_service.create_contact(
            user_id=1,
            full_name='John Doe',
            tier=2
        )
        
        # Verify contact was added to session
        mock_session.add.assert_called_once()
        assert result.full_name == 'John Doe'
        assert result.tier == 2
NEW FILE: tests/test_validators.py
pythonimport pytest
from app.utils.validators import validate_note_input, validate_contact_creation

class TestNoteValidator:
    def test_valid_note_input(self):
        """Test valid note input"""
        data = {
            'contact_id': 1,
            'content': 'This is a valid note with sufficient content.'
        }
        
        result = validate_note_input(data)
        assert result.is_valid is True
    
    def test_missing_contact_id(self):
        """Test note input without contact_id"""
        data = {
            'content': 'This is a valid note with sufficient content.'
        }
        
        result = validate_note_input(data)
        assert result.is_valid is False
        assert result.field == 'contact_id'
    
    def test_content_too_short(self):
        """Test note input with too short content"""
        data = {
            'contact_id': 1,
            'content': 'Short'
        }
        
        result = validate_note_input(data)
        assert result.is_valid is False
        assert result.field == 'content'
    
    def test_invalid_contact_id_type(self):
        """Test note input with invalid contact_id type"""
        data = {
            'contact_id': 'not_a_number',
            'content': 'This is a valid note with sufficient content.'
        }
        
        result = validate_note_input(data)
        assert result.is_valid is False
        assert result.field == 'contact_id'

class TestContactValidator:
    def test_valid_contact_creation(self):
        """Test valid contact creation input"""
        data = {
            'full_name': 'John Doe',
            'tier': 1
        }
        
        result = validate_contact_creation(data)
        assert result.is_valid is True
    
    def test_missing_full_name(self):
        """Test contact creation without full_name"""
        data = {
            'tier': 1
        }
        
        result = validate_contact_creation(data)
        assert result.is_valid is False
        assert result.field == 'full_name'
    
    def test_invalid_tier(self):
        """Test contact creation with invalid tier"""
        data = {
            'full_name': 'John Doe',
            'tier': 3
        }
        
        result = validate_contact_creation(data)
        assert result.is_valid is False
        assert result.field == 'tier'
5.2 Test Configuration and Coverage Setup
NEW FILE: pytest.ini
ini[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v 
    --tb=short
    --cov=app
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-fail-under=80
    --strict-markers
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow tests
    external: Tests requiring external services
UPDATE requirements.txt to add testing dependencies:
txt# Testing
pytest==7.4.3
pytest-cov==4.1.0
pytest-mock==3.12.0
pytest-flask==1.3.0
factory-boy==3.3.0
faker==20.1.0

# Development
black==23.11.0
flake8==6.1.0
mypy==1.7.1
NEW FILE: tests/test_integration.py
pythonimport pytest
import json
from app.models.user import User
from app.models.contact import Contact

@pytest.mark.integration
class TestContactIntegration:
    def test_full_contact_workflow(self, client, test_user):
        """Test complete contact creation and retrieval workflow"""
        # Login first
        login_response = client.post('/api/auth/login',
            json={
                'username': test_user.username,
                'password': 'testpass123'
            }
        )
        assert login_response.status_code == 200
        
        # Create contact
        create_response = client.post('/api/contacts/',
            json={
                'full_name': 'Integration Test Contact',
                'tier': 1,
                'telegram_handle': '@testcontact'
            }
        )
        assert create_response.status_code == 201
        create_data = json.loads(create_response.data)
        contact_id = create_data['contact']['id']
        
        # Get contact
        get_response = client.get(f'/api/contacts/{contact_id}')
        assert get_response.status_code == 200
        get_data = json.loads(get_response.data)
        assert get_data['contact']['full_name'] == 'Integration Test Contact'
        
        # Add note to contact
        note_response = client.post('/api/notes/process',
            json={
                'contact_id': contact_id,
                'content': 'This person is very interested in machine learning and AI development.'
            }
        )
        assert note_response.status_code == 200
        
        # Delete contact
        delete_response = client.delete(f'/api/contacts/{contact_id}')
        assert delete_response.status_code == 200
Phase 6: Containerization with Docker
6.1 Replace Platform-Specific Deployment with Docker
REMOVE render.yaml file.
NEW FILE: Dockerfile
dockerfileFROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV FLASK_ENV=production

# Set work directory
WORKDIR /app

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        postgresql-client \
        build-essential \
        libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy project
COPY . .

# Create non-root user
RUN adduser --disabled-password --gecos '' appuser \
    && chown -R appuser:appuser /app
USER appuser

# Create necessary directories
RUN mkdir -p logs uploads

# Expose port
EXPOSE 5000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# Run application
CMD ["gunicorn", "--worker-class", "gevent", "--workers", "2", "--bind", "0.0.0.0:5000", "main:app"]


NEW FILE: docker-compose.yml
yamlversion: '3.8'

services:
  web:
    build: .
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/kith_platform
      - REDIS_URL=redis://redis:6379/0
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - FLASK_SECRET_KEY=${FLASK_SECRET_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    restart: unless-stopped

  worker:
    build: .
    command: celery -A celery_app.celery worker --loglevel=info
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/kith_platform
      - REDIS_URL=redis://redis:6379/0
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./uploads:/app/uploads
    restart: unless-stopped

  scheduler:
    build: .
    command: celery -A celery_app.celery beat --loglevel=info
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/kith_platform
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
    restart: unless-stopped

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=kith_platform
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - web
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
NEW FILE: nginx.conf
nginxevents {
    worker_connections 1024;
}

http {
    upstream app {
        server web:5000;
    }

    server {
        listen 80;
        server_name localhost;

        client_max_body_size 16M;

        location / {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocket support
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            
            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        location /static/ {
            alias /app/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        location /health {
            access_log off;
            proxy_pass http://app;
        }
    }
}
NEW FILE: .dockerignore
.git
.gitignore
README.md
Dockerfile
.dockerignore
docker-compose.yml
.pytest_cache
htmlcov
.coverage
*.pyc
__pycache__
.env
venv
node_modules
.DS_Store
logs/*.log
uploads/*
!uploads/.gitkeep
NEW FILE: init.sql
sql-- Initialize PostgreSQL database
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE EXTENSION IF NOT EXISTS btree_gin;

-- Create database user for application (if not exists)
DO
$do$
BEGIN
   IF NOT EXISTS (
      SELECT FROM pg_catalog.pg_roles
      WHERE  rolname = 'kith_user') THEN

      CREATE ROLE kith_user LOGIN PASSWORD 'kith_password';
   END IF;
END
$do$;

-- Grant privileges
GRANT ALL PRIVILEGES ON DATABASE kith_platform TO kith_user;
GRANT ALL ON SCHEMA public TO kith_user;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO kith_user;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO kith_user;

-- Set default privileges for future tables
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO kith_user;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO kith_user;
6.2 Production Docker Configuration
NEW FILE: docker-compose.prod.yml
yamlversion: '3.8'

services:
  web:
    build:
      context: .
      dockerfile: Dockerfile.prod
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=postgresql://kith_user:${DB_PASSWORD}@db:5432/kith_platform
      - REDIS_URL=redis://redis:6379/0
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - FLASK_SECRET_KEY=${FLASK_SECRET_KEY}
      - SENTRY_DSN=${SENTRY_DSN}
    depends_on:
      - db
      - redis
    volumes:
      - uploads_data:/app/uploads
      - logs_data:/app/logs
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  worker:
    build:
      context: .
      dockerfile: Dockerfile.prod
    command: celery -A celery_app.celery worker --loglevel=info --concurrency=2
    environment:
      - DATABASE_URL=postgresql://kith_user:${DB_PASSWORD}@db:5432/kith_platform
      - REDIS_URL=redis://redis:6379/0
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - uploads_data:/app/uploads
    restart: unless-stopped
    deploy:
      replicas: 2

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=kith_platform
      - POSTGRES_USER=kith_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  uploads_data:
  logs_data:
NEW FILE: Dockerfile.prod
dockerfileFROM python:3.11-slim as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Production image
FROM python:3.11-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy Python dependencies from builder stage
COPY --from=builder /root/.local /root/.local

# Add Python dependencies to PATH
ENV PATH=/root/.local/bin:$PATH

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV FLASK_ENV=production

# Create app user
RUN adduser --disabled-password --gecos '' appuser

# Set work directory
WORKDIR /app

# Copy application code
COPY --chown=appuser:appuser . .

# Create necessary directories
RUN mkdir -p logs uploads && chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# Expose port
EXPOSE 5000

# Run application
CMD ["gunicorn", "--worker-class", "gevent", "--workers", "4", "--bind", "0.0.0.0:5000", "--access-logfile", "-", "--error-logfile", "-", "main:app"]
6.3 Development and Deployment Scripts
NEW FILE: scripts/dev-setup.sh
bash#!/bin/bash
set -e

echo "Setting up Kith Platform development environment..."

# Check if Docker is installed
if ! command -v docker &> /dev/null; then
    echo "Docker is not installed. Please install Docker and try again."
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "Docker Compose is not installed. Please install Docker Compose and try again."
    exit 1
fi

# Create .env file if it doesn't exist
if [ ! -f .env ]; then
    echo "Creating .env file..."
    cat > .env << EOL
# Flask Configuration
FLASK_SECRET_KEY=development-secret-key-change-in-production
FLASK_ENV=development

# Database
DB_PASSWORD=postgres

# External APIs (add your keys)
GEMINI_API_KEY=your-gemini-api-key-here
OPENAI_API_KEY=your-openai-api-key-here

# Monitoring (optional)
SENTRY_DSN=
EOL
    echo "Please edit .env file with your API keys"
fi

# Build and start services
echo "Building Docker images..."
docker-compose build

echo "Starting services..."
docker-compose up -d

# Wait for database to be ready
echo "Waiting for database to be ready..."
sleep 10

# Run database migrations
echo "Running database migrations..."
docker-compose exec web alembic upgrade head

# Create admin user
echo "Creating default admin user..."
docker-compose exec web python -c "
from app.models.user import User
from app.utils.database import DatabaseManager
from werkzeug.security import generate_password_hash

db_manager = DatabaseManager()
with db_manager.get_session() as session:
    admin = session.query(User).filter_by(username='admin').first()
    if not admin:
        admin = User(
            username='admin',
            password_hash=generate_password_hash('admin123'),
            role='admin'
        )
        session.add(admin)
        print('Admin user created: admin/admin123')
    else:
        print('Admin user already exists')
"

echo "Development environment is ready!"
echo "Application: http://localhost:5000"
echo "Admin login: admin/admin123"
echo ""
echo "To view logs: docker-compose logs -f"
echo "To stop: docker-compose down"
NEW FILE: scripts/deploy-prod.sh
bash#!/bin/bash
set -e

echo "Deploying Kith Platform to production..."

# Validate environment
if [ -z "$GEMINI_API_KEY" ] || [ -z "$OPENAI_API_KEY" ] || [ -z "$DB_PASSWORD" ]; then
    echo "Error: Required environment variables not set"
    echo "Please set: GEMINI_API_KEY, OPENAI_API_KEY, DB_PASSWORD"
    exit 1
fi

# Run tests
echo "Running tests..."
docker-compose -f docker-compose.test.yml up --build --abort-on-container-exit
test_result=$?
docker-compose -f docker-compose.test.yml down

if [ $test_result -ne 0 ]; then
    echo "Tests failed. Deployment aborted."
    exit 1
fi

# Build production images
echo "Building production images..."
docker-compose -f docker-compose.prod.yml build

# Deploy with zero downtime
echo "Deploying with zero downtime..."
docker-compose -f docker-compose.prod.yml up -d --remove-orphans

# Run migrations
echo "Running database migrations..."
docker-compose -f docker-compose.prod.yml exec web alembic upgrade head

# Health check
echo "Performing health check..."
sleep 30
response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:5000/health)
if [ "$response" != "200" ]; then
    echo "Health check failed. Rolling back..."
    docker-compose -f docker-compose.prod.yml down
    exit 1
fi

echo "Deployment successful!"
echo "Application is running at http://localhost"
NEW FILE: scripts/backup-db.sh
bash#!/bin/bash
set -e

# Create backup directory
BACKUP_DIR="./backups"
mkdir -p $BACKUP_DIR

# Generate backup filename with timestamp
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_FILE="$BACKUP_DIR/kith_platform_backup_$TIMESTAMP.sql"

echo "Creating database backup..."

# Create backup
docker-compose exec -T db pg_dump -U kith_user kith_platform > $BACKUP_FILE

# Compress backup
gzip $BACKUP_FILE

echo "Backup created: ${BACKUP_FILE}.gz"

# Clean up old backups (keep last 7 days)
find $BACKUP_DIR -name "*.sql.gz" -mtime +7 -delete

echo "Backup completed successfully"
Phase 7: Logging and Monitoring Implementation
7.1 Structured Logging System
NEW FILE: app/utils/logging.py
pythonimport logging
import json
import sys
from datetime import datetime
from pythonjsonlogger import jsonlogger
import traceback
from flask import request, g
from functools import wraps

class StructuredLogger:
    """Structured logging utility for the application"""
    
    @staticmethod
    def setup_logging(app):
        """Set up structured logging for the application"""
        # Clear existing handlers
        app.logger.handlers.clear()
        
        # Create formatter
        formatter = jsonlogger.JsonFormatter(
            '%(asctime)s %(name)s %(levelname)s %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        console_handler.setLevel(logging.INFO)
        
        # File handler (if not in container)
        if not app.config.get('DOCKER_ENV'):
            from logging.handlers import RotatingFileHandler
            import os
            
            if not os.path.exists('logs'):
                os.mkdir('logs')
            
            file_handler = RotatingFileHandler(
                'logs/app.log',
                maxBytes=10 * 1024 * 1024,  # 10MB
                backupCount=10
            )
            file_handler.setFormatter(formatter)
            file_handler.setLevel(logging.INFO)
            app.logger.addHandler(file_handler)
        
        app.logger.addHandler(console_handler)
        app.logger.setLevel(logging.INFO)
        
        # Set up request logging
        @app.before_request
        def log_request_info():
            g.start_time = datetime.utcnow()
            app.logger.info('Request started', extra={
                'event': 'request_started',
                'method': request.method,
                'url': request.url,
                'user_agent': request.headers.get('User-Agent'),
                'remote_addr': request.remote_addr,
                'request_id': getattr(g, 'request_id', None)
            })
        
        @app.after_request
        def log_request_response(response):
            duration = (datetime.utcnow() - g.start_time).total_seconds()
            app.logger.info('Request completed', extra={
                'event': 'request_completed',
                'method': request.method,
                'url': request.url,
                'status_code': response.status_code,
                'duration_seconds': duration,
                'request_id': getattr(g, 'request_id', None)
            })
            return response

def log_function_call(func):
    """Decorator to log function calls with parameters and results"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)
        
        # Log function entry
        logger.info(f'Function called: {func.__name__}', extra={
            'event': 'function_called',
            'function': func.__name__,
            'module': func.__module__,
            'args_count': len(args),
            'kwargs_keys': list(kwargs.keys())
        })
        
        try:
            result = func(*args, **kwargs)
            
            # Log successful completion
            logger.info(f'Function completed: {func.__name__}', extra={
                'event': 'function_completed',
                'function': func.__name__,
                'module': func.__module__,
                'success': True
            })
            
            return result
            
        except Exception as e:
            # Log error
            logger.error(f'Function failed: {func.__name__}', extra={
                'event': 'function_failed',
                'function': func.__name__,
                'module': func.__module__,
                'error': str(e),
                'error_type': type(e).__name__,
                'traceback': traceback.format_exc()
            })
            raise
    
    return wrapper

def log_service_call(service_name, operation):
    """Decorator to log external service calls"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            logger = logging.getLogger(func.__module__)
            
            start_time = datetime.utcnow()
            
            logger.info(f'External service call started', extra={
                'event': 'service_call_started',
                'service': service_name,
                'operation': operation,
                'function': func.__name__
            })
            
            try:
                result = func(*args, **kwargs)
                
                duration = (datetime.utcnow() - start_time).total_seconds()
                
                logger.info(f'External service call completed', extra={
                    'event': 'service_call_completed',
                    'service': service_name,
                    'operation': operation,
                    'function': func.__name__,
                    'duration_seconds': duration,
                    'success': True
                })
                
                return result
                
            except Exception as e:
                duration = (datetime.utcnow() - start_time).total_seconds()
                
                logger.error(f'External service call failed', extra={
                    'event': 'service_call_failed',
                    'service': service_name,
                    'operation': operation,
                    'function': func.__name__,
                    'duration_seconds': duration,
                    'error': str(e),
                    'error_type': type(e).__name__
                })
                raise
        
        return wrapper
    return decorator
7.2 Application Performance Monitoring
NEW FILE: app/utils/monitoring.py
pythonimport time
import psutil
import logging
from datetime import datetime, timedelta
from functools import wraps
from flask import g, request, current_app
from collections import defaultdict, deque
import threading

logger = logging.getLogger(__name__)

class PerformanceMonitor:
    """Application performance monitoring"""
    
    def __init__(self):
        self.request_times = deque(maxlen=1000)  # Keep last 1000 requests
        self.slow_queries = deque(maxlen=100)    # Keep last 100 slow queries
        self.error_counts = defaultdict(int)
        self.endpoint_stats = defaultdict(lambda: {'count': 0, 'total_time': 0, 'errors': 0})
        self.lock = threading.Lock()
    
    def record_request(self, endpoint, duration, status_code):
        """Record request metrics"""
        with self.lock:
            self.request_times.append({
                'endpoint': endpoint,
                'duration': duration,
                'status_code': status_code,
                'timestamp': datetime.utcnow()
            })
            
            # Update endpoint stats
            self.endpoint_stats[endpoint]['count'] += 1
            self.endpoint_stats[endpoint]['total_time'] += duration
            
            if status_code >= 400:
                self.endpoint_stats[endpoint]['errors'] += 1
                self.error_counts[status_code] += 1
    
    def record_slow_query(self, query, duration, params=None):
        """Record slow database queries"""
        with self.lock:
            self.slow_queries.append({
                'query': query[:200],  # Truncate long queries
                'duration': duration,
                'params': str(params)[:100] if params else None,
                'timestamp': datetime.utcnow()
            })
    
    def get_metrics(self):
        """Get current performance metrics"""
        with self.lock:
            now = datetime.utcnow()
            recent_requests = [
                req for req in self.request_times 
                if (now - req['timestamp']).seconds < 300  # Last 5 minutes
            ]
            
            if not recent_requests:
                return {
                    'request_rate': 0,
                    'avg_response_time': 0,
                    'error_rate': 0,
                    'slow_queries_count': len(self.slow_queries),
                    'system_metrics': self._get_system_metrics()
                }
            
            total_requests = len(recent_requests)
            error_requests = len([req for req in recent_requests if req['status_code'] >= 400])
            avg_response_time = sum(req['duration'] for req in recent_requests) / total_requests
            
            return {
                'request_rate': total_requests / 5,  # Requests per minute (5-minute window)
                'avg_response_time': avg_response_time,
                'error_rate': (error_requests / total_requests) * 100,
                'slow_queries_count': len(self.slow_queries),
                'endpoint_stats': dict(self.endpoint_stats),
                'system_metrics': self._get_system_metrics()
            }
    
    def _get_system_metrics(self):
        """Get system performance metrics"""
        try:
            return {
                'cpu_percent': psutil.cpu_percent(interval=1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_percent': psutil.disk_usage('/').percent,
                'timestamp': datetime.utcnow().isoformat()
            }
        except Exception as e:
            logger.warning(f"Failed to get system metrics: {e}")
            return {}

# Global monitor instance
performance_monitor = PerformanceMonitor()

def monitor_performance(func):
    """Decorator to monitor function performance"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            duration = time.time() - start_time
            
            # Log slow functions (>1 second)
            if duration > 1.0:
                logger.warning(f'Slow function detected', extra={
                    'event': 'slow_function',
                    'function': func.__name__,
                    'module': func.__module__,
                    'duration_seconds': duration
                })
    
    return wrapper

class DatabaseQueryMonitor:
    """Monitor database query performance"""
    
    @staticmethod
    def log_query(query, params, duration):
        """Log database query with performance info"""
        if duration > 0.5:  # Log queries slower than 500ms
            logger.warning('Slow query detected', extra={
                'event': 'slow_query',
                'query': str(query)[:200],
                'duration_seconds': duration,
                'params': str(params)[:100] if params else None
            })
            
            performance_monitor.record_slow_query(str(query), duration, params)
        else:
            logger.debug('Database query executed', extra={
                'event': 'query_executed',
                'duration_seconds': duration
            })
7.3 Health Check and Monitoring Endpoints
NEW FILE: app/api/monitoring.py
pythonfrom flask import Blueprint, jsonify, request
from flask_login import login_required
from app.utils.monitoring import performance_monitor
from app.utils.database import DatabaseManager
from app.utils.dependencies import container
import psutil
import logging

monitoring_bp = Blueprint('monitoring', __name__)
logger = logging.getLogger(__name__)

@monitoring_bp.route('/health', methods=['GET'])
def health_check():
    """Basic health check endpoint"""
    try:
        # Test database connection
        db_manager = container.database_manager
        with db_manager.get_session() as session:
            session.execute("SELECT 1")
        
        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.utcnow().isoformat(),
            'version': '5.0',
            'database': 'connected'
        }), 200
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 503

@monitoring_bp.route('/health/detailed', methods=['GET'])
@login_required
def detailed_health_check():
    """Detailed health check with system metrics"""
    try:
        # Database check
        db_status = 'unknown'
        try:
            db_manager = container.database_manager
            with db_manager.get_session() as session:
                session.execute("SELECT 1")
            db_status = 'healthy'
        except Exception as e:
            db_status = f'unhealthy: {str(e)}'
        
        # Redis check (for Celery)
        redis_status = 'unknown'
        try:
            import redis
            redis_client = redis.from_url('redis://redis:6379/0')
            redis_client.ping()
            redis_status = 'healthy'
        except Exception as e:
            redis_status = f'unhealthy: {str(e)}'
        
        # System metrics
        system_metrics = {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent,
            'load_average': list(psutil.getloadavg()) if hasattr(psutil, 'getloadavg') else None
        }
        
        # Application metrics
        app_metrics = performance_monitor.get_metrics()
        
        return jsonify({
            'status': 'healthy' if db_status == 'healthy' and redis_status == 'healthy' else 'degraded',
            'timestamp': datetime.utcnow().isoformat(),
            'components': {
                'database': db_status,
                'redis': redis_status
            },
            'system_metrics': system_metrics,
            'application_metrics': app_metrics
        }), 200
        
    except Exception as e:
        logger.error(f"Detailed health check failed: {e}")
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 503

@monitoring_bp.route('/metrics', methods=['GET'])
@login_required
def get_metrics():
    """Get application performance metrics"""
    try:
        metrics = performance_monitor.get_metrics()
        return jsonify(metrics), 200
    except Exception as e:
        logger.error(f"Failed to get metrics: {e}")
        return jsonify({'error': str(e)}), 500

@monitoring_bp.route('/logs', methods=['GET'])
@login_required
def get_recent_logs():
    """Get recent application logs"""
    try:
        lines = int(request.args.get('lines', 100))
        level = request.args.get('level', 'INFO').upper()
        
        # This is a simplified implementation
        # In production, you'd want to use a proper log aggregation system
        logs = []
        
        try:
            with open('logs/app.log', 'r') as f:
                all_logs = f.readlines()
                filtered_logs = [
                    log for log in all_logs[-lines:]
                    if level in log or level == 'ALL'
                ]
                logs = filtered_logs
        except FileNotFoundError:
            logs = ['No log file found']
        
        return jsonify({
            'logs': logs,
            'total_lines': len(logs),
            'level_filter': level
        }), 200
        
    except Exception as e:
        logger.error(f"Failed to get logs: {e}")
        return jsonify({'error': str(e)}), 500
Final Implementation Summary
This comprehensive refactoring plan addresses all the critical issues identified:

Database Issues: Unified PostgreSQL, proper migrations with Alembic, JSON columns, comprehensive indexing
Code Quality: Modular architecture, dependency injection, consistent error handling, separated concerns
Architecture: Microservices-ready structure, Celery for background jobs, proper logging and monitoring
Testing: Complete test suite with 80%+ coverage requirement, proper fixtures, integration tests
Deployment: Docker containerization, production-ready configuration, automated deployment scripts



Each phase can be implemented independently, allowing for incremental improvements while maintaining system functionality. The modular structure makes future maintenance and feature additions significantly easier while ensuring security, scalability, and reliability.




