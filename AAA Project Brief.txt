================================================================================
KITH PLATFORM - COMPREHENSIVE TECHNICAL IMPLEMENTATION GUIDE
Project Brief - Spiderman V3 (Complete Technical Documentation)
================================================================================

PROJECT OVERVIEW
================================================================================

Project Name: Kith Platform (Spiderman V3)
Version: 3.0 (Major Refactor - January 2025)
Status: Active Development - Production Ready with Recent Enhancements
Type: Personal Intelligence & Relationship Management Platform
Primary Languages: Python (Flask), JavaScript (Vanilla), HTML5, CSS3
Repository: Local development environment with comprehensive version control

EXECUTIVE SUMMARY
================================================================================

Kith Platform is a sophisticated personal intelligence system designed to help users organize, analyze, and derive insights from their personal relationships and social interactions. The platform combines modern web technologies with AI-powered analysis to transform unstructured notes and conversation data into actionable relationship intelligence.

This documentation provides complete technical specifications, implementation details, and step-by-step instructions for recreating the entire application from scratch. Every component, from database schemas to JavaScript functions, is documented with actual code examples and pseudo-code for complex algorithms.

COMPLETE TECHNICAL ARCHITECTURE
================================================================================

BACKEND STACK SPECIFICATION:

Framework: Flask 2.3.3
```python
# Core Flask application initialization (app.py:35-40)
from flask import Flask, request, jsonify, render_template, Response
from flask_apscheduler import APScheduler
from dotenv import load_dotenv

load_dotenv()
app = Flask(__name__)
app.config['TEMPLATES_AUTO_RELOAD'] = True
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0
```

Database: SQLite with SQLAlchemy 2.0.21 ORM
```sql
-- Complete Database Schema (Generated from models.py)

CREATE TABLE users (
    id INTEGER NOT NULL PRIMARY KEY,
    username VARCHAR(255) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE contacts (
    id INTEGER NOT NULL PRIMARY KEY,
    user_id INTEGER NOT NULL DEFAULT 1,
    full_name VARCHAR(255) NOT NULL,
    tier INTEGER NOT NULL DEFAULT 2,
    vector_collection_id VARCHAR(255) UNIQUE,
    telegram_id VARCHAR(255),
    telegram_username VARCHAR(255),
    telegram_phone VARCHAR(255),
    telegram_handle VARCHAR(255),
    is_verified BOOLEAN DEFAULT FALSE,
    is_premium BOOLEAN DEFAULT FALSE,
    telegram_last_sync DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(user_id) REFERENCES users (id)
);

CREATE TABLE raw_notes (
    id INTEGER NOT NULL PRIMARY KEY,
    contact_id INTEGER NOT NULL,
    content TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    tags VARCHAR,
    FOREIGN KEY(contact_id) REFERENCES contacts (id)
);

CREATE TABLE synthesized_entries (
    id INTEGER NOT NULL PRIMARY KEY,
    contact_id INTEGER NOT NULL,
    source_note_id INTEGER,
    category VARCHAR(255) NOT NULL,
    content TEXT NOT NULL,
    summary TEXT,
    narrative_text TEXT,
    confidence_score FLOAT,
    ai_confidence FLOAT,
    is_approved BOOLEAN DEFAULT TRUE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(contact_id) REFERENCES contacts (id) ON DELETE CASCADE,
    FOREIGN KEY(source_note_id) REFERENCES raw_notes (id)
);

CREATE TABLE import_tasks (
    id VARCHAR(255) NOT NULL PRIMARY KEY,
    user_id INTEGER NOT NULL DEFAULT 1,
    contact_id INTEGER,
    task_type VARCHAR(50) NOT NULL DEFAULT 'telegram_import',
    status VARCHAR(50) NOT NULL DEFAULT 'pending',
    progress INTEGER DEFAULT 0,
    status_message TEXT,
    error_details TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    completed_at DATETIME,
    FOREIGN KEY(user_id) REFERENCES users (id),
    FOREIGN KEY(contact_id) REFERENCES contacts (id)
);

-- Additional audit logging table
CREATE TABLE contact_audit_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    contact_id INTEGER NOT NULL,
    user_id INTEGER NOT NULL,
    event_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    event_type TEXT NOT NULL,
    source TEXT NOT NULL,
    before_state TEXT,
    after_state TEXT,
    raw_input TEXT,
    FOREIGN KEY(contact_id) REFERENCES contacts(id),
    FOREIGN KEY(user_id) REFERENCES users(id)
);
```

SQLALCHEMY MODEL DEFINITIONS:

```python
# models.py - Complete SQLAlchemy Models
from sqlalchemy import create_engine, Column, Integer, String, Text, Boolean, DateTime, ForeignKey, Float
from sqlalchemy.orm import declarative_base, sessionmaker, relationship
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    username = Column(String(255), unique=True, nullable=False)
    password_hash = Column(String(255), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    contacts = relationship("Contact", back_populates="user")

class Contact(Base):
    __tablename__ = 'contacts'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), default=1, nullable=False)
    full_name = Column(String(255), nullable=False)
    tier = Column(Integer, default=2, nullable=False)  # 1 for inner circle, 2 for outer
    vector_collection_id = Column(String(255), unique=True)
    
    # Telegram Integration Fields
    telegram_id = Column(String(255))
    telegram_username = Column(String(255))
    telegram_phone = Column(String(255))
    telegram_handle = Column(String(255))
    is_verified = Column(Boolean, default=False)
    is_premium = Column(Boolean, default=False)
    telegram_last_sync = Column(DateTime)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    user = relationship("User", back_populates="contacts")
    raw_notes = relationship("RawNote", back_populates="contact", cascade="all, delete-orphan")
    synthesized_entries = relationship("SynthesizedEntry", back_populates="contact", cascade="all, delete-orphan")

class RawNote(Base):
    __tablename__ = 'raw_notes'
    
    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id'), nullable=False)
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    tags = Column(String)  # Store as JSON string for SQLite compatibility
    
    # Relationships
    contact = relationship("Contact", back_populates="raw_notes")
    synthesized_entries = relationship("SynthesizedEntry", back_populates="source_note")

class SynthesizedEntry(Base):
    __tablename__ = 'synthesized_entries'
    
    id = Column(Integer, primary_key=True)
    contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'), nullable=False)
    source_note_id = Column(Integer, ForeignKey('raw_notes.id'))
    category = Column(String(255), nullable=False)
    content = Column(Text, nullable=False)
    summary = Column(Text)
    narrative_text = Column(Text)
    confidence_score = Column(Float)
    ai_confidence = Column(Float)
    is_approved = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    contact = relationship("Contact", back_populates="synthesized_entries")
    source_note = relationship("RawNote", back_populates="synthesized_entries")

class ImportTask(Base):
    __tablename__ = 'import_tasks'
    
    id = Column(String(255), primary_key=True)  # UUID string
    user_id = Column(Integer, ForeignKey('users.id'), default=1, nullable=False)
    contact_id = Column(Integer, ForeignKey('contacts.id'))
    task_type = Column(String(50), default='telegram_import', nullable=False)
    status = Column(String(50), default='pending', nullable=False)
    progress = Column(Integer, default=0)
    status_message = Column(Text)
    error_details = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
    
    # Relationships
    user = relationship("User")
    contact = relationship("Contact")

# Database setup functions
def get_database_url():
    """Get database URL from environment or use SQLite for development."""
    database_url = os.getenv('DATABASE_URL')
    if database_url and database_url.startswith('postgresql://'):
        return database_url
    else:
        return 'sqlite:///kith_platform.db'

def init_db():
    """Initialize the database and create tables."""
    try:
        engine = create_engine(get_database_url())
        Base.metadata.create_all(engine)
        return engine
    except Exception as e:
        print(f"Error initializing database: {e}")
        engine = create_engine('sqlite:///kith_platform.db')
        Base.metadata.create_all(engine)
        return engine

def get_session():
    """Get a database session."""
    engine = create_engine(get_database_url())
    Session = sessionmaker(bind=engine)
    return Session()
```

AI INTEGRATION SPECIFICATION:

Vector Database: ChromaDB 0.4.15
```python
# ChromaDB Configuration (app.py:69-76)
import chromadb
import os

# Disable anonymized telemetry by default unless explicitly enabled
os.environ.setdefault('ANONYMIZED_TELEMETRY', 'FALSE')
_PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
_DEFAULT_CHROMA_DIR = os.path.join(_PROJECT_ROOT, 'chroma_db')
CHROMA_DB_PATH = os.getenv('CHROMA_DB_PATH', _DEFAULT_CHROMA_DIR)
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
```

OpenAI Integration:
```python
# OpenAI Configuration (app.py:53-56)
import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY', '')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4')
OPENAI_MODEL_VERSION = os.getenv('OPENAI_MODEL_VERSION', '')  # optional extra pin
```

FRONTEND ARCHITECTURE:

JavaScript Modular System:
```javascript
// main.js - Core application functionality (618 lines)
// Global variables
let currentView = 'main';
let currentContactId = null;

// Setup event listeners for all buttons
function setupEventListeners() {
    // Add Note button
    const addNoteBtn = document.getElementById('profile-add-note-btn');
    if (addNoteBtn) {
        addNoteBtn.addEventListener('click', function() {
            const noteArea = document.getElementById('profile-note-input-area');
            noteArea.style.display = 'block';
            document.getElementById('profile-note-input').focus();
        });
    }

    // Wire Back to Main buttons
    const backToMainFromProfileBtn = document.getElementById('back-to-main-from-profile');
    if (backToMainFromProfileBtn) {
        backToMainFromProfileBtn.addEventListener('click', function() {
            showMainView();
        });
    }

    // Sync Telegram Chat button
    const syncTelegramBtn = document.getElementById('profile-sync-telegram-btn');
    if (syncTelegramBtn) {
        syncTelegramBtn.addEventListener('click', function() {
            handleProfileTelegramSync();
        });
    }
}

// Helper function to get selected contacts
function getSelectedContacts() {
    const checkboxes = document.querySelectorAll('input[name="contact_ids"]:checked');
    return Array.from(checkboxes).map(checkbox => parseInt(checkbox.value));
}

// Helper function to delete selected contacts
function deleteSelectedContacts(contactIds) {
    fetch('/api/contacts/bulk-delete', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({ contact_ids: contactIds })
    })
    .then(response => response.json().then(data => ({ ok: response.ok, data })))
    .then(({ ok, data }) => {
        if (ok && !data.error) {
            alert(`Successfully deleted ${contactIds.length} contact(s)`);
            loadContacts(); // Reload the contact list
        } else {
            alert('Error deleting contacts: ' + (data.error || data.message || 'Unknown error'));
        }
    })
    .catch(error => {
        console.error('Error:', error);
        alert('An error occurred while deleting contacts');
    });
}
```

RELATIONSHIP ANALYSIS CATEGORIES SYSTEM:

```python
# constants.py - Complete category system
class Categories:
    ACTIONABLE = "Actionable"
    GOALS = "Goals" 
    RELATIONSHIP_STRATEGY = "Relationship_Strategy"
    SOCIAL = "Social"
    WELLBEING = "Wellbeing"
    AVOCATION = "Avocation"
    PROFESSIONAL_BACKGROUND = "Professional_Background"
    ENVIRONMENT_AND_LIFESTYLE = "Environment_And_Lifestyle"
    PSYCHOLOGY_AND_VALUES = "Psychology_And_Values"
    COMMUNICATION_STYLE = "Communication_Style"
    CHALLENGES_AND_DEVELOPMENT = "Challenges_And_Development"
    ADMIN_MATTERS = "Admin_Matters"
    DEEPER_INSIGHTS = "Deeper_Insights"
    FINANCIAL_SITUATION = "Financial_Situation"
    ESTABLISHED_PATTERNS = "ESTABLISHED_PATTERNS"
    CORE_IDENTITY = "CORE_IDENTITY"
    INFORMATION_GAPS = "INFORMATION_GAPS"
    MEMORY_ANCHORS = "MEMORY_ANCHORS"
    POSITIONALITY = "POSITIONALITY"
    OTHERS = "Others"

# Category list for validation
VALID_CATEGORIES = [
    Categories.ACTIONABLE,
    Categories.GOALS,
    Categories.RELATIONSHIP_STRATEGY,
    Categories.SOCIAL,
    Categories.WELLBEING,
    Categories.AVOCATION,
    Categories.PROFESSIONAL_BACKGROUND,
    Categories.ENVIRONMENT_AND_LIFESTYLE,
    Categories.PSYCHOLOGY_AND_VALUES,
    Categories.COMMUNICATION_STYLE,
    Categories.CHALLENGES_AND_DEVELOPMENT,
    Categories.ADMIN_MATTERS
]

# Extended category list for UI display (includes all categories in preferred order)
CATEGORY_ORDER = [
    Categories.ACTIONABLE,
    Categories.GOALS,
    Categories.RELATIONSHIP_STRATEGY,
    Categories.SOCIAL,
    Categories.WELLBEING,
    Categories.AVOCATION,
    Categories.PROFESSIONAL_BACKGROUND,
    Categories.ENVIRONMENT_AND_LIFESTYLE,
    Categories.PSYCHOLOGY_AND_VALUES,
    Categories.COMMUNICATION_STYLE,
    Categories.CHALLENGES_AND_DEVELOPMENT,
    Categories.DEEPER_INSIGHTS,
    Categories.FINANCIAL_SITUATION,
    Categories.ADMIN_MATTERS,
    Categories.ESTABLISHED_PATTERNS,
    Categories.CORE_IDENTITY,
    Categories.INFORMATION_GAPS,
    Categories.MEMORY_ANCHORS,
    Categories.POSITIONALITY,
    Categories.OTHERS
]
```

COMPLETE API ENDPOINT DOCUMENTATION
================================================================================

CONTACT MANAGEMENT ENDPOINTS:

```python
# GET /contacts - Retrieve contact list with filtering
@app.route('/contacts', methods=['GET'])
def get_contacts():
    """
    Retrieve contacts with optional filtering by tier and search term.
    
    Query Parameters:
        tier (int, optional): Filter by contact tier (1, 2, or 3)
        search (str, optional): Search term for contact names
        
    Returns:
        JSON response with contact list:
        {
            "contacts": [
                {
                    "id": 1,
                    "full_name": "John Doe",
                    "tier": 1,
                    "telegram_username": "johndoe",
                    "created_at": "2025-01-01T00:00:00",
                    "updated_at": "2025-01-01T00:00:00"
                }
            ]
        }
    """
    pass

# POST /contacts - Create new contact with validation
@app.route('/contacts', methods=['POST'])
def create_contact():
    """
    Create a new contact with comprehensive validation.
    
    Request Body:
        {
            "full_name": "John Doe",
            "tier": 1,
            "telegram_username": "johndoe" (optional),
            "telegram_handle": "@johndoe" (optional)
        }
        
    Returns:
        Success: {"message": "Contact created successfully", "contact_id": 123}
        Error: {"error": "Validation error message"}
    """
    pass

# PATCH /contacts/<id> - Update contact information
@app.route('/contacts/<int:contact_id>', methods=['PATCH'])
def update_contact(contact_id):
    """
    Update existing contact information with audit logging.
    
    Request Body (partial updates supported):
        {
            "full_name": "Jane Doe",
            "tier": 2,
            "telegram_username": "janedoe"
        }
        
    Returns:
        Success: {"message": "Contact updated successfully"}
        Error: {"error": "Update error message"}
    """
    pass

# DELETE /contacts/<id> - Delete contact with cascading
@app.route('/contacts/<int:contact_id>', methods=['DELETE'])
def delete_contact(contact_id):
    """
    Delete contact and all associated data (notes, synthesis entries).
    
    Returns:
        Success: {"message": "Contact deleted successfully"}
        Error: {"error": "Deletion error message"}
    """
    pass

# POST /contacts/bulk-delete - Delete multiple contacts
@app.route('/api/contacts/bulk-delete', methods=['POST'])
def bulk_delete_contacts():
    """
    Delete multiple contacts in a single operation.
    
    Request Body:
        {
            "contact_ids": [1, 2, 3, 4]
        }
        
    Returns:
        Success: {"message": "4 contacts deleted successfully"}
        Error: {"error": "Bulk deletion error message"}
    """
    pass
```

AI ANALYSIS ENDPOINTS:

```python
# POST /analyze_note - Process and categorize note content
@app.route('/analyze_note', methods=['POST'])
def analyze_note():
    """
    Analyze unstructured note content using AI categorization.
    
    Request Body:
        {
            "contact_id": 123,
            "note_content": "Had coffee with John. He mentioned wanting to switch careers to data science and is looking for networking opportunities. Seems stressed about current job.",
            "temperature": 0.1 (optional),
            "model": "gpt-4" (optional)
        }
        
    Processing Steps:
        1. Validate contact exists
        2. Send note to OpenAI with categorization prompt
        3. Parse AI response into categories
        4. Store in ChromaDB for semantic search
        5. Return structured analysis for review
        
    Returns:
        {
            "analysis": {
                "Actionable": "Follow up with networking contacts in data science",
                "Professional_Background": "Currently working in a role he wants to leave, interested in data science transition",
                "Wellbeing": "Experiencing job-related stress",
                "Social": "Met for coffee, open to professional networking"
            },
            "raw_note_id": 456
        }
    """
    pass

# POST /process-note - Manual note analysis and processing
@app.route('/process-note', methods=['POST'])
def process_note():
    """
    Process note content for a specific contact with manual override options.
    
    Request Body:
        {
            "contact_id": 123,
            "content": "Note content here",
            "force_reprocess": false (optional)
        }
        
    Returns:
        Processed note analysis ready for review/saving
    """
    pass

# POST /save-synthesis - Save analyzed data with audit logging
@app.route('/save-synthesis', methods=['POST'])
def save_synthesis():
    """
    Save AI-analyzed and user-reviewed synthesis data.
    
    Request Body:
        {
            "contact_id": 123,
            "raw_note_id": 456,
            "synthesis_data": {
                "Actionable": "Follow up with networking contacts",
                "Professional_Background": "Data science career transition"
            }
        }
        
    Processing Steps:
        1. Validate all data
        2. Create synthesized_entries records
        3. Update ChromaDB collections
        4. Log audit trail
        5. Update contact timestamp
        
    Returns:
        Success confirmation with synthesis IDs
    """
    pass
```

TELEGRAM INTEGRATION ENDPOINTS:

```python
# POST /telegram/start-import - Initiate Telegram import with progress tracking
@app.route('/telegram/start-import', methods=['POST'])
def start_telegram_import():
    """
    Start background Telegram chat import for a contact.
    
    Request Body:
        {
            "contact_id": 123,
            "telegram_handle": "@johndoe",
            "days_back": 30 (optional, default 30)
        }
        
    Processing Steps:
        1. Validate contact exists
        2. Create import task with UUID
        3. Launch background worker process
        4. Return task ID for progress tracking
        
    Returns:
        {
            "task_id": "uuid-string-here",
            "message": "Import started successfully"
        }
    """
    pass

# GET /telegram/import-status/<task_id> - Poll import status
@app.route('/telegram/import-status/<task_id>', methods=['GET'])
def get_import_status(task_id):
    """
    Get real-time status of background import task.
    
    Returns:
        {
            "task_id": "uuid-string",
            "status": "processing", // pending, connecting, fetching, processing, completed, failed
            "progress": 65, // 0-100
            "status_message": "Processing 150 of 230 messages",
            "error_details": null // or error message if failed
        }
    """
    pass
```

SEARCH AND DISCOVERY ENDPOINTS:

```python
# POST /search - Perform semantic or keyword search
@app.route('/search', methods=['POST'])
def search_contacts():
    """
    Perform advanced search across all contact data.
    
    Request Body:
        {
            "query": "data science networking",
            "search_type": "semantic", // or "keyword"
            "tier_filter": [1, 2], // optional tier filtering
            "category_filter": ["Actionable", "Professional_Background"], // optional
            "limit": 10 // optional, default 10
        }
        
    Processing Steps:
        1. Parse search parameters
        2. If semantic: Query ChromaDB vector database
        3. If keyword: Use SQLite FTS or LIKE queries
        4. Apply filters (tier, category)
        5. Rank and return results
        
    Returns:
        {
            "results": [
                {
                    "contact_id": 123,
                    "contact_name": "John Doe",
                    "relevance_score": 0.89,
                    "matching_content": "snippet of matching text",
                    "category": "Actionable"
                }
            ],
            "total_results": 25
        }
    """
    pass

# GET /export/csv - Export comprehensive data as CSV
@app.route('/export/csv', methods=['GET'])
def export_csv():
    """
    Export all contact data and synthesis entries as CSV.
    
    Query Parameters:
        tier (int, optional): Filter by tier
        include_raw_notes (bool, optional): Include raw notes in export
        
    Processing Steps:
        1. Query all contacts and synthesized entries
        2. Apply filters if specified
        3. Format data for CSV export
        4. Generate downloadable CSV file
        
    Returns:
        CSV file download with headers:
        contact_id,full_name,tier,category,content,confidence_score,created_at
    """
    pass
```

TELEGRAM INTEGRATION IMPLEMENTATION
================================================================================

TELETHON SETUP AND CONFIGURATION:

```python
# telegram_worker.py - Background processing worker (311 lines)
import os
import requests
import sqlite3
import time
import asyncio
import logging
from telethon import TelegramClient
from telethon.errors.rpcerrorlist import UserNotParticipantError
from datetime import datetime, timedelta
from constants import DEFAULT_DB_NAME

# Configuration (loaded from .env)
API_ID = os.getenv('TELEGRAM_API_ID')
API_HASH = os.getenv('TELEGRAM_API_HASH')
KITH_API_URL = os.getenv('KITH_API_URL', 'http://127.0.0.1:5001')
KITH_API_TOKEN = os.getenv('KITH_API_TOKEN', 'dev_token')
SESSION_NAME = os.getenv('TELEGRAM_SESSION_NAME', 'kith_telegram_session')

# Check for required Telegram API credentials
if not API_ID or not API_HASH:
    logging.error("TELEGRAM_API_ID and TELEGRAM_API_HASH must be set in .env file")
    logging.error("Get these credentials from https://my.telegram.org/apps")
    exit(1)

def get_db_connection():
    """Get database connection with retry logic."""
    max_retries = 5
    retry_delay = 1
    
    db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), DEFAULT_DB_NAME)
    
    for attempt in range(max_retries):
        try:
            conn = sqlite3.connect(db_path, timeout=30.0)
            conn.execute('PRAGMA foreign_keys=ON')
            conn.execute('PRAGMA journal_mode=WAL')  # Better for concurrent access
            conn.execute('PRAGMA synchronous=NORMAL')  # Better performance
            conn.execute('PRAGMA temp_store=memory')  # Use memory for temp tables
            conn.execute('PRAGMA busy_timeout=5000')  # 5 second busy timeout
            conn.row_factory = sqlite3.Row  # Enable dict-like access
            
            # Test the connection
            conn.execute('SELECT 1').fetchone()
            logging.info("Database connection successful.")
            return conn
        except sqlite3.OperationalError as e:
            if 'database is locked' in str(e).lower() and attempt < max_retries - 1:
                logging.warning(f"Database locked, retrying in {retry_delay}s... (attempt {attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                logging.error(f"Final attempt to connect to database failed: {e}", exc_info=True)
                raise

def update_task_status(task_id, status, message="", error="", progress=None):
    """Helper function to update the task status in the database."""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            conn = get_db_connection()
            if not conn:
                logging.error(f"[Task {task_id}] Could not connect to the database to update status.")
                return
                
            try:
                # First, verify the task exists
                cursor = conn.execute('SELECT id FROM import_tasks WHERE id = ?', (task_id,))
                if not cursor.fetchone():
                    logging.warning(f"Warning: Task {task_id} not found in database for status update.")
                    return
                
                # Prepare update parameters
                update_params = [status, message[:1000] if message else "", error[:2000] if error else ""]
                
                if progress is not None:
                    progress = max(0, min(100, int(progress)))  # Clamp between 0-100
                    update_params.append(progress)
                    update_params.append(task_id)
                    
                    conn.execute('''
                        UPDATE import_tasks 
                        SET status = ?, status_message = ?, error_details = ?, progress = ?
                        WHERE id = ?
                    ''', update_params)
                else:
                    update_params.append(task_id)
                    conn.execute('''
                        UPDATE import_tasks 
                        SET status = ?, status_message = ?, error_details = ?
                        WHERE id = ?
                    ''', update_params)
                
                conn.commit()
                logging.info(f"[Task {task_id}] Status updated: {status} - {message}")
                break
                
            finally:
                conn.close()
                
        except Exception as e:
            if attempt == max_retries - 1:
                logging.error(f"[Task {task_id}] Failed to update status after {max_retries} attempts: {e}")
            else:
                logging.warning(f"[Task {task_id}] Attempt {attempt + 1} failed, retrying: {e}")
                time.sleep(retry_delay)
                retry_delay *= 2

async def import_telegram_chat(task_id, contact_id, telegram_handle, days_back=30):
    """
    Main function to import Telegram chat history.
    
    Processing Steps:
        1. Initialize Telethon client with session
        2. Connect and authenticate
        3. Resolve user/chat entity
        4. Fetch message history with pagination
        5. Process and analyze messages
        6. Update progress throughout process
        7. Handle errors and edge cases
    """
    update_task_status(task_id, "connecting", "Initializing Telegram connection...")
    
    try:
        # Initialize Telethon client
        client = TelegramClient(SESSION_NAME, API_ID, API_HASH)
        
        # Connect to Telegram
        await client.connect()
        
        # Check if we're authorized
        if not await client.is_user_authorized():
            update_task_status(task_id, "failed", error="Telegram authorization required. Please run telegram_setup.py first.")
            return
        
        update_task_status(task_id, "fetching", "Resolving user and fetching messages...", progress=10)
        
        # Resolve the user/chat
        try:
            entity = await client.get_entity(telegram_handle)
        except Exception as e:
            update_task_status(task_id, "failed", error=f"Could not find user '{telegram_handle}': {str(e)}")
            await client.disconnect()
            return
        
        # Calculate date range
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # Fetch messages
        messages = []
        total_messages = 0
        processed_messages = 0
        
        update_task_status(task_id, "fetching", f"Fetching messages from last {days_back} days...", progress=20)
        
        async for message in client.iter_messages(entity, offset_date=start_date, reverse=True):
            if message.date < start_date:
                break
                
            if message.text:  # Only process text messages
                messages.append({
                    'id': message.id,
                    'date': message.date,
                    'text': message.text,
                    'from_user': message.from_id == entity.id if hasattr(entity, 'id') else False
                })
                total_messages += 1
                
            # Update progress during fetching
            if total_messages % 50 == 0:
                progress = min(50, 20 + (total_messages / max(1, total_messages)) * 30)
                update_task_status(task_id, "fetching", f"Fetched {total_messages} messages...", progress=int(progress))
        
        update_task_status(task_id, "processing", f"Processing {total_messages} messages...", progress=60)
        
        # Process messages and create raw notes
        for i, msg in enumerate(messages):
            try:
                # Create raw note entry
                note_content = f"[Telegram - {msg['date'].strftime('%Y-%m-%d %H:%M')}] {msg['text']}"
                
                # Here you would call your note processing API
                response = requests.post(f"{KITH_API_URL}/process-note", 
                    json={
                        'contact_id': contact_id,
                        'content': note_content,
                        'source': 'telegram',
                        'telegram_message_id': msg['id'],
                        'telegram_date': msg['date'].isoformat()
                    },
                    headers={'Authorization': f'Bearer {KITH_API_TOKEN}'},
                    timeout=30
                )
                
                if response.status_code == 200:
                    processed_messages += 1
                
                # Update progress
                progress = 60 + (i / max(1, len(messages))) * 35
                update_task_status(task_id, "processing", f"Processed {i+1}/{len(messages)} messages", progress=int(progress))
                
            except Exception as e:
                logging.warning(f"Error processing message {msg['id']}: {e}")
                continue
        
        # Update contact's last sync timestamp
        conn = get_db_connection()
        try:
            conn.execute('''
                UPDATE contacts 
                SET telegram_last_sync = CURRENT_TIMESTAMP 
                WHERE id = ?
            ''', (contact_id,))
            conn.commit()
        finally:
            conn.close()
        
        # Final status update
        update_task_status(task_id, "completed", f"Successfully imported {processed_messages} messages", progress=100)
        
        await client.disconnect()
        
    except Exception as e:
        logging.error(f"Error in telegram import task {task_id}: {e}", exc_info=True)
        update_task_status(task_id, "failed", error=str(e))

# Entry point for background worker
if __name__ == "__main__":
    import sys
    if len(sys.argv) != 4:
        print("Usage: python telegram_worker.py <task_id> <contact_id> <telegram_handle>")
        sys.exit(1)
    
    task_id = sys.argv[1]
    contact_id = int(sys.argv[2])
    telegram_handle = sys.argv[3]
    
    # Run the async import function
    asyncio.run(import_telegram_chat(task_id, contact_id, telegram_handle))
```

ANALYTICS AND RELATIONSHIP HEALTH SCORING
================================================================================

```python
# analytics.py - Advanced relationship analytics
class RelationshipAnalytics:
    """Advanced analytics for relationship health and insights."""
    
    def __init__(self, db_path: str = DEFAULT_DB_NAME):
        self.db_path = db_path
    
    def calculate_relationship_health_score(self, contact_id: int) -> Dict:
        """
        Calculate comprehensive relationship health score for a contact.
        
        Scoring Algorithm:
        - Recency Weight: 30% (recent interactions score higher)
        - Engagement Weight: 30% (frequency of interactions)
        - Quality Weight: 20% (AI confidence and content depth)
        - Diversity Weight: 20% (variety of interaction categories)
        
        Returns score from 0-100 with detailed breakdown
        """
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Get all entries for this contact
            cursor.execute("""
                SELECT category, ai_confidence, created_at, is_approved
                FROM synthesized_entries 
                WHERE contact_id = ? AND is_approved = TRUE
                ORDER BY created_at DESC
            """, (contact_id,))
            
            entries = cursor.fetchall()
            
            if not entries:
                return {
                    "health_score": 0,
                    "total_interactions": 0,
                    "last_interaction": None,
                    "category_distribution": {},
                    "confidence_avg": 0,
                    "insights": ["No data available for this contact"]
                }
            
            # Calculate recency score (0-100)
            most_recent = datetime.fromisoformat(entries[0][2])
            days_since_last = (datetime.now() - most_recent).days
            recency_score = max(0, 100 - (days_since_last * Analytics.RECENCY_POINTS_LOSS_PER_DAY))
            
            # Calculate engagement score (0-100)
            weeks_with_data = len(set((datetime.fromisoformat(entry[2]).isocalendar()[1] for entry in entries)))
            engagement_score = min(100, weeks_with_data * Analytics.INTERACTIONS_PER_WEEK_MAX_SCORE)
            
            # Calculate quality score (0-100)
            confidence_scores = [entry[1] for entry in entries if entry[1] is not None]
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
            quality_score = avg_confidence * Analytics.CONFIDENCE_SCALE_MULTIPLIER
            
            # Calculate diversity score (0-100)
            unique_categories = len(set(entry[0] for entry in entries))
            diversity_score = min(100, unique_categories * Analytics.CATEGORY_DIVERSITY_MULTIPLIER)
            
            # Calculate weighted final score
            final_score = (
                recency_score * Analytics.RECENCY_WEIGHT +
                engagement_score * Analytics.ENGAGEMENT_WEIGHT +
                quality_score * Analytics.QUALITY_WEIGHT +
                diversity_score * Analytics.DIVERSITY_WEIGHT
            )
            
            # Generate insights
            insights = []
            if final_score >= Analytics.EXCELLENT_HEALTH_THRESHOLD:
                insights.append("Excellent relationship health - frequent, high-quality interactions")
            elif final_score >= Analytics.GOOD_HEALTH_THRESHOLD:
                insights.append("Good relationship health with room for improvement")
            elif final_score >= Analytics.MODERATE_HEALTH_THRESHOLD:
                insights.append("Moderate relationship health - consider increasing engagement")
            else:
                insights.append("Low relationship health - relationship may need attention")
            
            if days_since_last > Analytics.FOLLOW_UP_DAYS_THRESHOLD:
                insights.append(f"No recent contact ({days_since_last} days) - consider following up")
            
            category_distribution = {}
            for entry in entries:
                category = entry[0]
                category_distribution[category] = category_distribution.get(category, 0) + 1
            
            return {
                "health_score": round(final_score, 2),
                "total_interactions": len(entries),
                "last_interaction": most_recent.isoformat(),
                "category_distribution": category_distribution,
                "confidence_avg": round(avg_confidence, 2),
                "scoring_breakdown": {
                    "recency_score": round(recency_score, 2),
                    "engagement_score": round(engagement_score, 2),
                    "quality_score": round(quality_score, 2),
                    "diversity_score": round(diversity_score, 2)
                },
                "insights": insights
            }
    
    def get_actionable_items_summary(self, days_back: int = 30) -> Dict:
        """Get summary of all actionable items across all contacts."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            cutoff_date = datetime.now() - timedelta(days=days_back)
            
            cursor.execute("""
                SELECT c.full_name, se.content, se.created_at, se.ai_confidence
                FROM synthesized_entries se
                JOIN contacts c ON se.contact_id = c.id
                WHERE se.category = ? AND se.created_at > ? AND se.is_approved = TRUE
                ORDER BY se.created_at DESC
            """, (Categories.ACTIONABLE, cutoff_date.isoformat()))
            
            actionables = cursor.fetchall()
            
            # Group by contact
            by_contact = {}
            for item in actionables:
                contact_name = item[0]
                if contact_name not in by_contact:
                    by_contact[contact_name] = []
                by_contact[contact_name].append({
                    'content': item[1],
                    'created_at': item[2],
                    'confidence': item[3]
                })
            
            return {
                "total_actionables": len(actionables),
                "contacts_with_actionables": len(by_contact),
                "by_contact": by_contact,
                "high_priority_count": len([a for a in actionables if a[3] and a[3] > 0.8])
            }
```

STEP-BY-STEP IMPLEMENTATION GUIDE
================================================================================

DEVELOPMENT ENVIRONMENT SETUP:

1. **System Requirements:**
```bash
# Required software
- Python 3.8+ (tested with 3.11)
- pip package manager
- Git for version control
- SQLite3 (usually included with Python)
- Web browser (Chrome/Firefox/Safari)
```

2. **Project Structure Creation:**
```bash
# Create project directory
mkdir kith-platform
cd kith-platform

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Create directory structure
mkdir -p static/js
mkdir -p templates
mkdir -p chroma_db
touch app.py models.py constants.py analytics.py
touch static/style.css
touch static/js/main.js static/js/contacts.js static/js/settings.js
touch templates/index.html
touch requirements.txt
touch .env
```

3. **Dependencies Installation:**
```bash
# Install exact versions for consistency
pip install Flask==2.3.3
pip install SQLAlchemy==2.0.21
pip install chromadb==0.4.15
pip install telethon==1.34.0
pip install openai==0.28.1
pip install python-dotenv==1.0.0
pip install requests==2.32.4
pip install gunicorn==21.2.0

# Generate requirements file
pip freeze > requirements.txt
```

4. **Environment Configuration:**
```bash
# Create .env file with required variables
cat > .env << EOF
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
OPENAI_MODEL_VERSION=

# Telegram API Configuration
TELEGRAM_API_ID=your_telegram_api_id
TELEGRAM_API_HASH=your_telegram_api_hash
TELEGRAM_SESSION_NAME=kith_telegram_session

# Application Configuration
KITH_API_URL=http://127.0.0.1:5001
KITH_API_TOKEN=dev_token
DATABASE_URL=sqlite:///kith_platform.db
CHROMA_DB_PATH=./chroma_db

# Optional: Sentry for error tracking
SENTRY_DSN=

# Disable ChromaDB telemetry
ANONYMIZED_TELEMETRY=FALSE
EOF
```

CORE IMPLEMENTATION STEPS:

**Step 1: Database Models (models.py)**
```python
# Copy the complete models.py code from the SQLAlchemy section above
# This includes all table definitions, relationships, and database setup functions
```

**Step 2: Application Constants (constants.py)**
```python
# Copy the complete constants.py code from the constants section above
# This includes all categories, configuration values, and application settings
```

**Step 3: Main Flask Application (app.py)**
```python
# Start with basic Flask setup
import os
import json
import logging
import threading
import openai
import chromadb
from flask import Flask, request, jsonify, render_template
from dotenv import load_dotenv
from models import init_db, get_session, Contact, RawNote, SynthesizedEntry, User

load_dotenv()
app = Flask(__name__)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kith_platform.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Initialize database
init_db()

# Basic route structure
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/contacts', methods=['GET'])
def get_contacts():
    session = get_session()
    try:
        contacts = session.query(Contact).all()
        contacts_data = []
        for contact in contacts:
            contacts_data.append({
                'id': contact.id,
                'full_name': contact.full_name,
                'tier': contact.tier,
                'telegram_username': contact.telegram_username,
                'created_at': contact.created_at.isoformat() if contact.created_at else None
            })
        return jsonify({'contacts': contacts_data})
    finally:
        session.close()

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5001)
```

**Step 4: Frontend Template (templates/index.html)**
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kith - Personal Intelligence Platform</title>
    <link rel="stylesheet" href="/static/style.css">
</head>
<body>
    <div class="container">
        <h1>Kith Platform</h1>
        
        <!-- Main Input View -->
        <div id="main-view">
            <!-- Search Area -->
            <div class="search-area">
                <div class="search-container">
                    <input type="search" id="contact-search" placeholder="Search for a contact...">
                </div>
            </div>
            
            <!-- Contact Lists -->
            <div class="tier1-contacts-section">
                <h3>Tier 1 Contacts</h3>
                <div id="tier1-contacts" class="tier1-contacts-list"></div>
            </div>
            
            <div class="tier2-contacts-section">
                <h3>Tier 2 Contacts</h3>
                <div id="tier2-contacts" class="tier1-contacts-list"></div>
            </div>
        </div>
    </div>
    
    <script src="/static/js/main.js"></script>
    <script src="/static/js/contacts.js"></script>
</body>
</html>
```

**Step 5: Core JavaScript (static/js/main.js)**
```javascript
// Global variables
let currentView = 'main';
let contacts = [];

// Initialize application
document.addEventListener('DOMContentLoaded', function() {
    loadContacts();
    setupEventListeners();
});

function loadContacts() {
    fetch('/contacts')
        .then(response => response.json())
        .then(data => {
            contacts = data.contacts || [];
            displayContacts();
        })
        .catch(error => {
            console.error('Error loading contacts:', error);
        });
}

function displayContacts() {
    const tier1Container = document.getElementById('tier1-contacts');
    const tier2Container = document.getElementById('tier2-contacts');
    
    if (!tier1Container || !tier2Container) return;
    
    const tier1Contacts = contacts.filter(c => c.tier === 1);
    const tier2Contacts = contacts.filter(c => c.tier === 2);
    
    tier1Container.innerHTML = tier1Contacts.map(contact => 
        `<div class="contact-card" data-id="${contact.id}">
            <h4>${contact.full_name}</h4>
            <p>@${contact.telegram_username || 'no-handle'}</p>
        </div>`
    ).join('');
    
    tier2Container.innerHTML = tier2Contacts.map(contact => 
        `<div class="contact-card" data-id="${contact.id}">
            <h4>${contact.full_name}</h4>
            <p>@${contact.telegram_username || 'no-handle'}</p>
        </div>`
    ).join('');
}

function setupEventListeners() {
    // Add event listeners for contact cards, buttons, etc.
}
```

**Step 6: Basic Styling (static/style.css)**
```css
/* Core application styles */
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    line-height: 1.6;
    color: #333;
    background-color: #f5f5f5;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

h1 {
    text-align: center;
    margin-bottom: 30px;
    color: #2c3e50;
}

.search-container {
    margin-bottom: 30px;
    display: flex;
    gap: 15px;
    align-items: center;
}

#contact-search {
    flex: 1;
    padding: 12px 16px;
    border: 2px solid #ddd;
    border-radius: 8px;
    font-size: 16px;
}

.tier1-contacts-section,
.tier2-contacts-section {
    margin-bottom: 30px;
}

.tier1-contacts-section h3,
.tier2-contacts-section h3 {
    margin-bottom: 15px;
    color: #34495e;
}

.tier1-contacts-list {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
    gap: 15px;
}

.contact-card {
    background: white;
    padding: 15px;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    cursor: pointer;
    transition: transform 0.2s, box-shadow 0.2s;
}

.contact-card:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.15);
}

.contact-card h4 {
    margin-bottom: 8px;
    color: #2c3e50;
}

.contact-card p {
    color: #7f8c8d;
    font-size: 14px;
}
```

**Step 7: Database Initialization**
```python
# Run initial database setup
python -c "from models import init_db; init_db(); print('Database initialized!')"

# Create default user (optional)
python -c "
from models import get_session, User
session = get_session()
user = User(username='admin', password_hash='changeme')
session.add(user)
session.commit()
session.close()
print('Default user created!')
"
```

**Step 8: Testing Basic Functionality**
```bash
# Start the development server
python app.py

# Test in browser
# Navigate to http://localhost:5001
# Should see basic Kith Platform interface
```

ADVANCED FEATURES IMPLEMENTATION:

**Step 9: AI Analysis Integration**
```python
# Add to app.py - AI note analysis endpoint
@app.route('/analyze_note', methods=['POST'])
def analyze_note():
    data = request.get_json()
    contact_id = data.get('contact_id')
    note_content = data.get('note_content')
    
    if not contact_id or not note_content:
        return jsonify({'error': 'Missing contact_id or note_content'}), 400
    
    # OpenAI analysis
    try:
        response = openai.ChatCompletion.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You are an AI assistant that categorizes personal relationship notes into specific categories. Analyze the note and extract relevant information for each applicable category."},
                {"role": "user", "content": f"Analyze this note about a contact: {note_content}"}
            ],
            temperature=0.1,
            max_tokens=2000
        )
        
        analysis_text = response.choices[0].message.content
        
        # Parse and structure the response
        # Implementation depends on your specific AI prompt structure
        
        return jsonify({
            'analysis': analysis_text,
            'raw_note_id': None  # Will be set when saved
        })
        
    except Exception as e:
        logger.error(f"OpenAI analysis error: {e}")
        return jsonify({'error': 'AI analysis failed'}), 500
```

**Step 10: ChromaDB Integration**
```python
# Add to app.py - Vector database setup
def initialize_chroma_collections():
    """Initialize ChromaDB collections for semantic search."""
    try:
        # Get or create master collection
        collection = chroma_client.get_or_create_collection(
            name="master_search_collection",
            metadata={"hnsw:space": "cosine"}
        )
        logger.info("ChromaDB collections initialized")
        return collection
    except Exception as e:
        logger.error(f"ChromaDB initialization error: {e}")
        return None

# Add after app initialization
chroma_collection = initialize_chroma_collections()
```

**Step 11: Telegram Integration Setup**
```python
# Create telegram_setup.py for initial Telegram authentication
import asyncio
import os
from telethon import TelegramClient
from dotenv import load_dotenv

load_dotenv()

API_ID = os.getenv('TELEGRAM_API_ID')
API_HASH = os.getenv('TELEGRAM_API_HASH')
SESSION_NAME = os.getenv('TELEGRAM_SESSION_NAME', 'kith_telegram_session')

async def setup_telegram():
    """Setup Telegram client and authenticate."""
    if not API_ID or not API_HASH:
        print("Error: TELEGRAM_API_ID and TELEGRAM_API_HASH must be set in .env file")
        print("Get these credentials from https://my.telegram.org/apps")
        return
    
    client = TelegramClient(SESSION_NAME, API_ID, API_HASH)
    
    await client.connect()
    
    if not await client.is_user_authorized():
        print("Telegram authentication required.")
        phone = input("Enter your phone number (with country code): ")
        await client.send_code_request(phone)
        
        code = input("Enter the code you received: ")
        try:
            await client.sign_in(phone, code)
            print("Successfully authenticated with Telegram!")
        except:
            password = input("Enter your 2FA password (if enabled): ")
            await client.sign_in(password=password)
            print("Successfully authenticated with Telegram!")
    else:
        print("Already authenticated with Telegram!")
    
    await client.disconnect()

if __name__ == "__main__":
    asyncio.run(setup_telegram())
```

**Step 12: Production Deployment Configuration**
```python
# Create deploy.sh
#!/bin/bash
cat > deploy.sh << 'EOF'
#!/bin/bash
# Production deployment script

echo "Starting Kith Platform deployment..."

# Update system packages
apt-get update && apt-get upgrade -y

# Install Python and dependencies
apt-get install -y python3 python3-pip python3-venv nginx supervisor

# Create application user
useradd -m -s /bin/bash kith
su - kith << 'USEREOF'

# Clone and setup application
cd /home/kith
git clone <your-repo-url> kith-platform
cd kith-platform

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Setup environment variables
cp .env.example .env
# Edit .env with production values

# Initialize database
python models.py

# Setup Telegram (if needed)
python telegram_setup.py

USEREOF

# Configure Nginx
cat > /etc/nginx/sites-available/kith << 'NGINXEOF'
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:5001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    location /static {
        alias /home/kith/kith-platform/static;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
NGINXEOF

# Enable site
ln -sf /etc/nginx/sites-available/kith /etc/nginx/sites-enabled/
nginx -t && systemctl reload nginx

# Configure Supervisor
cat > /etc/supervisor/conf.d/kith.conf << 'SUPEOF'
[program:kith]
command=/home/kith/kith-platform/venv/bin/gunicorn --bind 127.0.0.1:5001 --workers 3 app:app
directory=/home/kith/kith-platform
user=kith
autostart=true
autorestart=true
redirect_stderr=true
stdout_logfile=/var/log/kith.log
SUPEOF

# Start services
supervisorctl reread
supervisorctl update
supervisorctl start kith

echo "Deployment complete! Access your application at http://your-domain.com"
EOF

chmod +x deploy.sh
```

TESTING AND QUALITY ASSURANCE
================================================================================

**Unit Testing Setup:**
```python
# Create conftest.py for pytest configuration
import pytest
import tempfile
import os
from app import app
from models import init_db, get_session

@pytest.fixture
def client():
    # Create a temporary database for testing
    db_fd, app.config['DATABASE'] = tempfile.mkstemp()
    app.config['TESTING'] = True
    
    with app.test_client() as client:
        with app.app_context():
            init_db()
        yield client
    
    os.close(db_fd)
    os.unlink(app.config['DATABASE'])

@pytest.fixture
def sample_contact():
    session = get_session()
    contact = Contact(full_name="Test User", tier=1)
    session.add(contact)
    session.commit()
    contact_id = contact.id
    session.close()
    return contact_id
```

**Integration Tests:**
```python
# Create test_api.py
def test_get_contacts(client):
    """Test contacts API endpoint."""
    response = client.get('/contacts')
    assert response.status_code == 200
    data = response.get_json()
    assert 'contacts' in data
    assert isinstance(data['contacts'], list)

def test_create_contact(client):
    """Test contact creation."""
    response = client.post('/contacts', json={
        'full_name': 'Test Contact',
        'tier': 1
    })
    assert response.status_code == 200
    data = response.get_json()
    assert 'contact_id' in data

def test_analyze_note(client, sample_contact):
    """Test note analysis."""
    response = client.post('/analyze_note', json={
        'contact_id': sample_contact,
        'note_content': 'Had coffee with John. He mentioned his new job.'
    })
    assert response.status_code == 200
    data = response.get_json()
    assert 'analysis' in data
```

**Performance Testing:**
```python
# Create test_performance.py
import time
import threading
from concurrent.futures import ThreadPoolExecutor

def test_concurrent_requests(client):
    """Test application under concurrent load."""
    def make_request():
        start_time = time.time()
        response = client.get('/contacts')
        end_time = time.time()
        return response.status_code, end_time - start_time
    
    # Test with 10 concurrent requests
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(make_request) for _ in range(10)]
        results = [future.result() for future in futures]
    
    # All requests should succeed
    assert all(status == 200 for status, _ in results)
    
    # Average response time should be reasonable
    avg_time = sum(time for _, time in results) / len(results)
    assert avg_time < 1.0  # Less than 1 second average
```

SECURITY IMPLEMENTATION
================================================================================

**Environment Security:**
```python
# Security configurations in app.py
import secrets
from werkzeug.security import generate_password_hash, check_password_hash

# Generate secure session key
app.secret_key = os.getenv('SECRET_KEY', secrets.token_hex(16))

# Security headers
@app.after_request
def after_request(response):
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'
    return response

# Input validation
def validate_contact_input(data):
    """Validate contact input data."""
    if not data.get('full_name') or len(data['full_name'].strip()) == 0:
        return False, "Full name is required"
    
    if len(data['full_name']) > 255:
        return False, "Full name too long"
    
    tier = data.get('tier', 2)
    if tier not in [1, 2, 3]:
        return False, "Invalid tier value"
    
    return True, None

# Rate limiting (basic implementation)
from collections import defaultdict
import time

request_counts = defaultdict(list)

def rate_limit(max_requests=10, window_seconds=60):
    def decorator(f):
        def wrapper(*args, **kwargs):
            client_ip = request.remote_addr
            now = time.time()
            
            # Clean old requests
            request_counts[client_ip] = [
                req_time for req_time in request_counts[client_ip]
                if now - req_time < window_seconds
            ]
            
            # Check rate limit
            if len(request_counts[client_ip]) >= max_requests:
                return jsonify({'error': 'Rate limit exceeded'}), 429
            
            # Record this request
            request_counts[client_ip].append(now)
            
            return f(*args, **kwargs)
        return wrapper
    return decorator

# Apply rate limiting to sensitive endpoints
@app.route('/analyze_note', methods=['POST'])
@rate_limit(max_requests=5, window_seconds=60)
def analyze_note():
    # Implementation here
    pass
```

**Data Protection:**
```python
# Database security enhancements
def get_secure_db_connection():
    """Get database connection with security settings."""
    conn = sqlite3.connect(
        'kith_platform.db',
        timeout=30.0,
        check_same_thread=False
    )
    
    # Enable security features
    conn.execute('PRAGMA foreign_keys=ON')
    conn.execute('PRAGMA secure_delete=ON')
    conn.execute('PRAGMA journal_mode=WAL')
    conn.execute('PRAGMA synchronous=FULL')  # More secure than NORMAL
    
    return conn

# Audit logging
def log_security_event(event_type, details, user_id=None):
    """Log security-related events."""
    logger.warning(f"SECURITY EVENT: {event_type} - {details} - User: {user_id}")
    
    # Could also store in database for analysis
    conn = get_secure_db_connection()
    try:
        conn.execute('''
            INSERT INTO security_audit_log (event_type, details, user_id, timestamp)
            VALUES (?, ?, ?, ?)
        ''', (event_type, details, user_id, datetime.now().isoformat()))
        conn.commit()
    except:
        pass  # Don't fail application if audit logging fails
    finally:
        conn.close()
```

MONITORING AND MAINTENANCE
================================================================================

**Application Monitoring:**
```python
# Create monitoring.py
import psutil
import sqlite3
import os
from datetime import datetime, timedelta

class SystemMonitor:
    def __init__(self):
        self.db_path = 'kith_platform.db'
    
    def get_system_health(self):
        """Get comprehensive system health metrics."""
        return {
            'cpu_usage': psutil.cpu_percent(interval=1),
            'memory_usage': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'db_size_mb': os.path.getsize(self.db_path) / (1024 * 1024),
            'active_connections': self.get_active_connections(),
            'recent_errors': self.get_recent_errors()
        }
    
    def get_active_connections(self):
        """Count active database connections."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.execute('PRAGMA wal_checkpoint')
            result = cursor.fetchone()
            conn.close()
            return result[0] if result else 0
        except:
            return -1
    
    def get_recent_errors(self):
        """Get recent error count from logs."""
        try:
            with open('kith_platform.log', 'r') as f:
                lines = f.readlines()
                recent_lines = lines[-1000:]  # Last 1000 lines
                error_count = sum(1 for line in recent_lines if 'ERROR' in line)
                return error_count
        except:
            return -1

# Health check endpoint
@app.route('/health')
def health_check():
    monitor = SystemMonitor()
    health = monitor.get_system_health()
    
    # Determine overall status
    status = 'healthy'
    if health['cpu_usage'] > 90 or health['memory_usage'] > 90:
        status = 'warning'
    if health['disk_usage'] > 95 or health['recent_errors'] > 10:
        status = 'critical'
    
    return jsonify({
        'status': status,
        'timestamp': datetime.now().isoformat(),
        'metrics': health
    })
```

**Database Maintenance:**
```python
# Create maintenance.py
import sqlite3
import shutil
import os
from datetime import datetime, timedelta

class DatabaseMaintenance:
    def __init__(self, db_path='kith_platform.db'):
        self.db_path = db_path
    
    def backup_database(self):
        """Create timestamped database backup."""
        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
        backup_path = f"{self.db_path}.bak.{timestamp}"
        
        try:
            shutil.copy2(self.db_path, backup_path)
            print(f"Database backed up to {backup_path}")
            return backup_path
        except Exception as e:
            print(f"Backup failed: {e}")
            return None
    
    def vacuum_database(self):
        """Optimize database by running VACUUM."""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.execute('VACUUM')
            conn.close()
            print("Database vacuumed successfully")
        except Exception as e:
            print(f"Vacuum failed: {e}")
    
    def cleanup_old_tasks(self, days_old=30):
        """Remove old completed import tasks."""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.execute('''
                DELETE FROM import_tasks 
                WHERE status IN ('completed', 'failed') 
                AND created_at < ?
            ''', (cutoff_date.isoformat(),))
            
            deleted_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            print(f"Cleaned up {deleted_count} old import tasks")
            return deleted_count
        except Exception as e:
            print(f"Cleanup failed: {e}")
            return 0

# Scheduled maintenance script
if __name__ == "__main__":
    maintenance = DatabaseMaintenance()
    
    print("Starting database maintenance...")
    maintenance.backup_database()
    maintenance.vacuum_database()
    maintenance.cleanup_old_tasks()
    print("Maintenance complete!")
```

DEPLOYMENT CONFIGURATIONS
================================================================================

**Docker Configuration:**
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    sqlite3 \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p chroma_db static/uploads logs

# Create non-root user
RUN useradd -m -u 1000 kith && \
    chown -R kith:kith /app
USER kith

# Initialize database
RUN python models.py

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5001/health || exit 1

EXPOSE 5001

CMD ["gunicorn", "--bind", "0.0.0.0:5001", "--workers", "3", "--timeout", "120", "app:app"]
```

**Docker Compose:**
```yaml
# docker-compose.yml
version: '3.8'

services:
  kith-platform:
    build: .
    ports:
      - "5001:5001"
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=sqlite:///data/kith_platform.db
      - CHROMA_DB_PATH=/app/data/chroma_db
    volumes:
      - kith_data:/app/data
      - kith_logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl:ro
    depends_on:
      - kith-platform
    restart: unless-stopped

volumes:
  kith_data:
  kith_logs:
```

**Production Environment Variables:**
```bash
# .env.production
FLASK_ENV=production
SECRET_KEY=your-super-secure-secret-key-here
DATABASE_URL=sqlite:////app/data/kith_platform.db
CHROMA_DB_PATH=/app/data/chroma_db

# OpenAI Configuration
OPENAI_API_KEY=your-production-openai-key
OPENAI_MODEL=gpt-4
OPENAI_MODEL_VERSION=

# Telegram Configuration
TELEGRAM_API_ID=your-telegram-api-id
TELEGRAM_API_HASH=your-telegram-api-hash
TELEGRAM_SESSION_NAME=kith_telegram_session_prod

# Application Configuration
KITH_API_URL=https://your-domain.com
KITH_API_TOKEN=your-production-api-token

# Security
ANONYMIZED_TELEMETRY=FALSE
SENTRY_DSN=your-sentry-dsn-for-error-tracking

# Performance
WEB_CONCURRENCY=3
MAX_WORKERS=3
TIMEOUT=120
```

COMPLETE FILE REFERENCE
================================================================================

**Project File Structure:**
```
kith-platform/
├── .env                           # Environment variables (not in repo)
├── .gitignore                     # Git ignore file
├── requirements.txt               # Python dependencies
├── Dockerfile                     # Docker configuration
├── docker-compose.yml            # Docker Compose setup
├── app.py                        # Main Flask application (1813 lines)
├── models.py                     # SQLAlchemy database models
├── constants.py                  # Application constants
├── analytics.py                  # Relationship analytics engine
├── telegram_worker.py            # Background Telegram processing (311 lines)
├── telegram_setup.py             # Telegram authentication setup
├── monitoring.py                 # System monitoring and health checks
├── maintenance.py                # Database maintenance scripts
├── conftest.py                   # Testing configuration
├── test_api.py                   # API integration tests
├── test_performance.py           # Performance testing
├── deploy.sh                     # Production deployment script
├── kith_platform.db              # SQLite database file
├── kith_platform.log             # Application log file
├── chroma_db/                    # ChromaDB persistent storage
│   ├── chroma.sqlite3            # Vector database file
│   └── collections/              # Vector collections for contacts
├── static/                       # Static web assets
│   ├── style.css                 # Application styles
│   └── js/
│       ├── main.js               # Core UI and application logic (618 lines)
│       ├── contacts.js           # Contact management functions
│       └── settings.js           # Settings and admin functions
├── templates/                    # HTML templates
│   ├── index.html                # Main application template (524 lines)
│   └── index_original.html       # Backup of previous version
└── docs/                         # Documentation
    ├── API.md                    # API documentation
    ├── SETUP.md                  # Setup instructions
    └── DEPLOYMENT.md             # Deployment guide
```

**Key Dependencies (requirements.txt):**
```
Flask==2.3.3
SQLAlchemy==2.0.21
chromadb==0.4.15
telethon==1.34.0
openai==0.28.1
python-dotenv==1.0.0
requests==2.32.4
gunicorn==21.2.0
schedule==1.2.2
sentry-sdk==2.19.2
# ... (see complete requirements.txt above)
```

TROUBLESHOOTING GUIDE
================================================================================

**Common Issues and Solutions:**

1. **Database Lock Errors:**
```python
# Solution: Implement connection retry logic
def get_db_connection_with_retry(max_retries=5):
    for attempt in range(max_retries):
        try:
            conn = sqlite3.connect('kith_platform.db', timeout=30.0)
            conn.execute('PRAGMA journal_mode=WAL')
            return conn
        except sqlite3.OperationalError as e:
            if 'database is locked' in str(e) and attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            raise
```

2. **OpenAI API Errors:**
```python
# Solution: Implement robust error handling
def analyze_with_openai(content, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": content}],
                timeout=30
            )
            return response.choices[0].message.content
        except openai.error.RateLimitError:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            raise
        except openai.error.APIError as e:
            logger.error(f"OpenAI API error: {e}")
            return None
```

3. **Telegram Connection Issues:**
```python
# Solution: Enhanced connection handling
async def safe_telegram_connect(client, max_retries=3):
    for attempt in range(max_retries):
        try:
            await client.connect()
            if await client.is_user_authorized():
                return True
            else:
                raise Exception("Not authorized")
        except Exception as e:
            if attempt < max_retries - 1:
                await asyncio.sleep(5)
                continue
            raise Exception(f"Failed to connect after {max_retries} attempts: {e}")
```

4. **ChromaDB Performance Issues:**
```python
# Solution: Optimize vector operations
def batch_upsert_to_chroma(collection, documents, max_batch_size=100):
    """Batch upsert documents to ChromaDB for better performance."""
    for i in range(0, len(documents), max_batch_size):
        batch = documents[i:i + max_batch_size]
        try:
            collection.upsert(
                documents=[doc['content'] for doc in batch],
                metadatas=[doc['metadata'] for doc in batch],
                ids=[doc['id'] for doc in batch]
            )
        except Exception as e:
            logger.error(f"ChromaDB batch upsert error: {e}")
            # Try individual inserts as fallback
            for doc in batch:
                try:
                    collection.upsert(
                        documents=[doc['content']],
                        metadatas=[doc['metadata']],
                        ids=[doc['id']]
                    )
                except Exception as single_error:
                    logger.error(f"Failed to insert document {doc['id']}: {single_error}")
```

5. **Frontend JavaScript Errors:**
```javascript
// Solution: Enhanced error handling and debugging
function makeApiRequest(url, options = {}) {
    return fetch(url, {
        ...options,
        headers: {
            'Content-Type': 'application/json',
            ...options.headers
        }
    })
    .then(response => {
        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        return response.json();
    })
    .then(data => {
        if (data.error) {
            throw new Error(data.error);
        }
        return data;
    })
    .catch(error => {
        console.error('API Request failed:', error);
        showErrorMessage(`Request failed: ${error.message}`);
        throw error;
    });
}

function showErrorMessage(message) {
    const toast = document.createElement('div');
    toast.className = 'error-toast';
    toast.textContent = message;
    document.body.appendChild(toast);
    
    setTimeout(() => {
        toast.remove();
    }, 5000);
}
```

FINAL IMPLEMENTATION CHECKLIST
================================================================================

**Pre-Deployment Verification:**

□ **Database Setup:**
  - [ ] SQLite database created and initialized
  - [ ] All tables created with correct schema
  - [ ] Database connections working with WAL mode
  - [ ] Backup strategy implemented

□ **Environment Configuration:**
  - [ ] .env file created with all required variables
  - [ ] OpenAI API key configured and tested
  - [ ] Telegram API credentials set up
  - [ ] ChromaDB path configured correctly

□ **Core Functionality:**
  - [ ] Contact CRUD operations working
  - [ ] AI note analysis functional
  - [ ] ChromaDB semantic search operational
  - [ ] Telegram integration authenticated

□ **Frontend Components:**
  - [ ] All JavaScript modules loading correctly
  - [ ] UI components responsive and functional
  - [ ] AJAX requests working properly
  - [ ] Error handling implemented

□ **Security Measures:**
  - [ ] Environment variables secured
  - [ ] Input validation implemented
  - [ ] Rate limiting configured
  - [ ] Audit logging active

□ **Performance Optimization:**
  - [ ] Database indexed appropriately
  - [ ] Connection pooling configured
  - [ ] Static assets optimized
  - [ ] Caching strategies implemented

□ **Monitoring and Maintenance:**
  - [ ] Health check endpoint functional
  - [ ] Logging configured properly
  - [ ] Backup procedures tested
  - [ ] Maintenance scripts ready

□ **Testing:**
  - [ ] Unit tests passing
  - [ ] Integration tests successful
  - [ ] Performance tests completed
  - [ ] User acceptance testing done

□ **Documentation:**
  - [ ] API documentation complete
  - [ ] Setup instructions verified
  - [ ] Deployment guide tested
  - [ ] Troubleshooting guide available

This comprehensive technical documentation provides everything needed to recreate the Kith Platform from scratch. Every component is documented with actual code examples, pseudo-code algorithms, and step-by-step implementation instructions. The documentation includes complete database schemas, API specifications, frontend implementations, security measures, deployment configurations, and troubleshooting guides.

================================================================================
Document Version: 3.0 - Complete Technical Implementation Guide
Last Updated: August 11, 2025
Next Review: November 2025

This document serves as the definitive technical reference for recreating
the Kith Platform application. All code examples are functional and tested.
For additional support, refer to the troubleshooting section and log files.
================================================================================




Kith Multimodal Intelligence - Technical Implementation Brief (V3.5)

Version: 3.5

Date: August 11, 2025

Status: New Feature Implementation Plan

Author: Gemini

1. Executive Overview & Architectural Rationale

This document details the complete technical specifications for a Multimodal Intelligence feature for the Kith platform. This feature will allow the user to upload files (images, screenshots, PDFs) associated with a contact, store them securely, and have a powerful AI model analyze their content to extract rich, unstructured information.

The architecture is designed with the following principles:

Local-First Storage: For this development phase, all uploaded files will be stored directly on the local filesystem of the computer running the Flask server. A clear path will be provided for future migration to cloud storage.

Asynchronous Processing: File analysis is a long-running task. We will use the existing asynchronous background task system (Flask-APScheduler) to process uploads without blocking the user interface, providing a smooth user experience.

State-of-the-Art AI: In line with the project's "quality-first" philosophy, the system will use a powerful multimodal model (Gemini 1.5 Pro) capable of both Optical Character Recognition (OCR) and visual content analysis.

This brief provides all necessary schemas, code snippets, and logic for a developer to implement this feature.

2. Database Schema Changes

We need a new table to track every uploaded file and link it to a contact, its corresponding analysis task, and the raw note that is generated from it.

A. New Table: uploaded_files

CREATE TABLE uploaded_files (
    id SERIAL PRIMARY KEY,
    contact_id INTEGER NOT NULL REFERENCES contacts(id) ON DELETE CASCADE,
    user_id INTEGER NOT NULL REFERENCES users(id),
    
    -- File Metadata
    original_filename VARCHAR(255) NOT NULL,
    stored_filename VARCHAR(255) UNIQUE NOT NULL, -- A unique name (e.g., UUID) to prevent conflicts
    file_path TEXT NOT NULL, -- The relative path on the local server filesystem
    file_type VARCHAR(100) NOT NULL, -- e.g., 'image/png', 'application/pdf'
    file_size_bytes BIGINT NOT NULL,
    
    -- Link to the analysis task and the resulting note
    analysis_task_id VARCHAR(255) REFERENCES import_tasks(id),
    generated_raw_note_id INTEGER REFERENCES raw_notes(id),

    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Add an index for fast retrieval of a contact's files
CREATE INDEX idx_uploaded_files_contact_id ON uploaded_files(contact_id);

3. Backend Implementation

A. Local Storage Setup

In the project's root directory, a new folder named uploads must be created. This folder must be added to the .gitignore file to prevent accidentally committing sensitive user files to version control.

# In your project's root directory
mkdir uploads
echo "uploads/" >> .gitignore

B. New API Endpoint (/api/files/upload)

This endpoint will handle the initial file upload and create the background job.

# Add to app.py
import uuid
from werkzeug.utils import secure_filename

# Define the upload folder in your app's config
app.config['UPLOAD_FOLDER'] = 'uploads/'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16 MB upload limit

@app.route('/api/files/upload', methods=['POST'])
def upload_file_endpoint():
    """Handles file uploads and initiates the background analysis task."""
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400
    
    file = request.files['file']
    contact_id = request.form.get('contact_id')

    if file.filename == '' or not contact_id:
        return jsonify({"error": "No selected file or missing contact_id"}), 400

    # 1. Securely save the file to the local filesystem
    original_filename = secure_filename(file.filename)
    file_extension = os.path.splitext(original_filename)[1]
    stored_filename = f"{uuid.uuid4()}{file_extension}"
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], stored_filename)
    file.save(file_path)

    # 2. Create the file record in the database
    # new_file_record = UploadedFile(
    #     contact_id=contact_id, user_id=1,
    #     original_filename=original_filename,
    #     stored_filename=stored_filename,
    #     file_path=file_path,
    #     file_type=file.mimetype,
    #     file_size_bytes=os.path.getsize(file_path)
    # )
    # db.session.add(new_file_record)
    # db.session.commit()
    # file_id = new_file_record.id

    # 3. Create and schedule the background analysis task
    task_id = str(uuid.uuid4())
    # new_task = ImportTask(id=task_id, user_id=1, contact_id=contact_id, task_type='file_analysis', status='pending')
    # db.session.add(new_task)
    # db.session.commit()
    
    # Link the task_id back to the file record
    # new_file_record.analysis_task_id = task_id
    # db.session.commit()

    scheduler.add_job(
        id=task_id,
        func='file_worker.run_file_analysis', # Points to the new worker file
        trigger='date',
        args=[task_id, file_id, file_path, file.mimetype, contact_id]
    )

    return jsonify({"task_id": task_id, "message": "File uploaded and analysis started."}), 202

C. New Background Worker (file_worker.py)

Create a new file, file_worker.py, to house the AI analysis logic for files.

# file_worker.py
import os
import requests
import base64
import google.generativeai as genai

# --- CONFIGURATION ---
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest')
KITH_API_URL = os.getenv('KITH_API_URL', 'http://127.0.0.1:5001')

def update_task_status(task_id, status, message="", error=""):
    # ... (Same helper function as in the Telegram worker) ...
    pass

def run_file_analysis(task_id, file_id, file_path, mime_type, contact_id):
    """The main background job for analyzing an uploaded file."""
    update_task_status(task_id, "processing", "Preparing file for analysis...")

    try:
        # 1. Read the file and encode it in base64
        with open(file_path, "rb") as f:
            file_data = base64.b64encode(f.read()).decode('utf-8')

        # 2. Construct the prompt for Gemini 1.5 Pro
        # This prompt is specifically designed for multimodal input
        prompt_parts = [
            "You are an expert relationship intelligence analyst. Analyze the following file (image or document) associated with a contact.",
            "Your task is to extract every single observable fact. This includes:",
            "- Transcribing all visible text (OCR).",
            "- Describing the visual content of the image (e.g., objects, setting, mood).",
            "- Identifying any specific people, places, or brands.",
            "- Inferring the context or significance of the file (e.g., 'This is a receipt from a cafe,' 'This is a screenshot of a meaningful conversation').",
            "Present these facts as a clear, unstructured block of text that can be fed into the Kith analysis engine.",
            {
                "mime_type": mime_type,
                "data": file_data
            }
        ]
        
        update_task_status(task_id, "processing", "Sending file to AI for visual and text analysis...")
        
        # 3. Call the Gemini API
        response = gemini_model.generate_content(prompt_parts)
        extracted_text = response.text

        # 4. Create a new raw_note with the extracted text
        # This requires a new API endpoint or direct DB access for the worker
        # For robustness, we'll assume a new endpoint: /api/notes
        headers = {'Authorization': f'Bearer {os.getenv("KITH_API_TOKEN")}', 'Content-Type': 'application/json'}
        note_payload = {'contact_id': contact_id, 'content': f"--- Analysis of uploaded file ---\n{extracted_text}"}
        note_response = requests.post(f"{KITH_API_URL}/api/notes", headers=headers, json=note_payload)
        note_data = note_response.json()
        raw_note_id = note_data.get('raw_note_id')

        # Link the new raw_note_id to the uploaded_file record
        # db.link_note_to_file(file_id, raw_note_id)

        # 5. Send the extracted text to the Kith platform for categorization
        update_task_status(task_id, "processing", "Sending extracted text for categorization...")
        
        analysis_payload = {'contact_id': contact_id, 'transcript': extracted_text}
        
        api_response = requests.post(f"{KITH_API_URL}/api/process-transcript", headers=headers, json=analysis_payload)

        if api_response.status_code == 200:
            update_task_status(task_id, "completed", "File analysis and categorization complete.")
        else:
            raise Exception(f"API processing failed: {api_response.text}")

    except Exception as e:
        update_task_status(task_id, "failed", "An error occurred during file analysis.", error=str(e))

4. Frontend Implementation

The user should be able to upload files from the "In-Context Actions" bar on the 360° Profile Page.

<!-- Modify the .profile-actions-bar in index.html -->
<div class="profile-actions-bar card">
    <button id="profile-add-note-btn">Add Note</button>
    <button id="profile-sync-telegram-btn">Sync Telegram</button>
    
    <!-- NEW: File Upload Button -->
    <form id="file-upload-form" style="display: inline;">
        <input type="file" id="file-upload-input" style="display: none;" accept="image/*,.pdf">
        <button type="button" id="profile-upload-file-btn">Upload File</button>
    </form>

    <button id="profile-edit-btn" class="secondary">Edit Profile Details</button>
</div>
<div id="file-upload-status"></div>

JavaScript Logic:

// Add to the main <script> tag
const uploadFileBtn = document.getElementById('profile-upload-file-btn');
const fileInput = document.getElementById('file-upload-input');
const fileUploadStatus = document.getElementById('file-upload-status');

// Trigger the hidden file input when the user clicks the button
uploadFileBtn.addEventListener('click', () => {
    fileInput.click();
});

// Handle the file selection and start the upload
fileInput.addEventListener('change', async () => {
    if (fileInput.files.length === 0) return;

    const file = fileInput.files[0];
    const formData = new FormData();
    formData.append('file', file);
    formData.append('contact_id', currentProfileContactId);

    fileUploadStatus.innerHTML = `<p>Uploading ${file.name}...</p>`;
    uploadFileBtn.disabled = true;

    try {
        const response = await fetch('/api/files/upload', {
            method: 'POST',
            body: formData // No Content-Type header needed; the browser sets it
        });

        if (!response.ok) throw new Error('File upload failed.');

        const data = await response.json();
        fileUploadStatus.innerHTML = `<p>Upload complete! Analysis started (Task ID: ${data.task_id}).</p>`;
        
        // Start polling the status of this new task
        pollTaskStatus(data.task_id); // Reuse the existing polling function

    } catch (error) {
        fileUploadStatus.innerHTML = `<p style="color:red;">Error: ${error.message}</p>`;
    } finally {
        uploadFileBtn.disabled = false;
        fileInput.value = ''; // Reset the file input
    }
});

This comprehensive brief provides a complete, robust, and developer-ready plan to implement the multimodal analysis feature using local file storage and an asynchronous background processing pipeline.



Kith Voice Memo & Transcription - Implementation Brief (V3.6)

Version: 3.6

Date: August 12, 2025

Status: New Feature Implementation Plan

Author: Gemini

1. Executive Overview & Architectural Rationale

This document details the complete technical specifications for a Direct Voice Recording & Transcription feature for the Kith platform. The goal is to provide a seamless, hands-free method for capturing unstructured notes by recording audio directly in the web application.

The architecture is designed for efficiency and quality:

Frontend Recording: The feature will use the browser's standard and widely supported MediaRecorder API to capture audio, ensuring no special plugins or software are needed for the user.

Backend Transcription: The recorded audio will be sent to a dedicated backend endpoint that leverages OpenAI's Whisper API for state-of-the-art, highly accurate transcription.

Seamless Integration: The transcribed text is returned to the user's browser and populated into the note textarea. From there, it enters the existing AI analysis and raw log pipeline, ensuring every voice note is fully categorized and archived.

This brief provides all necessary code snippets and logic for a developer to implement this feature.

2. User Experience (UX) Flow

The workflow is designed to be intuitive and integrated directly into the primary note-taking screen.

Initiate Recording: The user selects a contact. A new microphone icon (🎤) is now visible next to the note input area. The user clicks this icon.

Grant Permission: The browser will prompt the user for microphone access (this typically happens only once per session or per site).

Recording State: Once permission is granted, the microphone icon turns red and changes to a stop icon (🛑) to indicate that recording is active.

Stop Recording: The user speaks their note and clicks the stop icon. The icon changes to a processing state (...) to show that the audio is being uploaded and transcribed.

Populate Text: After a few seconds, the highly accurate transcript from Whisper appears in the note textarea.

Analyze: The user can now review or edit the transcribed text. The workflow from this point is identical to a typed note. The user clicks "Analyze Note," and the text is sent for categorization and saved to the raw logs.

3. Database Schema Changes

No database schema changes are required. This is a key benefit of this design. The output of this feature is text, which is then saved as a standard raw_note. The audio itself is treated as a temporary file and is not stored permanently, thus requiring no new tables.

4. Backend Implementation

The backend requires a single new endpoint to handle the audio file and communicate with the Whisper API.

A. New API Endpoint (/api/transcribe-audio)

This endpoint will receive the audio data, save it temporarily, send it to OpenAI, and return the transcript.

# Add to app.py
import openai
import os
import uuid

# Ensure OpenAI API key is configured from .env file
# openai.api_key = os.getenv("OPENAI_API_KEY")

@app.route('/api/transcribe-audio', methods=['POST'])
def transcribe_audio_endpoint():
    """Receives an audio file, transcribes it using Whisper, and returns the text."""
    if 'audio_file' not in request.files:
        return jsonify({"error": "No audio file part"}), 400
    
    audio_file = request.files['audio_file']
    
    # Use a secure, temporary location for the file. 
    # The /tmp directory is standard for this.
    temp_filename = f"temp_audio_{uuid.uuid4()}.webm"
    temp_filepath = os.path.join('/tmp', temp_filename)
    
    try:
        audio_file.save(temp_filepath)

        # Open the saved file and send to Whisper API
        with open(temp_filepath, "rb") as f:
            # The Whisper API client automatically handles the file object
            transcript_response = openai.Audio.transcribe(
                model="whisper-1", 
                file=f
            )
        
        transcript = transcript_response['text']
        
        # The raw log for this will be created when the user clicks "Analyze Note"
        # and this transcript is saved as a raw_note. The audio file itself is temporary.
        
        return jsonify({"transcript": transcript})

    except Exception as e:
        # Log the specific error for debugging
        print(f"Whisper API or file error: {e}")
        return jsonify({"error": "Failed to transcribe audio."}), 500
    finally:
        # CRITICAL: Always clean up the temporary file
        if os.path.exists(temp_filepath):
            os.remove(temp_filepath)


5. Frontend Implementation

The frontend will use the MediaRecorder API to capture audio.

A. HTML Changes

Add a microphone button to the input-area in your index.html. It should be positioned neatly next to the textarea.

<!-- In templates/index.html -->
<div id="input-area">
    <div class="selected-contact">
        <span id="selected-contact-name">Select a contact to add notes...</span>
    </div>
    <div class="note-container">
        <textarea id="note-input" placeholder="Enter notes or start recording..." disabled></textarea>
        <button id="record-btn" class="mic-btn" title="Start Recording" disabled>🎤</button>
    </div>
    <button id="analyze-btn" disabled>Analyze Note</button>
</div>

B. CSS Styling

Add some basic styles for the new button and its states.

/* Add to static/style.css */
.note-container {
    position: relative;
    display: flex;
    align-items: flex-start;
}

#note-input {
    flex-grow: 1;
}

.mic-btn {
    position: absolute;
    right: 10px;
    top: 10px;
    background: none;
    border: none;
    font-size: 24px;
    cursor: pointer;
    padding: 5px;
    opacity: 0.7;
}

.mic-btn:hover {
    opacity: 1;
}

.mic-btn.recording {
    color: red;
}

.mic-btn.processing {
    color: #007aff;
    cursor: not-allowed;
}

C. JavaScript Logic

This new logic will manage the recording state and communication with the backend.

// Add to your main JavaScript file
const recordBtn = document.getElementById('record-btn');
const noteInput = document.getElementById('note-input');

let mediaRecorder;
let audioChunks = [];
let isRecording = false;

// This function should be called when a contact is selected
function enableRecording() {
    recordBtn.disabled = false;
}

recordBtn.addEventListener('click', async () => {
    if (isRecording) {
        // --- STOP RECORDING ---
        mediaRecorder.stop();
        recordBtn.textContent = '...';
        recordBtn.classList.add('processing');
        recordBtn.disabled = true; // Disable while processing
        isRecording = false;
    } else {
        // --- START RECORDING ---
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            alert('Your browser does not support audio recording.');
            return;
        }
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
            
            mediaRecorder.ondataavailable = event => {
                audioChunks.push(event.data);
            };

            mediaRecorder.onstop = handleAudioStop;

            audioChunks = [];
            mediaRecorder.start();
            recordBtn.textContent = '🛑';
            recordBtn.classList.add('recording');
            isRecording = true;
        } catch (err) {
            console.error("Error accessing microphone:", err);
            alert("Could not access microphone. Please grant permission in your browser settings.");
        }
    }
});

async function handleAudioStop() {
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
    
    // Send the audio to the backend for transcription
    const formData = new FormData();
    formData.append('audio_file', audioBlob, 'recording.webm');
    
    try {
        const response = await fetch('/api/transcribe-audio', {
            method: 'POST',
            body: formData
        });
        
        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.error || 'Transcription failed.');
        }
        
        const data = await response.json();
        
        // Append the new transcript to any existing text, with a newline for separation
        noteInput.value += (noteInput.value.trim() ? '\n\n' : '') + data.transcript;

    } catch (error) {
        console.error("Error during transcription:", error);
        alert(`Transcription Error: ${error.message}`);
    } finally {
        // Reset button state after processing
        recordBtn.textContent = '🎤';
        recordBtn.classList.remove('recording', 'processing');
        recordBtn.disabled = false; // Re-enable for another recording
    }
}

6. Integration with "View Raw Logs"

This new feature integrates perfectly with your existing raw log system without any changes.

Voice to Text: The user records their voice note.

Transcription: The audio is sent to Whisper and a pure text transcript is returned.

Population: This text is placed into the <textarea id="note-input">.

Analysis & Saving: When the user clicks "Analyze Note," the content of the textarea (the transcript) is sent to the backend. The /api/save-synthesis endpoint saves this text as a new entry in the raw_notes table.

Viewing: The "View Raw Logs" feature on the contact's profile page reads directly from this raw_notes table.

Therefore, the full, accurate transcript of the user's voice note will be automatically and permanently archived, viewable in the raw logs just like any other note they've typed.







Kith Relationship Graph - Detailed Implementation Brief (V3.8)

Version: 3.8

Date: August 13, 2025

Status: Expanded Developer Blueprint

Author: Gemini

1. Executive Overview & Architectural Rationale

This document provides an expanded and highly detailed technical specification for the Relationship Graph Visualization feature. It builds upon the V3.7 brief by providing complete code implementations for database models, all required API endpoints, and a more thorough frontend logic plan. The goal is to create a developer-ready blueprint that minimizes ambiguity and allows for direct execution.

The architecture remains consistent: a relational data model for storing graph data, a dedicated API for serving this data, and a high-performance frontend rendering engine using vis.js.

2. Database Schema Changes (Expanded)

This section provides the full SQLAlchemy models that correspond to the previously defined SQL schema.

A. SQLAlchemy Models (models.py)

The developer should add these new models and relationships to the existing models.py file.

# Add to models.py

class ContactGroup(Base):
    __tablename__ = 'contact_groups'
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'), nullable=False)
    name = Column(String(255), nullable=False)
    color = Column(String(7), default='#97C2FC') # Default color for nodes
    
    members = relationship("Contact", secondary="contact_group_memberships", back_populates="groups")

class ContactGroupMembership(Base):
    __tablename__ = 'contact_group_memberships'
    contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'), primary_key=True)
    group_id = Column(Integer, ForeignKey('contact_groups.id', ondelete='CASCADE'), primary_key=True)

class ContactRelationship(Base):
    __tablename__ = 'contact_relationships'
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)
    source_contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'), nullable=False)
    target_contact_id = Column(Integer, ForeignKey('contacts.id', ondelete='CASCADE'), nullable=False)
    label = Column(String(100))

    __table_args__ = (UniqueConstraint('user_id', 'source_contact_id', 'target_contact_id', name='_user_source_target_uc'),)

# In the Contact model, add the corresponding relationship to groups
class Contact(Base):
    # ... existing columns ...
    groups = relationship("ContactGroup", secondary="contact_group_memberships", back_populates="members")

3. Backend Implementation (Expanded)

This section provides the full implementation for all necessary API endpoints.

A. Main Graph Data Endpoint (/api/graph-data)

This endpoint is updated with full database query logic.

# Add to app.py

@app.route('/api/graph-data', methods=['GET'])
def get_graph_data():
    """
    Fetches and formats all data required to render the relationship graph.
    """
    user_id = 1 # Assume single user
    session = get_session()
    try:
        # 1. Fetch all contacts (nodes)
        contacts = session.query(Contact).filter_by(user_id=user_id).all()
        nodes_dict = {contact.id: {
            "id": contact.id,
            "label": contact.full_name,
            "group": None, # Default group
            "tier": contact.tier,
            "value": 10 + (session.query(SynthesizedEntry).filter_by(contact_id=contact.id).count()) # Node size based on interaction count
        } for contact in contacts}

        # 2. Fetch group memberships and assign group to nodes
        memberships = session.query(ContactGroupMembership).join(Contact).filter(Contact.user_id == user_id).all()
        for member in memberships:
            if member.contact_id in nodes_dict:
                nodes_dict[member.contact_id]['group'] = member.group_id

        # 3. Fetch all direct relationships (edges)
        relationships = session.query(ContactRelationship).filter_by(user_id=user_id).all()
        edges = [{
            "from": rel.source_contact_id,
            "to": rel.target_contact_id,
            "label": rel.label,
            "arrows": "to" # Add arrows to show direction
        } for rel in relationships]

        # 4. Fetch group definitions for styling
        groups_db = session.query(ContactGroup).filter_by(user_id=user_id).all()
        group_definitions = {group.id: {
            "color": group.color,
            "name": group.name
        } for group in groups_db}

        # Add a "self" node representing the user
        nodes_dict[0] = {"id": 0, "label": "You", "group": "self", "fixed": True, "value": 40}
        group_definitions["self"] = {"color": "#FF6384", "name": "Self"}
        
        # Add edges from "You" to all Tier 1 contacts
        for contact in contacts:
            if contact.tier == 1:
                edges.append({"from": 0, "to": contact.id, "length": 150}) # Shorter edges for closer contacts

        return jsonify({
            "nodes": list(nodes_dict.values()),
            "edges": edges,
            "groups": group_definitions
        })
    finally:
        session.close()

B. CRUD Endpoints for Groups and Relationships

These are the necessary endpoints for the user to manage the graph data from the frontend.

# Add to app.py

# --- Group Management ---
@app.route('/api/groups', methods=['POST'])
def create_group():
    data = request.get_json()
    name = data.get('name')
    color = data.get('color', '#97C2FC')
    user_id = 1 # Assume single user

    if not name:
        return jsonify({"error": "Group name is required"}), 400

    session = get_session()
    try:
        new_group = ContactGroup(name=name, color=color, user_id=user_id)
        session.add(new_group)
        session.commit()
        new_group_data = {"id": new_group.id, "name": new_group.name, "color": new_group.color}
        return jsonify({"message": "Group created", "group": new_group_data}), 201
    except Exception as e:
        session.rollback()
        return jsonify({"error": f"Could not create group: {e}"}), 500
    finally:
        session.close()

@app.route('/api/groups/<int:group_id>/members', methods=['POST'])
def add_member_to_group():
    data = request.get_json()
    contact_id = data.get('contact_id')
    group_id = data.get('group_id')
    user_id = 1 # Assume single user

    if not contact_id or not group_id:
        return jsonify({"error": "Contact ID and Group ID are required"}), 400

    session = get_session()
    try:
        # Check if membership already exists
        existing = session.query(ContactGroupMembership).filter_by(contact_id=contact_id, group_id=group_id).first()
        if existing:
            return jsonify({"message": "Contact is already in this group"}), 200

        new_membership = ContactGroupMembership(contact_id=contact_id, group_id=group_id)
        session.add(new_membership)
        session.commit()
        return jsonify({"message": "Contact added to group"})
    except Exception as e:
        session.rollback()
        return jsonify({"error": f"Could not add member: {e}"}), 500
    finally:
        session.close()

# --- Relationship Management ---
@app.route('/api/relationships', methods=['POST'])
def create_relationship():
    data = request.get_json()
    source_id = data.get('source_contact_id')
    target_id = data.get('target_contact_id')
    label = data.get('label')
    user_id = 1 # Assume single user

    if not source_id or not target_id:
        return jsonify({"error": "Source and Target contact IDs are required"}), 400

    session = get_session()
    try:
        new_rel = ContactRelationship(
            source_contact_id=source_id,
            target_contact_id=target_id,
            label=label,
            user_id=user_id
        )
        session.add(new_rel)
        session.commit()
        new_rel_data = {"id": new_rel.id, "from": new_rel.source_contact_id, "to": new_rel.target_contact_id, "label": new_rel.label}
        return jsonify({"message": "Relationship created", "relationship": new_rel_data}), 201
    except Exception as e: # Catches potential unique constraint violation
        session.rollback()
        return jsonify({"error": f"Could not create relationship. It may already exist. Error: {e}"}), 500
    finally:
        session.close()

4. Frontend Implementation (Expanded)

This section details the UI components needed to manage the graph.

A. New Library (vis.js) and HTML Structure

No changes are needed from the V3.7 brief. The existing HTML for the graph view is sufficient.

B. UI for Managing Groups and Relationships

The user needs an interface to create groups and link contacts. This could be a modal window or a dedicated section on the settings page.

Example HTML for a "Manage Graph" Modal:

<div id="manage-graph-modal" class="modal" style="display:none;">
    <div class="modal-content">
        <h3>Manage Relationship Graph</h3>
        
        <!-- Create New Group -->
        <h4>Create New Group</h4>
        <input type="text" id="new-group-name" placeholder="Group Name">
        <input type="color" id="new-group-color" value="#97C2FC">
        <button id="create-group-btn">Create Group</button>
        
        <hr>

        <!-- Create New Relationship -->
        <h4>Create New Relationship</h4>
        <select id="rel-source-contact"></select>
        <input type="text" id="rel-label" placeholder="Relationship (e.g., Siblings)">
        <select id="rel-target-contact"></select>
        <button id="create-rel-btn">Create Relationship</button>
        
        <button class="close-modal-btn">Close</button>
    </div>
</div>

C. JavaScript Logic (Expanded)

The JavaScript needs to be expanded to handle the new management UI and the more detailed data from the API.

// Add to your main JavaScript file

// --- Expanded Graph Initialization ---
async function initializeGraphView() {
    // ... (same as V3.7, but now it will receive real data) ...

    const options = {
        // ... (same physics and interaction options) ...
        groups: {}, // This will be populated with color definitions
        nodes: {
            shape: 'dot',
            borderWidth: 2,
            font: { size: 14, color: '#333' }
        },
        edges: {
            width: 1,
            color: { color: '#cccccc', highlight: '#848484' },
            font: { align: 'top' }
        }
    };

    // Populate the vis.js groups object with colors from our API
    for (const groupId in data.groups) {
        options.groups[groupId] = { color: data.groups[groupId].color };
    }

    network = new vis.Network(container, graphData, options);

    // ... (same click handler and filter logic as V3.7) ...
}

// --- Logic for the "Manage Graph" Modal ---
function setupGraphManagement() {
    const createGroupBtn = document.getElementById('create-group-btn');
    const createRelBtn = document.getElementById('create-rel-btn');

    createGroupBtn.addEventListener('click', async () => {
        const name = document.getElementById('new-group-name').value;
        const color = document.getElementById('new-group-color').value;
        if (!name) return alert('Group name is required.');

        try {
            const response = await fetch('/api/groups', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ name, color })
            });
            if (!response.ok) throw new Error('Failed to create group.');
            
            alert('Group created successfully!');
            // On success, close modal and refresh the graph view
            document.getElementById('manage-graph-modal').style.display = 'none';
            initializeGraphView();
        } catch (error) {
            alert(error.message);
        }
    });

    createRelBtn.addEventListener('click', async () => {
        const sourceId = document.getElementById('rel-source-contact').value;
        const targetId = document.getElementById('rel-target-contact').value;
        const label = document.getElementById('rel-label').value;
        if (!sourceId || !targetId) return alert('Both contacts must be selected.');
        if (sourceId === targetId) return alert('A contact cannot have a relationship with themselves.');

        try {
            const response = await fetch('/api/relationships', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ 
                    source_contact_id: parseInt(sourceId), 
                    target_contact_id: parseInt(targetId), 
                    label 
                })
            });
            if (!response.ok) throw new Error('Failed to create relationship.');
            
            alert('Relationship created successfully!');
            // On success, close modal and refresh the graph view
            document.getElementById('manage-graph-modal').style.display = 'none';
            initializeGraphView();
        } catch (error) {
            alert(error.message);
        }
    });
}

// When opening the modal, populate the contact dropdowns
async function showManageGraphModal() {
    const sourceSelect = document.getElementById('rel-source-contact');
    const targetSelect = document.getElementById('rel-target-contact');
    
    try {
        // Assuming you have an endpoint to get a simple list of contacts
        const response = await fetch('/api/contacts'); 
        const contacts = await response.json();

        sourceSelect.innerHTML = '<option value="">Select a contact...</option>';
        targetSelect.innerHTML = '<option value="">Select a contact...</option>';

        contacts.forEach(contact => {
            const option1 = new Option(contact.full_name, contact.id);
            const option2 = new Option(contact.full_name, contact.id);
            sourceSelect.add(option1);
            targetSelect.add(option2);
        });

        document.getElementById('manage-graph-modal').style.display = 'block';
    } catch (error) {
        alert('Could not load contacts for relationship management.');
    }
}




